[{"id":"dd102e7597f052d8f60bce0e3074afe4","title":"IELTS Writing Task 1","content":" Writing Task 1\n\n\n\n\n\n\n\n\n\næ ¸å¿ƒï¼šç”¨ä½ çš„è¯­è¨€å‘ˆç°å›¾ä¸­çš„ä¿¡æ¯\n(å°½é‡é¿å…æ€»ç»“ï¼Œä¸ªäººè§‚ç‚¹å’Œæ€è€ƒ)\n æ•°æ®ç±»æ¨¡æ¿\n ç¬¬ä¸€æ®µï¼Œä»‹ç»å›¾è¡¨ï¼š\nThe given xxx chart/graph illustrates/demonstrates the details concerning xxx (timeâ€¦)\n ç¬¬äºŒæ®µï¼Œæ˜æ˜¾å†…å®¹ï¼š\nIn general/Generally, it could be obviously witnessed that xxx.\nWhat is more, xxx\nMoreover, it is noteworthy that â€¦\n ç¬¬ä¸‰æ®µï¼Œç»†èŠ‚æ•°æ®ï¼š\nUpon closer examination of the data, it is discernible that â€¦\n ç¬¬å››æ®µï¼Œç»†èŠ‚æ•°æ®ï¼š\nFurthermore, xxx.\nMoreover, it is palpable and noteworthy that xxx.\n ç¬¬äºŒæ®µå…·ä½“å¥å¼(æ€»ä½“)\n\næŒç»­åœ°ä¸Šå‡ï¼Œä¸‹é™ï¼š\n\nThe number/percentage/xxx of A exhibits a persistent upward/downward trajectory from 2000 to 2005.\nThe data reveals a consistent rise in xxx.\nThe xxx exhibited a consistent growth pattern during the period between 2000 and 2005\nThe graph depicts a continuous upward trend in xxx.\n\n\n\n\næ€¥å‰§ä¸‹é™:\n\nThe figure plummeted dramatically in xxx.\n\n\n\n\nç¨³å®š:\n\n\nThe figures plateaued at xxx.\n\n\nThere has been little to no fluctuation in xxx.\n\n\n\n\n\næ­£è´Ÿç›¸å…³:\n\n\nThe number/percentage/xxx is positively/negatively related to xxx.\n\n\nThe two sets of figures move in tandem/opposite directions.\n\n\n\n\n\nä¸­é€”è¶…è¿‡:\n\n\nThe number/percentage/xxx of A surpassed that of B from 2000 to 2005.\n\n\nA outpaces B in terms of xxx.\n\n\nA surpass B but a noticeable margin.\n\n\n\n\n ç¬¬ä¸‰æ®µå…·ä½“å¥å¼ï¼ˆç»†èŠ‚ï¼‰\n\næ•°æ®æ˜¯å¤šå°‘\n\nThere was a reported figure of 100 for xxx in 2000.\n\n\n\n\næ•°æ®æ¯”è°é«˜å¤šå°‘\n\nThe number of A was higher than that of B by 100.\nThe data indicates that A significantly outstripped B, with Aâ€™s figure reaching xxx compared to Bâ€™s xxx.\n\n\n\n\nå€æ•°å…³ç³»\n\nThe quantity of A was threefold that of B, which an accounting for xxx, a tripling compared to Bâ€™s xxx.\n\n\n\n\næ•°æ®å˜åˆ°äº†å¤šå°‘\n\n\nThe number of A experienced a remarkable surge, elevating sharply from 100 to 200.\n\n\nThere is a significant surge in the number of A, escalating from xxx in 2000 to xxx in 2005.\n\n\n\n\n\næ•°æ®æ³¢åŠ¨\n\nThe data for A demonstrated considerable fluctuations, oscillating between xxx and xxx throughout the 10 years.\n\n\n\n\næ•°æ®ç¨³å®š\n\n\nThe figures for A remained stable, consistently hovering around from xxx to xxx.\n\n\nThe xxx remained relatively constant, with an average of roughly â€¦\n\n\n\n\n è¿æ¥è¯\né¡ºæ‰¿: subsequently\nå¯¹æ¯”: in contrast, conversely\nå¼ºè°ƒ: indeed, notably\nä¸¾ä¾‹: for instance\n èŒƒæ–‡\n\nLine Graph\nThe line graph shows the sales of childrenâ€™s books, adultâ€™s fictions and educational books between 2002 and 2006 in one country.\n\nThe given line graph depicts the figures for three kinds of books in a certain nation between the years 2002 and 2006.\nIn general, it is observable that the sales of childrenâ€™s book exhibited a consistent growth pattern during the aforementioned period. Moreover, it is noteworthy that the sales of educational books remained lower than those of others from 2002 to 2005.\nUpon closer examination of the data, it is discernible that the sales of childrenâ€™s book began at approximately 32 million dollars, escalated to around 46 million dollars in 2005, and culminated at its highest point in 2006. Conversely, the sales of educational books remained relatively constant, with an average of roughly 28 million dollars.\nFurthermore, the sales of adultsâ€™ fiction commenced at about 45 million dollars in 2002 and underwent a reduction of about 9 million dollars in 2003. Then, a marginal increase was observed followed by a drop of approximately 10 million dollars in the subsequent years.\n\n\n æµç¨‹å›¾\n ä½ç½®å›¾\n","slug":"Writing_Task_1","date":"2024-06-11T22:55:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"a5201f72282a8a0869129d066b95cef6","title":"Language and Computing","content":" Turing Problem\nType-0-languages = Recursively Enumerable Languages = Semi-decidable Languages\nDecidable languages = Recursive languages\n\nif there is a Turing Machine that accepts it and always stop on any word\n\nContext-sensitive languages(Type-1-languages): subset of recursive languages\n\nThese are grammars where the left-hand side of a production is always shorter than the right-hand side\n\nHalt problem: the language of encodings of Turing machines that will always stop\nThere are languages that are accepted by a TM (i.e., type 0 languages) but that are undecidable\n DFA\nA deterministic finite automaton (DFA) A=(Q,Î£,Î´,q0,F)A = (Q,Î£, Î´, q_0, F)A=(Q,Î£,Î´,q0â€‹,F) is given by:\n\nA finite set of statesstatesstates QQQ\nA finite set of input symbols, the alphabet,Î£alphabet, Î£alphabet,Î£\nA transitiontransitiontransition functionfunctionfunction Î´âˆˆQÃ—Î£â†’QÎ´ \\in Q \\times Î£ \\rightarrow QÎ´âˆˆQÃ—Î£â†’Q\nAn initialinitialinitial statestatestate q0âˆˆQq_0 \\in Qq0â€‹âˆˆQ\nA set of finalfinalfinal statesstatesstates FâŠ†QF \\sube QFâŠ†Q\n\n The language of DFA\nUsing the extended transition function Î´^\\hat{\\delta}Î´^, we define the language L(A)L(A)L(A) of a DFA AAA formally:\n\n\nÎ´^(q,Ïµ)=q\\hat{\\delta}(q,\\epsilon) = qÎ´^(q,Ïµ)=q\n\n\nÎ´^(q,xw)=Î´^(Î´(q,x),w)\\hat{\\delta} (q, xw) = \\hat{\\delta}(\\delta(q,x),w)Î´^(q,xw)=Î´^(Î´(q,x),w)\n\n\nL(A)={wâ€…â€Šâˆ£â€…â€ŠÎ´^(q0,w)âˆˆF}L(A) = \\{w \\;|\\; \\hat{Î´}(q_0,w) \\in F\\}L(A)={wâˆ£Î´^(q0â€‹,w)âˆˆF}\n\n\n NFA\nA nondeterministic finite automaton (NFA) A=(Q,Î£,Î´,S,F)A = (Q,Î£, Î´, S, F)A=(Q,Î£,Î´,S,F) is given by:\n\nA finite set of statesstatesstates QQQ,\nA finite set of input symbols, the alphabet,Î£alphabet, Î£alphabet,Î£\nA transitiontransitiontransition functionfunctionfunction Î´âˆˆQÃ—Î£â†’P(Q)Î´ \\in Q \\times Î£ \\rightarrow \\mathcal{P}(Q)Î´âˆˆQÃ—Î£â†’P(Q),\nA set of initialinitialinitial statesstatesstates SâŠ†QS \\sube QSâŠ†Q,\nA set of finalfinalfinal (or acceptingacceptingaccepting) states FâŠ†QF \\sube QFâŠ†Q.\n\n The language of NFA\nÎ´^âˆˆP(Q)Ã—Î£âˆ—â†’P(Q)\\hat{Î´} \\in \\mathcal{P}(Q)\\timesÎ£^âˆ— \\rightarrow \\mathcal{P}(Q)Î´^âˆˆP(Q)Ã—Î£âˆ—â†’P(Q), Î´^(P,w)\\hat{Î´}(P,w)Î´^(P,w) is set of states that are reachable from one of states in PPP on word www\n\nÎ´^(P,Ïµ)=P\\hat{\\delta}(P, \\epsilon) = PÎ´^(P,Ïµ)=P\nÎ´^(P,xw)=Î´^(â‹ƒ{Î´(q,x)â€…â€Šâˆ£â€…â€ŠqâˆˆP},w)\\hat{\\delta}(P, xw) = \\hat{\\delta}(\\bigcup\\{\\delta(q,x)\\;|\\;q\\in P\\}, w)Î´^(P,xw)=Î´^(â‹ƒ{Î´(q,x)âˆ£qâˆˆP},w)\nL(A)={wâ€…â€Šâˆ£â€…â€ŠÎ´^(S,w)âˆ©F=Ì¸âˆ…}L(A) = \\{w\\;|\\;\\hat{\\delta}(S,w)\\cap F\\not =\\empty\\}L(A)={wâˆ£Î´^(S,w)âˆ©Fî€ =âˆ…}\n\n Context-free Grammar\nA context-free grammar G=(N,T,P,S)G = (N, T, P, S)G=(N,T,P,S) is given by\n\nA finite set NNN of nonterminalnonterminalnonterminal symbolssymbolssymbols or nonterminalsnonterminalsnonterminals.\nA finite set TTT of terminalterminalterminal symbolssymbolssymbols or terminalsterminalsterminals.\nNâˆ©T=âˆ…N \\cap T = \\emptysetNâˆ©T=âˆ…; i.e., the sets NNN and TTT are disjoint.\nA finite set PâŠ†NÃ—(NâˆªT)âˆ—P \\sube N \\times(N \\cup T)^âˆ—PâŠ†NÃ—(NâˆªT)âˆ— of productions. A production (A,Î±)(A, Î±)(A,Î±), where AâˆˆNA \\in NAâˆˆN and Î±âˆˆ(NâˆªT)âˆ—Î± \\in (N \\cup T)^âˆ—Î±âˆˆ(NâˆªT)âˆ— is a sequence of nonterminal and terminal symbols. It is written as Aâ†’Î±A \\rightarrow Î±Aâ†’Î± in the following.\nSâˆˆNS\\in NSâˆˆN: the distinguished start symbol.\n\n The language of a grammar\nL(G)âŠ†Tâˆ—L(G) \\sube T^âˆ—L(G)âŠ†Tâˆ—, consists of all terminal sentential forms:\n\nL(G)={wâˆˆTâˆ—â€…â€Šâˆ£â€…â€ŠSâ‡’Gâˆ—w}L(G) = \\{w\\in T^*\\;|\\;S\\xRightarrow[G]{*}w\\}L(G)={wâˆˆTâˆ—âˆ£Sâˆ—Gâ€‹w}\n\n Pushdown Automaton\nA Pushdown Automaton P=(Q,Î£,Î“,Î´,q0,Z0,F)P = (Q,Î£, Î“, Î´, q_0,Z_0, F)P=(Q,Î£,Î“,Î´,q0â€‹,Z0â€‹,F) is given by the following data\n\nA finite set QQQ of states,\nA finite set Î£Î£Î£ of input symbols (the alphabet),\nA finite set Î“Î“Î“ of stack symbols,A\ntransition function\nÎ´âˆˆQÃ—(Î£âˆª{Ïµ})Ã—Î“â†’Pfin(QÃ—Î“âˆ—)Î´ \\in Q \\times (Î£ \\cup \\{Ïµ\\}) \\times Î“ \\rightarrow P_{fin}(Q \\times Î“^âˆ—)Î´âˆˆQÃ—(Î£âˆª{Ïµ})Ã—Î“â†’Pfinâ€‹(QÃ—Î“âˆ—)\nHere Pfin(A)P_{fin}(A)Pfinâ€‹(A) are the finite subsets of a set; i.e., this can be defined as\nPfin(A)={Xâˆ£XâŠ†Aâˆ§XÂ isÂ finite.}P_{fin}(A) = \\{X | X \\sube A \\wedge X \\text{ is finite.}\\}Pfinâ€‹(A)={Xâˆ£XâŠ†Aâˆ§XÂ isÂ finite.}\nThus, PDAs are in general nondeterministic because they may have a choice of transitions from any state. However, there are always only finitely many choices.\nAn initial state q0âˆˆQq_0 \\in Qq0â€‹âˆˆQ,\nAn initial stack symbol Z0âˆˆÎ“Z_0 \\in Î“Z0â€‹âˆˆÎ“,\nA set of final states FâŠ†QF \\sube QFâŠ†Q.\n\n ID(Instantaneous Description)\n\n\n\n\n\n\n\n\n\nSuch a triple (q,w,Î³)âˆˆQÃ—Î£âˆ—Ã—Î“âˆ—(q,w, Î³) \\in Q\\timesÎ£^âˆ—\\timesÎ“^âˆ—(q,w,Î³)âˆˆQÃ—Î£âˆ—Ã—Î“âˆ— is called an Instantaneous Description (ID)\n Acceptance by final state\n\nL(P)={wâ€…â€Šâˆ£â€…â€Š(q0,w,Zo)âŠ¢âˆ—(q,Ïµ,Î³)âˆ§qâˆˆF}L(P) = \\{w\\;|\\;(q_0,w,Z_o)\\vdash^*(q,\\epsilon,\\gamma)\\wedge q \\in F\\}L(P)={wâˆ£(q0â€‹,w,Zoâ€‹)âŠ¢âˆ—(q,Ïµ,Î³)âˆ§qâˆˆF}\n\n Acceptance by empty stack\n\nL(P)={wâ€…â€Šâˆ£â€…â€Š(q0,w,Z0)âŠ¢âˆ—(q,Ïµ,Ïµ)}L(P)=\\{w\\;|\\;(q_0,w,Z_0)\\vdash^*(q,\\epsilon, \\epsilon)\\}L(P)={wâˆ£(q0â€‹,w,Z0â€‹)âŠ¢âˆ—(q,Ïµ,Ïµ)}\n\n Deterministic PDAs\n\nâˆ£Î´(q,x,z)âˆ£+âˆ£Î´(q,Ïµ,z)âˆ£â‰¤1,qâˆˆQ,xâˆˆÎ£,zâˆˆÎ“|\\delta(q,x,z)| +|\\delta(q,\\epsilon,z)| \\leq 1, q \\in Q,x\\in \\Sigma,z\\in \\Gammaâˆ£Î´(q,x,z)âˆ£+âˆ£Î´(q,Ïµ,z)âˆ£â‰¤1,qâˆˆQ,xâˆˆÎ£,zâˆˆÎ“\n\n Turing Machine\nA Turing Machine M=(Q,Î£,Î“,Î´,q0,B,F)M = (Q,Î£, Î“, Î´, q_0,B, F)M=(Q,Î£,Î“,Î´,q0â€‹,B,F) is:\n\nA finite set QQQ of states;\nA finite set Î£Î£Î£ of symbols (the alphabet);\nA finite set Î“Î“Î“ of tape symbols s.t. Î£âŠ†Î“Î£ \\sube Î“Î£âŠ†Î“. This is the case because we use the tape also for the input;\nA transition function\nÎ´âˆˆQÃ—Î“â†’{stop}âˆªQÃ—Î“Ã—{L,R}Î´ \\in Q \\times Î“ \\rightarrow \\{stop\\} \\cup Q \\times Î“ \\times \\{L, R\\}Î´âˆˆQÃ—Î“â†’{stop}âˆªQÃ—Î“Ã—{L,R}\nThe transition function defines how the machine behaves if is in state qqq and the symbol on the tape is xxx. If Î´(q,x)Î´(q, x)Î´(q,x) = stop then the machine stops otherwise if Î´(q,x)=(qâ€²,y,d)Î´(q, x) = (qâ€², y, d)Î´(q,x)=(qâ€²,y,d) the machines gets into state qâ€²qâ€²qâ€², writes yyy on the tape (replacing xxx) and moves left if d=Ld = Ld=L or right, if d=Rd = Rd=R;\nAn initial state q0âˆˆQq_0 \\in Qq0â€‹âˆˆQ;\nThe blank symbol BâˆˆÎ“B \\in Î“BâˆˆÎ“ but BâˆˆÌ¸Î£B \\not\\in Î£Bî€ âˆˆÎ£. Initially, only a finite section of the tape containing the input is non-blank;\nA set of final states FâŠ†QF \\sube QFâŠ†Q.\n\n ID(Instantaneous Description)\n\n\n\n\n\n\n\n\n\nAn element (Î³L,q,Î³R)âˆˆID(Î³_L, q, Î³_R) \\in ID(Î³Lâ€‹,q,Î³Râ€‹)âˆˆID describes a situation where the TM is in state QQQ, the non-blank portion of the tape on the left of the head is Î³LÎ³_LÎ³Lâ€‹ and the non-blank portion of the tape on the right, including the square under the head, is Î³RÎ³_RÎ³Râ€‹\n The language of a Turing Machine\n\nL(M)={wâˆˆÎ£âˆ—â€…â€Šâˆ£â€…â€Š(Ïµ,q0,w)âŠ¢âˆ—(Î³L,qâ€²,Î³R)âˆ§qâ€²âˆˆF}L(M) = \\{w\\in \\Sigma^*\\;|\\;(\\epsilon, q_0,w)\\vdash^*(Î³_L, q&#x27;, Î³_R)\\wedge q&#x27; \\in F\\}L(M)={wâˆˆÎ£âˆ—âˆ£(Ïµ,q0â€‹,w)âŠ¢âˆ—(Î³Lâ€‹,qâ€²,Î³Râ€‹)âˆ§qâ€²âˆˆF}\n\n Predictive parsing\nConsider productions for a nonterminal X\n\nXâ†’Î±â€…â€Šâˆ£â€…â€ŠÎ²X \\rightarrow \\alpha \\;|\\;\\betaXâ†’Î±âˆ£Î²\n\n1234parseX (t : ts) =| t âˆˆ first(Î±) -&gt; parse Î±| t âˆˆ first(Î²) -&gt; parse Î²| otherwise -&gt; Nothing\nSuppose it can be the case that\n\nÎ²â‡’âˆ—Ïµ\\beta \\xRightarrow[]{*}\\epsilonÎ²âˆ—â€‹Ïµ\n\n1234parseX (t : ts) =| t âˆˆ first(Î±) -&gt; parse Î±| t âˆˆ first(Î²) âˆª follow(X) -&gt; parse Î²| otherwise -&gt; Nothing\n Disambiguating context-free grammars\n\nEâ†’E+Eâ€…â€Šâˆ£â€…â€ŠEâˆ—Eâ€…â€Šâˆ£â€…â€ŠEâ†‘Eâ€…â€Šâˆ£â€…â€Š(E)â€…â€Šâˆ£â€…â€ŠNE \\rightarrow E + E \\;|\\; E * E \\;|\\;E \\uparrow E \\;|\\; (E) \\;|\\; NEâ†’E+Eâˆ£Eâˆ—Eâˆ£Eâ†‘Eâˆ£(E)âˆ£N\nNâ†’0â€…â€Šâˆ£â€…â€Š1â€…â€Šâˆ£â€…â€Š2N\\rightarrow0\\;|\\;1\\;|\\;2Nâ†’0âˆ£1âˆ£2\n$\\uparrow(right) ;&gt;;*(left);&gt;;+(left) $\n\n The subexpressions of expressions of the highest precedence\n\nEâ†’E1+E1â€…â€Šâˆ£â€…â€ŠE1E\\rightarrow E_1+E_1\\;|\\;E_1Eâ†’E1â€‹+E1â€‹âˆ£E1â€‹\nE1â†’E2âˆ—E2â€…â€Šâˆ£â€…â€ŠE2E_1\\rightarrow E_2*E_2\\;|\\;E_2E1â€‹â†’E2â€‹âˆ—E2â€‹âˆ£E2â€‹\nE2â†’E3â†‘E3â€…â€Šâˆ£â€…â€ŠE3E_2\\rightarrow E_3 \\uparrow E_3\\;|\\;E_3E2â€‹â†’E3â€‹â†‘E3â€‹âˆ£E3â€‹\nE3â†’(E)â€…â€Šâˆ£â€…â€ŠNE_3\\rightarrow (E)\\;|\\;NE3â€‹â†’(E)âˆ£N\nNâ†’0â€…â€Šâˆ£â€…â€Š1â€…â€Šâˆ£â€…â€Š2N\\rightarrow 0\\;|\\;1\\;|\\;2Nâ†’0âˆ£1âˆ£2\n\n Make the corresponding productions left- and right-recursive\n\nEâ†’E+E1â€…â€Šâˆ£â€…â€ŠE1E\\rightarrow E+E_1\\;|\\;E_1Eâ†’E+E1â€‹âˆ£E1â€‹\nE1â†’E1âˆ—E2â€…â€Šâˆ£â€…â€ŠE2E_1\\rightarrow E_1*E_2\\;|\\;E_2E1â€‹â†’E1â€‹âˆ—E2â€‹âˆ£E2â€‹\nE2â†’E3â†‘E2â€…â€Šâˆ£â€…â€ŠE3E_2\\rightarrow E_3 \\uparrow E_2\\;|\\;E_3E2â€‹â†’E3â€‹â†‘E2â€‹âˆ£E3â€‹\nE3â†’(E)â€…â€Šâˆ£â€…â€ŠNE_3\\rightarrow (E)\\;|\\;NE3â€‹â†’(E)âˆ£N\nNâ†’0â€…â€Šâˆ£â€…â€Š1â€…â€Šâˆ£â€…â€Š2N\\rightarrow 0\\;|\\;1\\;|\\;2Nâ†’0âˆ£1âˆ£2\n\n Elimination of left recursion\n\n","slug":"lac","date":"2024-06-09T22:39:56.000Z","categories_index":"Notes","tags_index":"Language,Automata","author_index":"Huaji1hao"},{"id":"e8690cb5cccdb863caaf29d6fe84aa6a","title":"AI Methods","content":" Introduction, Heuristic search (introduction), Pseudo-random numbers\n Preliminaries\n Decision support\nThis term is used often and in a variety of contexts related to decision making\n System\n\nDegree of dependence of systems on the environment\n\nClosed systems are totally independent\nOpen systems dependent on their environment\n\n\nEvaluations of systems\n\nSystem effectiveness: the degree to which goals are achieved, i.e. result, output\nSystem efficiency: a measure of the use of inputs (or resources) to achieve output, e.g., speed\n\n\n\n Solving problems by searching\n\nSearch for paths to goals\n\ntypical algorithms are the depth firstsearch, breadth first search, uniform cost search, branch and bound, A*\n\n\nSearch for solutions (optimisation)\n\nmore general class than searching for paths to goals\nefficiently finding a solution to a problem in a large space of candidate solutions\nsubsumes the first type, since a path through a search tree can be encoded as a candidate solution\n\n\nSearch in Continuous vs Discrete Space\n\n Solving an (mathematical) optimisation problem - steps\n\nFirst choose a quantity (typically a function of several variables â€“objective function) to be maximised or minimised, which might be subject to one or more constraints (constraint optimisation)\nNext choose a mathematical or search method to solve the optimisation problem (searching the space of solutions and detecting absolutely the best/optimal solution)\n\n Optimization\n\n\nFundamental problem of optimization is to arrive at the best possible (optimal) decision/solution in any given set of circumstances\n\n\nGlobal Optimization\nGlobal optimization is the task of finding the absolutely best set of admissible conditions to achieve your objective, formulated in mathematical terms\n\n\nIn most cases â€œthe bestâ€ (optimal) is unattainable\n\n\nGlobal vs Local Optimum\n\nGlobal Optimum- better than all other solutions (best)\nLocal Optimum- better than all solutions in a certainneighbourhood\n\n\n\n Problem and Problem Instance\n\nProblem refers to the high level question or optimization issue to be solved\nAn instance of this problem is the concrete expression, which represents the input for a decision or optimization problem\n\n Combinatorial optimization problems (COP)\n\nRequire finding an optimal object from a finite set of objects\nFor NP-hard COPs, the time complexity of finding solutions can grow exponentially with instance size\n\n Optimization/Search Methods\n\n\nExact/Exhaustive/Systematic Methods\n\ne.g., Dynamic Programming, Branch&amp;Bound, Constraint Satisfaction, â€¦\n\nlimitations: only work if the problem is structured - in many cases for small problem instances\nquite often used to solve sub-problems\n\n\n\n\n\nInexact/Approximate/Local Search Methods\n\ne.g., heuristics,metaheuristics, hyper-heuristics,â€¦\n\n\n\n Search Paradigms\n\nPerturbativeâ†â†’Constructive\n\n\nstart from complete solutions\n\n\nstart from partial solutions\n\n\n\ndeterministic â†â†’ stochastic\n\nprovide the same solution regardless of how many times\ncontain a random component and may return a different solution at each time\n\n\nsystematic â†â†’ local search\nsequential â†â†’ parallel\nsingle objective â†â†’ multi-objective\n\n Heuristic Search/Optimization\n Heuristic Search Methods\n\n\n\n\n\n\n\n\n\nA heuristic is a rule of thumb method derived from human intuition.\n\n\nA heuristic is a problem dependent search method which seeks good, i.e. near-optimal solutions, at a reasonable cost (e.g. speed) without being able to guarantee optimality\n\n\nGood for solving ill-structured problems, or complex well-structured problems (large-scale combinatorial problems that have many potential solutions to explore)\n\n\n Case study: Traveling Salesman Problem (TSP)\n\n\n\n\n\n\n\n\n\n&quot;Given a list of cities and the distances between each pair of cities,what is the shortest possible route that visits each city and returns to the origin city?â€ â€“ NP hard\nExamples heuristics for TSP\n\nThe nearest neighbour (NN) algorithm - Constructive(Stochastic, Systematic)\nA Constructive Stochastic Local Search Algorithm for TSP(based on NN algorithm)\n\nStep 1: Choose a random city\nStep 2: Apply nearest neighbour to construct a complete solution\nStep 3: Compare the new solution to the best found so far and update the best solution as appropriate\nStep 4: Go-to Step 1 and repeat while the maximum number of iterations is not exceeded (parameter)\nStep 5: Return the best solution\n\n\nPairwise exchange (2-opt) - Perturbative (Stochastic, Local Search)\nA Perturbative Stochastic Local Search Algorithm for TSP(based on 2-opt)\n\nStep 1: Create a random current solution (build a permutation array and shuffle its content)\nStep 2: Apply 2-opt: swap two randomly chosen cities forming a new solution\nStep 3: Compare the new solution to the current solution and if there is improvement make the new solution current solution, otherwise continue\nStep 4: Go-to Step 2 and repeat while the maximum number of iterations is not exceeded (parameter)\nStep 5: Return the current solution\n\n\n\n Drawbacks of Heuristic Search\n\nThere is no guarantee for the optimality of the obtained solutions\nUsually can be used only for the specific situation for which they are designed\nOften, heuristics have some parameters\n\nPerformance of a heuristic could be sensitive to the setting of those parameters\n\n\nMay give a poor solution\n\n Pseudo-random numbers\n Some problems with pseudo-random numbers\n\nShorter than expected periods for some seed states; such seed states may be called â€˜weakâ€™ in this context\nLack of uniformity of distribution (e.g., 0.17 appears 100 times in10000 successive numbers while 0.29 appears 5 times more)\nCorrelation of successive values\nThe distances between where certain values occur are distributed differently from those in a random sequence distribution\n\n Components of heuristic search, Hill climbing (HC) , Performance analysis\n Main components of heuristic search methods\n\nRepresentation\nEvaluation function (objective function)\nInitialization (e.g., random)\nNeighborhood relation (move operators)\nSearch process (guideline)\nMechanism for escaping from local optima\n\n Representation\n\n\n\n\n\n\n\n\n\nEncoding of candidate solutions\n Characteristics\n\ncompleteness: all solutions associated with the problem must be represented\nconnexity: a search path must exist between any two solutions of the search space. Any solution of the search space, especially the global optimum solution, can be attained\nefficiency: The representation must be easy to manipulate by the search operators\n\n Example\n\nBinary encoding (e.g. 10110010110010â€¦1011)\n\nGiven a binary string of length N (representing N items), search space size is 2^N\n\n\nPermutation encoding (e.g. for TSP: 1 5 3 2 6 4 7 9 8)\n\nGiven N cities (pubs), search space size is N!\n\n\nInteger encoding (e.g. 1 3 4 5 5 5 4 1 1 â€¦ 2 2 1)\n\nFor a general problem with M composite materials to form an N-layer composite structure, search space size is M^N\n\n\nValue encoding (e.g. ATFCTTCGG) (e.g. 1.2324 5.3243 â€¦) (e.g.&lt;back, back, right, forward, left, â€¦&gt;)\nNonlinear encoding (e.g. tree encoding - genetic programming)\nSpecial encodings (e.g. random key encoding)\n\n Evaluation function\n\n\n\n\n\n\n\n\n\nIndicates the quality of a given solution, distinguishing between better and worse solutions\n\nAlso referred to as objective , cost, fitness, penalty, etc.\nServes as a major link between the algorithm and the problem being solved\n\nprovides an important feedback for the search process\n\n\nMany types: (non)separable, uni/multi-modal, single/multi-objective, etc.\nEvaluation functions could be computationally expensive\nExact vs. approximate\n\nCommon approaches to constructing approximate models: polynomials, regression, SVMs, etc.\nConstructing a globally valid approximate model remains difficult, and so beneficial to selectively use the original evaluation function together with the approximate model\n\n\n\n Evaluation Function - Delta (Incremental) Evaluation\n\nKey idea: calculate effects of differences between current search position S and a neighbour Sâ€™ on the evaluation function value.\nEvaluation function values often consist of independent contributions of solution components; hence,\nf(Sâ€™) can be efficiently calculated from f(S) by differences between S and Sâ€™ in terms of solution components\nCrucial for efficient implementation of heuristics/metaheuristics/hyper-heuristics\n\n Neighborhoods\n\n\n\n\n\n\n\n\n\nA neighborhood of a solution is a set of solutions that can be reached from by a simple operator (move operator/heuristic)\n Example neighborhood for binary representation:\nBit-flip operator\n\nflips a bit in a given solution\nHamming Distance\n\nBetween two bit strings (vectors) of equal length is the number of positions at which the corresponding symbols differ.\ne.g. HD(011, 010) = 1\n\n\nIf the binary string is of size n, then the neighborhood size is n\n\nA discrete value is replaced by any other character of the alphabet\n\nIf the solution is of size n and alphabet is of size k , then the neighborhood size is (k - 1) * n\n\nAdjacent pairwise interchange\n\nswap adjacent entries in the permutation (e.g. 5 1 4 3 2 -&gt; 1 5 4 3 2)\nIf permutation is of size n, then the neighborhood size is n - 1\n\nInsertion operator\n\n\ntake an entry in the permutation and insert itin another position (e.g. 5 1 4 3 2 -&gt; 1 4 5 3 2)\n\n\nNeighborhood size: n * (n - 1)\n\n\nExchange operator\n\narbitrarily selected two entries are swapped(e.g. 5 4 3 1 2 -&gt; 1 4 3 5 2)\n\nInversion operator\n\nselect two arbitrary entries and invert the sequence in between them (e.g. 1 4 5 3 2 -&gt; 1 3 5 4 2)\n\n Summary of components\n\nChoosing an appropriate encoding to represent a candidate solution is crucial in heuristic optimisation\nInitialisation could influence the performance of an optimisation algorithm\nEvaluation function guides the search process and fast evaluation is important\n\n Hill climbing algorithms\n Search paradigm\n\n\n\n\n\n\n\n\n\nPerturbative heuristics/operators:\n\nMutational (diversification/exploration) vs.\nHill-climbing (intensification/exploitation)\n\n\n\nMutational heuristics/operator:\nProcesses a given candidate solution and generates a solution which is not guaranteed to be better\n\n\nHill climbing heuristics/operator:\nProcesses a given candidate solution and generates a better or equal quality solution\n\n\n Minimisation problem &amp; Maximisation problem\n\n\nA local search algorithm which constantly moves in the direction of decreasing level/objective value (for a minimisation problem) to find the nadir/the lowest point of the landscape or best/near optimal solution to the problem\n\nThe hill climbing algorithm halts when it detects a nadir value (where no neighbour has a lower value)\n\n\n\nA local search algorithm which constantly moves in the direction of increasing level/objective value (for a maximisation problem) to find the peak/the highest point of the landscape or best/near optimal solution to the problem\n\nThe hill climbing algorithm halts when it detects a peak value (where no neighbour has a higher value)\n\n\n\n Pseudocode\n\n\nPick an initial starting point (current state) in the search space\n\n\nRepeat\n\n\nConsider the neighbors of the current state\n\n\nCompare new point(s) in the neighborhood of the current state with the current state using an evaluation function and choose a new point with the best quality(among them) and move to that state\n\n\n\n\nUntil there is no more improvement or when a predefined number of iterations is reached\n\n\nReturn the current state as the solution state\n\n\n Note of algorithm\n\n\nInitial starting points may be chosen\n\nrandomly\nuse a constructive heuristic/operator(s)\naccording to some regular pattern\nbased on other information (e.g. results of a prior search)\n\n\n\nVariations of hill-climbing algorithms differ in the way for selecting a new solution compared to the current solution\n\n\nimproving vs. non-worsening ( tmpEval &lt; bestEval vs. tmpEval &lt;=bestEval )\n\n\nWhen to stop\n\nIf the target objective is known, then the search can be stopped when that target objective value is achieved\nHill climbing could be applied repeatedly until a termination criterion is satisfied\n\nHowever, there is no point in applying Best Improvement,Next Improvement and Davisâ€™s (Bit) Hill Climbing if there is no improvement after any single pass over a solution\nRandom Mutation Hill Climbing requires consideration\n\n\n\n\n\n Simple hill climbing heuristic\n\n\nSimple Hill Climbing examining neighbors:\n\nBest improvement (steepest descent/ascent)\nFirst improvement (next descent/ascent)\nTrade-off between the number of search steps required for finding a local optimum and the computation time for each search step.\n\nTypically, for First Improvement search steps can be computed more efficiently than when using Best Improvement, since especially as long as there are multiple improving search steps from a current candidate solution, only a small part of the local neighborhood is evaluated by First Improvement. (Best improvement has larger search range)\nHowever, the improvement obtained by each step of First Improvement local search is typically smaller than for Best Improvement and therefore, more search steps have to be applied to reach a local optimum.\nAdditionally, Best Improvement benefits more than First Improvement from the use of caching and updating mechanisms for evaluating neighbors efficiently.\n\n\n\n\n\nStochastic Hill Climbing (randomly choose neighbors)\n\nDavisâ€™s (bit) hill-climbing (DBHC)\nRandom selection/mutation hill climbing\n\n\n\nRandom-restart (shotgun) hill climbing is built on top of hill climbing and operates by changing the starting solution for the hill climbing, randomly and returning the best\n\n\n Hill climbing vs. Random walk\n\nA Hill-climbing method exploits the best available solution for possible improvement but neglect exploring a large portion of the search space\nRandom walk explores the search space thoroughly but misses exploiting promising regions\n\n Advantage:\nVery easy to implement, requiring:\n\na representation;\nan evaluation function;\na measure that defines the neighborhood around a point in the search space.\n\n Disadvantage:\n\nLocal Optimum: If all neighboring states are worse or the same. The algorithm will halt even though the solution may be far from satisfactory\nPlateau (neutral space/shoulder): All neighbouring states are the same as the current state. In other words the evaluation function is essentially flat. The search will conduct a random walk\nRidge/valley: The search may oscillate from side to side, making little progress. In each case, the algorithm reaches a point at which no progress is being made. If this happens, an obvious thing to do is start again from a different starting point\nAs a result, hill climbing algorithm may not find the optimal solution and may get stuck at a local optimum\nNo information as to how much the discovered local optimum deviates from the global (or even other local optima)\nUsually no upper bound on computation time\nSuccess/failure of each iteration depends on starting point\n\n Question Example\nexample: â€œAssume that Davisâ€™s Bit Hill Climbing , First Improvement Hill Climbing and Steepest Descent Hill Climbing\nalgorithms are applied to a MAX-SAT problem instance resulting in average objective values of 12.4, 34.3 and 25.7, respectively, over 30 runs.â€\n\nDavisâ€™s Bit Hill Climbing would perform the best for solving MAX-SAT problems assuming a minimisation problem formulation (âœ• -Any comment for 1 instance is valid only for 1 problem instance,not for the whole algorithm)\nDavisâ€™s Bit Hill Climbing performs the best based on the average objective value on this problem instance (âœ• - do not know itâ€™s a maximisation / minimisation problem)\nAssuming that the problem is formulated as a maximisation problem, then First Improvement Hill Climbing performs the best based on the average objective value on this problem instance (âœ“)\n\n Statistical tests\n\nThe null hypothesis states the results are due to chance and are not significant in terms of supporting the idea being investigated\nA p-value/ probability value, is a number describing how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true)\nApply non-parametric statistical test - one tailed:\n\nGiven two algorithms: X vs. Y, X &gt; Y (X &lt; Y) denotes that X(Y) is better than Y(X) and this performance difference is statistically significant within a confidence interval of 95% and X &gt;= Y ( X &lt;= Y) indicates that X(Y) performs better on average than  Y(X) but no statistical significance\nA stronger conclusion can be provided for one instance\nAlways repeat the experiments more than or equal to 30 times for any given instance for a meaningful statistical comparison\n\n\n\n\n Boxplots\n\n\n\n\n\n\n\n\n\nBoxplots illustrates groups of numerical data through their quartiles\n\n Notched boxplots\n\n\n\n\n\n\n\n\n\nNotched boxplots allows you to evaluate confidence intervals (by default 95% confidence interval) for the medians of each boxplot\n\n\nSince the notches in the boxplots A vs. B vs. C do not overlap,you can conclude that with 95% confidence that the true medians do differ between each pair of those algorithms on current instance: A performs significantly better than B as well as C, and B performs significantly better than C\n\n Progress plot - per instance\n\n\n\n\n\n\n\n\n\nObjective value from a run or mean of objective values from multiple runs per iteration/time unit\n\n Metaheuristic\n\n\n\n\n\n\n\n\n\nA metaheuristic is a high-level problem independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic optimization algorithms\n Components of metaheuristic\n\nRepresentation of candidate solutions\nEvaluation function\nInitialisation: E.g., initial candidate solution may be chosen\n\nrandomly use a constructive heuristic\naccording to some regular pattern\nbased on other information (e.g. results of a prior search), and more\n\n\nNeighborhood relation (move operators)\nSearch process (guideline)\nStopping conditions\nMechanism for escaping from local optima\n\n\n Mechanism for escaping from local optima\n\nIterate with different solutions, or restart (reinitialise search whenever a local optimum is encountered)\n\nInitialisation could be costly\nrestart could be partial (e.g. change 10% of previous solution)\ne.g. Iterated Local Search (ILS), GRASP\n\n\nChange the search landscape\n\nChange the objective function (e.g. Guided Local Search)\nUse (mix) different neighborhoods (e.g. Variable Neighbourhood Search, Hyper-heuristics)\n\n\nUse Memory\n(e.g. tabu search (TS))\nAccept non-improving moves\nallow search using candidate solutions with equal or worse evaluation function value than the one in hand\n\nCould lead to long walks on plateaus (neutral regions) during the search process, potentially causing cycles â€“ visiting of thesame states\n\n\nNone of the mechanisms is guaranteed to always escape effectively from local optima\n\n Stopping conditions (examples)\n\nStop if a fixed maximum number of iterations, or moves, or objective function evaluations, or a fixed amount of CPU time is exceeded\nStop if consecutive number of iterations since the last improvement in the best objective function value is larger thana specified number\nStop if evidence can be given than an optimum solution has been obtained (i.e. optimum objective value is known)\nStop if no feasible solution can be obtained for a fixed number of steps/time (a solution is feasible if it satisfies all constraints in an optimisation problem)\n\n Deal with (in)feasible solution\n\n\nsimply reject infeasible solution\n\n\nUse a problem domain specific repair operator\n\ne.g. for 0/1 Knapsack Problem with constraints of 15kg, randomly flip a bit to 0 until the solution in hand feasible:1 1 0 1 0: $16 (18 kg, âœ•) -&gt; 1 0 0 1 0: $14 (16 kg, âœ•) -&gt; 1 0 0 0 0: $4 (12 kg, âœ“)\n\n\n\nPenalise each constraint violation for the infeasible solutions such that they canâ€™t be better than the worst feasible solution for a given instance\n\nSet a fixed (death) penalty value poorer than the worst\n\ne.g., fâ€²(s)f&#x27;(s)fâ€²(s)= if s is infeasible, then min{pi,âˆ€i}/2min\\{p_i,\\forall i\\}/2min{piâ€‹,âˆ€i}/2, pip_ipiâ€‹ is the profit from the i th item\n\n\nDistinguish the level of infeasibility of a solution with the penalty\n\ne.g., fâ€²(s)f&#x27;(s)fâ€²(s)= if s is infeasible, then min{pi,âˆ€i}/(2âˆ—(total_weightâˆ’capacity))min\\{p_i,\\forall i\\}/(2*(total\\_weight-capacity))min{piâ€‹,âˆ€i}/(2âˆ—(total_weightâˆ’capacity))\n\n\n\n\n\n Single Point Based Iterative Search - Local Search Metaheuristics  - Stochastic Local Search\n Pseudocode\n12345678910s0; // starting solutions* = initialise(s0) // e.g., improve s0 or use the same repeatRepeat\t// generate a new solution    s&#x27; = makeMove(s*, memory); // choose a neighbour of s*    accept = moveAcceptance(s*, s&#x27;, memory); // remember s_best    if(accept) s* = s&#x27;; // else reject new solution s&#x27;    Until (termination conditions are satisfied)\n\n\nMove Acceptance decides whether to accept or reject the new solution considering its evaluation/quality\n\n\nAccepting non-improving moves could be used as a mechanism to escape from local optimum\n\n\nEffective search techniques provide a mechanism to balance exploration and exploitation\n\nExploration aims to prevent stagnation of search process getting trapped at a local optimum\nExploitation aims to greedily increase solution quality or probability , e.g., by exploiting the evaluation function\n\n\n\nAim is to design search algorithms/metaheuristics that can\n\nescape local optima\nbalance exploration and exploitation\nmake the search independent from the initial configuration\n\n Iterated Local Search (ILS) - Local Search Metaheuristics - Stochastic Local Search\n1234567891011121314//random or construction heuristics0 = GenerateInitialSolution()s* = LocalSearch(s0) //not always usedRepeat\t// random move    s&#x27; = Perturbation(s*, memory)    // hill climbing    s&#x27; = LocalSearch(s&#x27; )    // remember s_best    s* = AcceptanceCriterion(s*, s&#x27;, memory)     // the conditions that the new local optimum    // must satisfy to replace the current solutionUntil (termination conditions are satisfied)return s*\n Based on visiting a sequence of locally optimal solutions by\n\n\nPerturbing the current local optimum (exploration)\n\n\nA perturbation phase might consist of one or more steps\n\n\nThe perturbation strength is crucial\n\nweak perturbations usually lead to shorter local search phases than strong perturbations, because the iterative improvement algorithm takes less steps to identify a local optimum\nToo small / weak: may generate cycles, fall back into the local optimum just visited leading to a stagnation of the search process\nToo big / strong: good properties of the local optima are lost , similar to a random restart of the search process\n\n\n\n\n\napplying local search/hill climbing(exploitation) after starting from the modified solution\n\n\n Acceptance criteria\n\nExtreme in terms of intensification: accept only improving solutions &lt;-&gt; Extreme in terms of diversification: accept any solution\nOther: deterministic (like threshold), probabilistic (like Simulated Annealing)\n\n Memory\nVery simple use: restart search if for a number of iterations noimproved solution is found\n Guidelines\n\nInitial solution should be to a large extent irrelevant for longer runs\nThe interactions among perturbation strength and acceptance criterion can be particularly important\n\nit determines the relative balance of intensification anddiversification\nlarge perturbations are only useful if they can be accepted\n\n\nAdvanced acceptance criteria may take into account search history,\n\ne.g. by occasionally reverting to incumbent solution\n\n\nAdvanced ILS algorithms may change nature and/or strength of perturbation adaptively during search\nLocal search should be as effective and as fast as possible.\n\nBetter local search generally leads to better ILS performance\n\n\nChoose a perturbation operator whose steps cannot be easily undone by the local search\n\n\n Tabu Search (TS) - Local Search Metaheuristics - Stochastic Local Search\n Basic idea\n\n\n\n\n\n\n\n\n\nuses history (memory structures) to escape from local minima/ maxima\n Pseudocode\n1234567determine initial candidate solution sWhile termination criterion is not satisfied\tdetermine set N&#x27; of non-tabu neighbours of s\tchoose a best improving candidate solution s&#x27; in N&#x27;\tupdate tabu attributes based on s&#x27;\ts = s&#x27;\n\n\nIn each step, move to â€˜non-tabuâ€™ best neighbouring solution (admissible neighbours), although it may be worse than current one\n\n\nTo avoid cycles, TS tries to avoid revisiting previously seen solutions\n\n\nTo avoid storing complete solutions, TS bases the memory on attributes of recently seen solutions\n\n\nTabu solution attributes are often defined via local search moves\n\n\nTabu-list contains moves which have been made in the recent past\n\n\nTabu tenure/tabu list length:\nthe length of time/number of steps ttt for which a move is forbidden\n\nttt too low - risk of cycling\nttt too high - may restrict the search too much\nttt = 7 has often been found sufficient to prevent cycling\nt=nt = \\sqrt{n}t=nâ€‹\nnumber of tabu moves: 5 ~ 9\n\n\n\nSolutions which contain tabu attributes are forbidden for a certain number of iterations\n\n\n\n\nOften, an additional aspiration criterion is used: this specifies conditions under which tabu status may be overridden (e.g. if considered step leads to improvement in incumbent solution)\n\nIf a tabu move is smaller than the aspiration level then we accept the move (use of aspiration criteria to override tabu status)\n\n\n\n 3 main components\n\nForbidding strategy: control what enters the tabu list\nFreeing strategy: control what exits the tabu list and when\nShort-term strategy: manage interplay between the forbidding strategy and freeing strategy to select trial solutions\n\n Memory\nheavily relies on the use of an explicit memory of the search process\n\nsystematic use of memory to guide search process\nmemory typically contains only specific attributes of previously seen solutions\nsimple tabu search strategies exploit only short term memory\nmore complex tabu search strategies exploit long term memory\n\n Introduction to Scheduling\n\n\n\n\n\n\n\n\n\nScheduling deals with the allocation of resources to tasks over given time periods and its goal is to optimize one or more objectives. The resources and tasks in an organization can take many different forms.\n Framework &amp; Notation\n\n\njobs j=1,2,...,nj = 1, 2, ..., nj=1,2,...,n (number of jobs are assumed to be finite)\n\n\nmachines i=1,2,...,mi = 1, 2, ..., mi=1,2,...,m (number of machines are assumed to be finite)\n\n\n(i,j)(i, j)(i,j)- processing step, or operation of job j on machine i\n\n\nscheduling problem - Î± | Î² | Î³\n\nÎ± - machine characteristics (environments)\nÎ² - processing/job characteristic\nÎ³ - optimality criteria (objective to be minimised)\n\n\n\n Sample Machine Characteristics (Î±)\n\n\n111 Single machine\n\n\nPmPmPm Identical machines in parallel\n\nmmm machines in parallel\nJob jjj requires a single operation and may be processed on any of the m machines\n\n\n\nQmQmQm Machines in parallel with different speeds\n\n\nRmRmRm Unrelated machines in parallel machines have different speeds for different jobs\n\n\n Sample Job Characteristics (Î²)\n\nProcessing time pijp_{ij}pijâ€‹ - processing time of job jjj on machine iii (if a single machine then pjp_jpjâ€‹)\nDue date djd_jdjâ€‹ - committed shipping or completion (due) date of job jjj\nWeight wjw_jwjâ€‹ - importance of job jjj relative to the other jobs in the system\nRelease date rjr_jrjâ€‹ - earliest time at which job jjj can start its processing\nPrecedence precprecprec â€“ Precedence relations might be given for the jobs. If kkk precedes lll, then starting time of lll should be not earlier than completion time of kkk.\nSequence dependent setup times sjks_{jk}sjkâ€‹ - setup time between jobs jjj and kkk\nBreakdowns brkdwnbrkdwnbrkdwn - machines are not continuously available\n\n Sample Optimality Criteria (Î³)\n\n\nCijC_{ij}Cijâ€‹ completion time of the operation of job jjj on machine iii\n\n\nCjC_jCjâ€‹ time when job jjj exits the system\n\n\nCmaxC_{max}Cmaxâ€‹ makespan is the time difference from the start (often, t=0) to\nfinish when the last job exits the system\n\n\nLj=Cjâˆ’djL_j = C_j - d_jLjâ€‹=Cjâ€‹âˆ’djâ€‹ lateness of job jjj\n\n\nTj=max(Cjâˆ’dj,0)T_j = max(C_j - d_j , 0)Tjâ€‹=max(Cjâ€‹âˆ’djâ€‹,0) tardiness of job jjj\n\n\nU_j = \n\\begin{cases}\n1 &amp; \\text{if  } C_j &gt; d_j\\\\\n0 &amp; otherwise\\\\\n\\end{cases}$$**unit penalty** of job $j$\n\n\n\n\n\n\n1âˆ£precâˆ£Cmaxâˆ’A1 | prec | C_{max} - A1âˆ£precâˆ£Cmaxâ€‹âˆ’A\nA single machine, general precedence constraints, minimising makespan (maximum completion time)\n\n\nP3âˆ£dj,sjkâˆ£âˆ‘Ljâˆ’3P3 | d_j, s_{jk} | \\sum L_j - 3P3âˆ£djâ€‹,sjkâ€‹âˆ£âˆ‘Ljâ€‹âˆ’3\n3 identical machines, each job has a due date and sequence dependent setup times between jobs,minimising total lateness of jobs\n\n\nRâˆ£âˆ£âˆ‘CjR||\\sum C_jRâˆ£âˆ£âˆ‘Cjâ€‹\nvariable number of unrelated parallel machines, no constraints, minimising total completion time\n\n\n1âˆ£djâˆ£âˆ‘wjTj1|d_j|\\sum w_jT_j1âˆ£djâ€‹âˆ£âˆ‘wjâ€‹Tjâ€‹\nGiven nnn jobs to be processed by a single machine, each job jjj with a due date djd_jdjâ€‹ , processing time pjp_jpjâ€‹ , and a weight wjw_jwjâ€‹ , find the optimal sequencing of jobs producing the minimal weighted tardiness wjTjw_jT_jwjâ€‹Tjâ€‹\n\n\n Move Acceptance in Local search Metaheuristics, Parameter Setting issues\n Move Acceptance Methods of Local Search Metaheuristics\n\n Parameter setting mechanisms in Move Acceptance\n\n\nStatic - either there is no parameter to set or parameters are set toa fixed value (e.g. IoM=5 )\n\n\nDynamic - parameter values vary with respect to time/iteration count. Given the same candidate and current solutions at the same current elapsed time or iteration count, the acceptance threshold or acceptance probability would be the same irrespective of search history\n(e.g. IoM=round(1+(itercurrent/itermax)âˆ—4)IoM = round(1 +(itercurrent / itermax) * 4)IoM=round(1+(itercurrent/itermax)âˆ—4))\n\n\nAdaptive - Given the same candidate and current solutions at the same current elapsed time or iteration count, the acceptance threshold or acceptance probability is not guaranteed to be the same as one or more components depend on search history\n(e.g. if for 100 steps best solution found so far cannot be improved, then IoM++, and after any improvement, reset IoM=1)\n\n\n Non-stochastic &amp; Basic Move Acceptance Methods\n\nReuse the objective values of previously encountered solutions for the accept/reject decisions\nstatic\n\nall moves fâ€²(s)f&#x27;(s)fâ€²(s)\nimproving moves only f(sâ€²)&lt;f(s)f(s&#x27;) &lt; f(s)f(sâ€²)&lt;f(s)\nimproving and equal f(sâ€²)â‰¤f(s)f(s&#x27;) \\leq f(s)f(sâ€²)â‰¤f(s)\n\n\ndynamic: none\nadaptive\n\nLate Acceptance: compares the quality of the solution with that of the solution accepted/visited LLL iterations previously stâˆ’Ls_{t-L}stâˆ’Lâ€‹, and accepts the move if and only if f(sâ€²)â‰¤f(stâˆ’L)f(s&#x27;) \\leq f(s_{t-L})f(sâ€²)â‰¤f(stâˆ’Lâ€‹)\n\nInitialisation: assign all elements of the list to be equal to the initial cost (objective value)\nList implementation: List for the history of the objective values of the recent solutions is implemented as a circular queue\n\n\n\n\n\n Non-stochastic &amp; Threshold Move Acceptance Method\nDetermine a threshold which is in the vicinity of a chosen solution quality, e.g. the quality of the best solution found so far or current solution, and accept all solutions below that threshold pseudocode for minimisation\n Pseudocode for minimisation\n12345678910s0 = generateInitialSolution();s, s_best = s0;// initialise other relevant parameters if there is anyREPEAT\ts&#x27; = makeMove(s, memory); // choose a neighbour of s*\tthreshold = moveAcceptance-&gt;getThreshold(s, s&#x27;, memory);\tif(f(s&#x27;) &lt;= threshold) s = s&#x27;; // else reject new solution s&#x27;\ts_best = updateBest(s, s&#x27;); //keep track of s_bestUNTIL(termination conditions are satisfied):RETURN s_best\n\n\nstatic\n\nAccept a worsening solution if the worsening of the objective value is no worse than a fixed value\n\n\n\ndynamic\n\n\nGreat Deluge\n\n\n\nFlex Deluge\n\n\n\n\nAdaptive\n\nRecord to record travel (RRT)\nExtended Great Deluge\n\nbased on Great Deluge\nFeedback is received during the search and decay-rate is updated/reset accordingly whenever there is no improvement for a long time\n\n\nModified Great Deluge\n\n\n\n Stochastic Move Acceptance\n Psuedocode\n12345678910s0 = generateInitialSolution();s, s_best = s0;REPEAT\ts&#x27; = makeMove(s, memory); // choose a neighbour of s\tP = moveAcceptance-&gt;getAcceptanceProbability(s, s&#x27;, memory);\tr = getRandomValue(); // a uniform random value between [0, 1]\tif(f(s&#x27;).isBetterThan(f(s)) || r &lt; P) s = s&#x27;; // else reject new solution s&#x27;\ts_bset&lt;-updateBest(s, s&#x27;); // keep track of s_bestUNTIL(termination conditions are satisfied);RETURN s_best;\n\n\nstatic\n\nNaive Acceptance: P is fixed, e.g. if improving P = 1.0, else P = 0.5\n\n\n\ndynamic\n\n\nSimulated Annealing : P changes in time with respect to the difference in the quality of current and previous solutions\n\n\nadvantages:\n\neasy to implement\nachieves good performance given sufficient running time\n\n\n\ndrawbacks:\n\nrequires a good parameter setting for improved performance\nHas interesting theoretical properties (convergence),but these are of very limited practical relevance\n\n\n\nPseudocode\n123456789101112131415INPUT: ğ‘‡0, ğ‘‡ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘ 0 â† ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘’ğ¼ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ğ‘†ğ‘œğ‘™ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›();ğ‘‡ â† ğ‘‡0; // initialise temperature to ğ‘‡0S_ğ‘ğ‘’ğ‘ ğ‘¡ â† ğ‘ 0; ğ‘  â† ğ‘ 0; // set ğ‘  and S_ğ‘ğ‘’ğ‘ ğ‘¡ to initial solutionREPEAT    ğ‘ â€² â† ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ ) ; // choose a neighbouring solution of ğ‘     Î” = ğ‘“(ğ‘ â€²) âˆ’ ğ‘“(ğ‘ );    ğ‘Ÿ â† ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š âˆˆ[0,1]; // get a uniform random number in the range [0,1)    if(Î” &lt; 0 || ğ‘Ÿ &lt; ğ‘ƒ(Î”,ğ‘‡) ) &#123; // if solution is non-worsening or in Boltzmann probability        s â† ğ‘ â€²;    &#125;    S_best â† ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ğµğ‘’ğ‘ ğ‘¡(); // keep track of best solution    ğ‘‡ â† ğ‘ğ‘œğ‘œğ‘™ğ‘‡ğ‘’ğ‘šğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’(); // decrease the temperature according to cooling scheduleUNTIL (Termination conditions are satisfied);Return S_ğ‘ğ‘’ğ‘ ğ‘¡;\n\n\nAccepting moves\n\nÎ”=F(snew)âˆ’F(sold)Î” = F(s_{new}) - F(s_{old})Î”=F(snewâ€‹)âˆ’F(soldâ€‹)\nImproving moves (i.e. Î” &lt; 0, assuming minimisation, below are same) are accepted\nWorsening moves are accepted using the Metropolis criterion at a given temperature TTT\n\nfor Î” &gt; 0 accept with a Boltzman probability of P(Î”,T)=eâˆ’Î”TP(Î”, T) = e^{\\frac{-Î”}{T}}P(Î”,T)=eTâˆ’Î”â€‹\nU(0,1)U(0, 1)U(0,1) generates a random number in [0, 1)\naccept if U(0,1)&lt;P(Î”,T)U(0, 1) &lt; P(Î”, T)U(0,1)&lt;P(Î”,T)\n\n\n\n\n\nCooling / Annealing\n\n\nAs the temperature T decreases, the probability of accepting worsening moves decreases\n\n\nStarting Temperature (T0)\n\nhot enough: to allow almost all neighbors\nnot so hot: random search for sometime\nEstimate a suitable starting temperature:\n\nReduce quickly to 60% of worse moves are accepted\nUse this as the starting temperature\n\n\n\n\n\nFinal Temperature\n\nUsually 0, however in practice, not necessary\nT is low: accepting a worse move is almost the same as T=0\nThe stopping criteria: either be a suitably low T, or â€œfrozenâ€ at the current T (i.e. no worse moves are accepted)\n\n\n\nTemperature Decrement\n\nLinear: $$T = T - x$$\nGeometric: $$T = T * Î±$$\n\nExperience: Î± is typically in the interval [0.9, 0.99]\n\n\nLundy Mees: $$T = \\frac{T}{1 + Î²T}$$\n\nOne iteration at each T, but decrease T very slowly.\nExperience: Î² is typically a very small value, that is close to 0 (e.g., 0.0001)\n\n\n\n\n\nIterations at each temperature\n\nOne iteration at each TTT\nA constant number of iterations at each TTT\nCompromise\n\nEither: a large number of iterations at a few TTTs, or\nA small number of iterations at many TTTs, or\nA balance between the two\n\n\nDynamically change the no. of iterations\n\nAt higher TTTs: less no. of iterations\nAt lower TTTs: large no. of iterations, local optimum fully exploited\n\n\n\n\n\nReheating\n\nIf stuck at a local optimum for a while, increase the current temperature with a certain rate\n\n\n\n\n\n\n\n\n\nadaptive\n\nSimulated Annealing with reheating:\nP is modified time to time causing partial restart â€“increasing the probability of acceptance of non-improving solutions\nSimulated Annealing using the best found solution so far(or in a phase) for acceptance with a cooling schedule\n\n\n\n Parameter Setting Issues and Tuning Methods\n Parameter types\n\nCategorical/symbolic/structural parameters - e.g. choice of initialisation method, choice of mutation, â€¦\nOrdinal parameters - e.g. neighborhoods (e.g., small, medium,large),â€¦\nNumerical/behavioural parameters\n\ninteger, real-valued, â€¦\ne.g. population sizes, evaporation rates, â€¦\nvalues may depend on the setting of categorical or ordinal parameters\n\n\n\n\n Parameter Setting Methods\n\n\nParameter tuning (Off-line setting):\nFinding the best initial settings for a set of parameters before the search process starts (off-line)\nE.g., fixing the mutation strength in ILS, mutation probability in genetic algorithms, etc.\nThe initial parameter setting influences the performance of a metaheuristic.\n\nSequential tuning\nDesign of Experiments\nMeta-optimisation\n\n\n\nParameter control (Online setting):\nManaging the settings of parameters during the search process (online) (dynamic, adaptive, self-adaptive)\nE.g., changing the mutation strength in ILS, changing the mutation probability in genetic algorithms during the search process\nControlling parameter setting could yield a system which is not sensitive to its initial setting\n\nDynamic\nAdaptive\n\nSelf-adaptive\n\n\n\n\n\n Parameter Tuning methods\n\n\nTraditional approaches\n\nUse of an arbitrary setting\nTrial&amp;error with settings based on intuition\nUse of theoretical studies\nA mixture of above BOOKMARK\n\n\n\nSequential tuning: fix parameter values successively (e.g., fix A=20 and tune B that is try {0, 0.3, 0.5, 0.8, 1.0 }}, then fixing the best setting for B from the previous trials and tune A that is try {20, 40,50, 60, 80})\n\n\nDesign of experiments (DoE)\n\n\nA systematic method (controlled experiments) to determine the relationship between controllable and uncontrollable factors (inputs to the process, variables) affecting a process(e.g. running of an algorithm), their levels (settings) and the response (output) of that process (e.g. quality of solutions obtained performance of an algorithm)\n\n\nImportant outcomes are measured and analysed to determine the factors and their settings that will provide the best overall outcome\n\n\nFractional Factorial Designs - designed to draw out valuableconclusions from fewer runs (&lt;-&gt; a number of factors is k in an n level factorial design can results in n^k runs for even a single replicate)\n\nKey observation: Responses are often affected by a small number of main effects and lower order interactions,while higher order interactions are relatively unimportant\n\n\n\nSampling - whenever factorial design is not possible, sampling is performed\n\n\nrandom\n\nGenerate each sample point independently (M)\n\n\n\nLatin Hyper-cube\n\nDecide the number of sample points(M) for N variables and for each sample point remember in which row and column the sample point was taken\n\n\n\n\n\n\nOrthogonal\n\nThe sample space is divided into equally probable subspaces. Sample points simultaneously,ensuring they form an ensemble of Latin Hypercube sample\n\n\n\n\n\n\nTaguchi Orthogonal Arrays Method for Parameter Tuning\n\n\n\n\n\n\nMeta-optimisation: use a metaheuristic to obtain â€œoptimalâ€parameter settings\n\n\nTaguchi Orthogonal Arrays Method for Parameter Tuning\n(Parameter Tuning methods - Design of experiments (DoE) - Sampling - Orthogonal)\n\nAim : make a â€œproductâ€ or â€œprocessâ€ less variable (more robust) in the face of variation over which we have little or no control\n\n\n\n      \n Parameter Control\n\n\nStatic:\n\n\n\n\n\n\n\n\n\nfixed  parameter value\nExample:  Accept a worsening solution if the worsening of the objective value is no worse than a fixed value\n\n\nDynamic:\n\n\n\n\n\n\n\n\n\nparameter value vary with respect to time/iteration count\nExample:  Great Deluge,  Flex Deluge, Simulated Annealing\n\n\nAdaptive:\n\n\n\n\n\n\n\n\n\ndepend on search history\nExample:  Record to record travel (RRT), Extended Great Deluge, Modified Great Deluge, Simulated Annealing with reheating\n\n\n\n\n Evolutionary Algorithms (EAs) (I/II): Genetic Algorithms (GAs),Memetic Algorithms (MAs), Benchmark functions\n Evolutionary Algorithms (EAs)\n\n\n\n\n\n\n\n\n\nEAs simulate natural evolution (Darwinian Evolution) of individual structures at the genetic level using the idea of survival of the fittest via processes of selection, mutation, and reproduction (recombination)\n\nEA includes GA, GP, EP\n\n\n\nAn individual/chromosome represents a candidate solution for the problem at hand\n\n\nA collection of individuals currently â€œaliveâ€, called population (set of individuals/chromosomes) is evolved from one generation (iteration) to another depending on the fitness of individuals in a given environment , indicating how fit an individual is, (how close it is to the optimal solution)\n\n\nHope: last generation will contain the final solution\n\n\n History\n\n\nGenetic Algorithms\n\nMemetic Algorithms\n\n\n\n\n\n\n\n\n\n\nevolves bit strings\n\n\nEvolutionary Programming\n\n\n\n\n\n\n\n\n\nevolves parameters of a program with a fixed structure\n\n\nEvolution Strategies\n\n\n\n\n\n\n\n\n\nvectors of real numbers\n\n\nGenetic Programming\n\n\n\n\n\n\n\n\n\nevolves computer programs in tree form\n\n\nGene Expression Programming\n\n\n\n\n\n\n\n\n\ncomputer programs of different sizes are encoded in linear chromosomes of fixed length\n\n\nGrammatical Evolution\n\n\n\n\n\n\n\n\n\nevolves solutions wrt a specified grammar\n\n\n\n\n Key Features of EAs\n\npopulation based search approaches\n\nBe independent of initial starting point(s) - Start search from many points in the search space\nConduct search in parallel over the search space - implicit parallelism\n\n\nAvoid converging to local optima\n\n Weakness\n\nLimited theoretical and mathematical analyses - this is a growing field of study\nConsidered slow for online applications and even for some large offline problems\n\n Genetic Algorithms(GAs)\n Pseudocode\n1234567891011begingenerate initial population; // initialisecalculate fitness values; //evaluate populationdo&#123;\tperform reproduction; //select parents\trecombine pairs with p_c; // apply corssover\tapply mutation with p_m; // mutate offspring\tcalculate fitness values; // eval. population\treplace current population;&#125; while termination criteria not satisfied;end\n Basic components of GAs\n\nA genetic representation (encoding) for candidate solutions(individuals) to the problem at hand\nAn initialisation scheme to generate the first population (set) of candidate solutions (individuals)\nA fitness (evaluation) function that plays the role of the environment, rating the solutions in terms of their fitness\nA scheme for selecting mates (parents) for recombination\nCrossover (recombination) exchanges genetic material between mates producing offspring children\nMutation perturbs an individual creating a new one\nReplacement strategy to select the surviving individuals for the next generation\nTermination Criteria\nValues for various parameters that GA uses (population size, probabilities of applying genetic operators, etc)\n\n Representation\n\nHaploid structure: each individual contains one chromosome\nchromosome contain a fixed number of genes: chromosome length\neach individual is evaluated and has an associated fitness value\nTraditionally binary encoding is used for each gene: Allele value âˆˆ\\inâˆˆ {0, 1}\na population contains a fixed number of individuals: population size\neach iteration is referred as generation\n\n Initialisation\n\nRandom initialisation\n\nPopulation size number of individuals are created randomly\nEach gene at a locus of an individual is assigned an allele value 0 or 1 randomly\n\n\n\n Fitness calculation\n\nFitness value indicates â€œhow fit the individual is to survive and reproduce under the current conditionsâ€\ni.e. how much the current solution meets the requirements of the objective function\nis obtained by applying the fitness function to the individualâ€™s chromosome (candidate solution)\ni.e. genotype (e.g. 101110) to phenotype (e.g.1) mapping\n\n Reproduction\n\n\nselecting individuals : apply selection pressure considering the fitness of individuals in the population (e.g. roulette wheel selection, tournament selection, rank selection, truncation selection, Boltzmann selection, â€¦)\n\nSelection pressure means the individuals with better fitness have higher chance for being selected\n\n\n\nusually 2 parents (individuals/candidate solutions) are selected using the same method, which will go under the crossover operation\n Roulette Wheel Selection\n\nFitness level is used to associate a probability of selection(probiprob_iprobiâ€‹) with each individual chromosome\nExpected number of representatives of each individual in the pool is proportional to its fitness\nâ€œWhile candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may beâ€\n\n Tournament Selection\n\nThis method involves running a number of &quot;tournaments&quot;among randomly chosen individuals (of tour size)\nselecting the one with best fitness at the end\n\n\n\n Crossover / Recombination\n\n\napplied with a crossover probability pcp_cpcâ€‹ which in general is chosen to 1.0\n One Point Crossover(1PTX)\n\nGenerate a random number in [0, 1), if it is smaller than a crossover probability pcp_cpcâ€‹ Then\n\nSelect a random crossover site in [1, chromosome length]\nSplit individuals at the selected site\nExchange segments between pairs to form two new individuals\n\n\nElse\n\nCopy the individuals as new individuals\n\n\n\n\n\n\n 2 Point Crossover (2PTX)\n K-point Crossover\n Uniform Crossover (UX)\n\nconsiders each bit in the parent strings for exchange with a probability of 0.5\n\n\n\n\n\n\n Mutation\n\nprovides diversity and allows GA to explore different regions of the search space (escaping)\nLoop through all the alleles of all the individuals one by one, and if that allele is selected for mutation with a given probability you can either change it by a small amount or replace it with a new value (for binary representation, flipping a gene value)\n\nMutation rate is typically chosen to be very small (e.g. 0.001)\nChoosing pmp_mpmâ€‹ as (1 / chromosome length) implies on average a single gene will be mutated for an individual\n\n\n\n Replacement\n\n\nGeneration gap (Î±Î±Î±) controls the fraction of the population to be replaced in each generation, where $Î± \\in [1/N,1.0] $Number of offspring produced at each generation is g=Î±âˆ—Ng=Î±*Ng=Î±âˆ—N\n\n\n(Trans-) Generational GA\nNNN individuals produce Î±NÎ±NÎ±N offspring, so (N+Î±N)â†’N(N + Î±N) â†’ N(N+Î±N)â†’N\n\nÎ±NÎ±NÎ±N replaces worst Î±NÎ±NÎ±N of NNN\n\nlargest generation gap where Î±=1.0Î±=1.0Î±=1.0 yields g=Ng=Ng=N.\nGA relies on improvement of average objective values from one population to another\n\nIt is always a good idea not to loose the best solution found so far.\n\n\n\n\nsort (N+Î±N)(N + Î±N)(N+Î±N) and choose the NNN best (elitism)\n\n\n\nSteady-State GA (g=2, that is Î±=2/N)\nTwo offspring replace two individuals from the old generation.\n\nMethod#1: two offspring replace two parents\nMethod#2: two offspring replace worst two of the population\nMethod#3: best two of (parents and offspring) replace two parents (elitism)\nMethod#4: best two of (parents and offspring) replace worst two of the population (strong elitism)\n\n\n\n Termination Criteria\n\nA predefined maximum number of generations is exceeded\nA goal is reached (e.g. Expected fitness is achieved, Population converges)\nBest fitness does not change for a while\nA condition is satisfied depending on a combination of above\n\n Convergence - definition\n\nDefined as the progression towards uniformity (individuals become alike)\nGene convergence: a location on a chromosome is converged when 95% of the individuals have the same gene value for that location\nPopulation (Genotypic) convergence: a population is converged when all the genes have converged (all individuals are alike they might have different fitness)\nPhenotypic Convergence: average fitness of the population approaches to the best individual in the population (all individuals have the same fitness)\n\n Memetic Algorithms (MAs)\n\n\n\n\n\n\n\n\n\nMeme: contagious piece of information\n\nMemes are similar to local refinement/local search\nGene vs. Meme\n\nMemes can change, evolve using rules and time scales other than the traditional genetic ones\n\n\n\n\nMAs aim to improve GAs by embedding local search\n\nMAs are much faster and accurate than GAs on some problems\n\n\nMAs make use of exploration capabilities of GAs and exploitation capabilities of local search (i.e. an explicit mechanism to balance exploration and exploitation)\n\n\n Benchmark functions\n\n\n\n\n\n\n\n\n\nBenchmark functions serves as a testbed for performance comparison of (meta/hyper)heuristic optimisation algorithms\n Why use benchmark functions\n\nTheir global minimum are known\nThey can be easily computed\nEach function is recognised to have certain characteristics potentially representing a different real world problem\n\n Classification\n\n\nContinuity (Differentiability)\n\n\nf(x)=âˆ£xâˆ£f(x)=âˆ£xâˆ£f(x)=âˆ£xâˆ£, continuous but not differentiable\n\n\ndiscontinuous vs. continuous\n\n\n\n\nDimensionality (Scalability)\n\n\nModality\n[The number of ambiguous peaks in the function landscape]\n\nUnimodal\nMultimodal with few local minima\nMultimodal with exponential number of local minima\n\n\n\nSeparability\n[each variable of a function is independent of the other variables; A function of variables is separable if it can be rewritten as a sum of functions of just one variable (note: see examples for comprehensive understanding)]\n\nseparable functions allows delta evaluation\n\n Examples\n\nf(x)=âˆ‘i=1nxi2f(x) = \\sum_{i=1}^{n}x_i^2f(x)=âˆ‘i=1nâ€‹xi2â€‹\n\ncontinuous, differentiable, separable, scalable\n\n\nf(x)=âˆ‘i=1n(âŒŠâˆ£xiâˆ£âŒ‹)f(x) = \\sum_{i=1}^{n}(âŒŠâˆ£x_iâˆ£âŒ‹)f(x)=âˆ‘i=1nâ€‹(âŒŠâˆ£xiâ€‹âˆ£âŒ‹)\n\ndiscontinuous, non-differentiable, separable, scalable\n\n\nf(x,y)=âˆ’20exp(âˆ’0.20.5(x2+y2)âˆ’exp(0.5(cos(2Ï€x)+cos(2Ï€y)))+e+20f(x, y) = -20exp(-0.2\\sqrt{0.5(x^2+y^2)}-exp(0.5(cos(2\\pi x) +cos(2\\pi y)))+e+20f(x,y)=âˆ’20exp(âˆ’0.20.5(x2+y2)â€‹âˆ’exp(0.5(cos(2Ï€x)+cos(2Ï€y)))+e+20\n\ncontinuous, differentiable, non-separable, non-scalable\n\n\n\n\n\n Evolutionary Algorithms (EAs) (II/II): MA &amp; GA (cont.), Multimeme Memetic Algorithms (MMAs)\n Case study of GAs/MAs\n Binary Coding vs. Gray Coding\n\nGray encoding ensures a Hamming distance of 1 for the adjacent numbers\nShown to be useful in GAs empowering the algorithm to mutate a solution in the right direction\n\n Binary Representation for Encoding Permutation / Permutation basedGenetic Operators\n\n Partially Mapped Crossover (PMX)\n\n Order Crossover (OX)\n\n\n Cycle Crossover (CX)\n\n Multimeme Memetic Algorithms (MMAs)\n Self Adaptation\n\n\n\n\n\n\n\n\n\nDeciding which operators and settings to use on the fly whenever needed receiving feedback during the evolutionary search process\n\n\ni.e. co-evolve genetic and memetic material\n\nMultimeme Algorithm can indeed learn how to choose an operator and relevant settings through the evolutionary process (co evolution)\nThere is a trade off as learning requires time and a memetic algorithm with a single setting could perform better\n\n\n\nMemes represent instructions for self improvement\n\nSpecify set of rules, programs, heuristics, strategies,behaviors, etc.\n\n\n\nMeme of each operator can be combined under a memeplex\n\n\nGrammar for memeplex - Compound of Memes\n\n\n\nExample\n\n\n\n Mutating Memes during evolution\n\n\nInnovation rate IRâˆˆ[0,1]IR\\in [0, 1]IRâˆˆ[0,1] : the probability of mutating the memes\n\nIR=0IR = 0IR=0 - no innovation\n\nif a meme option is not introduced in the initial generation, it will not be reintroduced again\n\n\nIR=1IR=1IR=1\n\nAll different strategies implied by the available MMM memes might be equally used\n\n\n\n\n\nConcentration of a meme Ci(t)C_i(t)Ciâ€‹(t)\n\ntotal number of individuals that carry the meme iii at a given generation ttt\nCrude measure of a meme success; gives no information about continual usage of a meme\n\n\n\nEvolutionary activity of a meme ai(t)a_i(t)aiâ€‹(t)\n\nthe accumulation of meme concentration until a given generation\na_i(t) = \\left\\{\\begin{array}{**lr**}  \\int_0^t c_i(t)dt, &amp; \\text{if } c_i(t) &gt; 0 \\\\ 0, &amp; \\text{otherwise}\\\\  \\end{array}   \\right.\nSlope in a plot represents the rate of increase of a meme concentration\n\n\n\n Hyper-heuristics (I/II): Motivation / Characteristics / Classification /Misconceptions of Hyper-heuristics, Selection Hyper-heuristic controlling Perturbative Heuristics\n Hyper-heuristics\n\n\n\n\n\n\n\n\n\nA hyper-heuristic is a search method or learning mechanism for selecting or generating heuristics to solve computationally difficult problems\nA class of methodologies for cross domain (i.e. no domain knowledge) search\n\n Motivation\nraise the level of generality of search methods / cross domain research\n Characteristics of Hyper heuristics (initial framework)\n\nOperate on a search space of heuristics (neighborhood operators) rather than directly on a search space of solutions\nAim is to take advantage of strengths and avoid weaknesses of each heuristic (operator)\nNo problem specific knowledge is required during the search over the heuristics (operator) space\nEasy to implement, practical to deploy (easy, cheap, fast)\nExisting (or computer generated) heuristics (operators) can be used within hyper heuristics\n\n Classification of Hyper-heuristics\n\n\nGeneration hyper-heuristics\nautomatically construct new heuristics from given components\nSelection hyper-heuristics\nchoose and control a predefined set of heuristics\nOffline learning hyper-heuristics\n\nusually trained on a set of selected instances and then generalise to unseen instances\n\n\nOnline learning hyper-heuristics\n\nreceive feedback during the search process while solving a given instance of a problem\n\n\n\n Misconceptions about Hyper-heuristics\n\nHyper-heuristics do not require parameter tuning (âœ•)\n\nrequire parameter tuning (unless have a parameter control mechanism)\n\n\nHyper-heuristics are all tested under a fair setting (HyFlex) (âœ•)\n\nTime allocated (and instances used) for tuning - no one seems to take into account the time spent for parameter tuning, much time on tuning\n\n\nApplying a hyper heuristic to a new domain is easy (âœ•)\n\nthe hyper-heuristic part itself is easy, but decide who is going to implement these new operators in the new domain / choose of domain-specific operators/heuristics is hard\n\n\nDomain specific information should not be passed to the hyper-heuristics (objective value is not a domain specific information, all others are) (âœ•)\n\nobjective value is domain specific information, and, the hyper-heuristic, as an interface, should allow such information to be passed, and, with domain-specific information hyper-heuristic can be more capable\n\n\n\n\n Selection Hyper-heuristic controlling Perturbative Heuristics\n A Selection Hyper heuristic Framework - Single Point Search\n1234567generate initial candidate solution pwhile(termination critera not satisfied)&#123;\tselect a heuristic (or subset of heuristic) h from &#123;H1,..., Hn&#125;\tgenerate a new solution (or solutions) s by applying h to p\tif(s is accepted) tehn p = s; // decide whether to accept s or not&#125;return p;\n\n Hyper-heuristics Flexible Interface (HyFlex) - Multi-Point / Population Search\n\n\nHeuristic types: mutational (MU), ruin-recreate (RC), local search(HC), crossover (XO)\nParameters: intensity of mutation, depth of search\n\n Heuristic (Operator) selection methods\n\n\nSimple Random / Random Permutation - with no learning\n\n\nGreedy (GR) - with learning\n\nApply each low level heuristic to the candidate solution and choose the one that generates the best objective value\n\n\n\nReinforcement Learning (RL) - with learning\n\nA machine learning technique\nConcerned with how an agent ought to take actions in an environment to maximize some notion of long term reward\nreward and punishment\nMaintains a score for each heuristic\nIf an improving move then increase (e.g. +1) other wise decrease (e.g. - 1) the score of the heuristic\n\n\n\nChoice Function (CF) - with learning\n\nmaintains a record of the performance of each heuristic with 3 criteria\n\nits individual performance (f1f_1f1â€‹)\nhow well it has performed with other heuristics (f2f_2f2â€‹)\nthe elapsed time since the heuristic has been called (f3f_3f3â€‹)\n\n\nFn(hj)=Î±nf1(hj)+Î²nf2(hk,hj)+Î³nf3(hj)F_n(h_j) = \\alpha _n f_1(h_j) + \\beta _n f_2(h_k, h_j) + \\gamma _n f_3(h_j)Fnâ€‹(hjâ€‹)=Î±nâ€‹f1â€‹(hjâ€‹)+Î²nâ€‹f2â€‹(hkâ€‹,hjâ€‹)+Î³nâ€‹f3â€‹(hjâ€‹)\n\n\n\nAn Iterated Multi-stage Selection Hyper-heuristic\n\n\nCrossover operators are ignored\n\n\n\n\n\nStage 1 Hyper-heuristic (S1HH)\n\n\nselect a low level heuristic iii with probability scorei/âˆ‘âˆ€kscorekscore_i / \\sum _{\\forall k} score_kscoreiâ€‹/âˆ‘âˆ€kâ€‹scorekâ€‹\n\n\napply the chosen heuristic\n\n\nAccept/reject based on an adaptive threshold acceptance method\n\n\n\nStage 1 terminates if a duration of is exceeded without any improvement\n\n\n\n\nStage 2 Hyper-heuristic (S2HH)\n\nGiven NNN LLHs, pair up all (which increase the number of LLHs to N+N2N + N^2N+N2), then reduce the number of LLHs (N+N2â†’nN+N^2\\rightarrow nN+N2â†’n)  and assign probabilities\nparameter tuning needed (6 parameters for this specific framework)\n\n\n\n\n\n Hyper-heuristics (II/II): Selection Hyper-heuristic controlling Constructive Heuristics, Generation Hyper-heuristics (Genetic Programming (GP))\n Selection Hyper-heuristic controlling Constructive Heuristics\n A Graph-based Hyper-heuristic (GHH)\n\n\nGraph colouring problem - no two adjacent vertices share the same colour (vertex colouring)\n\n\nminimum colouring problem (an NP-hard problem)\n\n\ndegree of a vertex: number of edges connected to that vertex\n\n\nsaturation degree of a vertex: number of differently coloured vertices already connected to it\n\n(note : 1. coloured; 2.differently)\n\n\n\n\n\n Graph colouring heuristics\n\nlargest degree\n\nsort the vertices from largest degree to smallest\ncolour the first vertex in the list with an colour\n\n(starting with the first) that is different than its neighbours\n\n\ndelete the vertex from the list go to the previous step unless no vertices left\n\n\nsaturation degree\n\nuse saturation degree instead of degree in largest degree\n\n\n\n Case study: examination timetabling problem\n\n\nA general framework employing a set of low level constructive graph colouring heuristics\n\nLow level heuristics: sequential methods that order events by the difficulties of assigning them\n\n\n\nmodel the problem as a graph colouring problem\n\nNodes: exams\nEdges: adjacent exams have common students\nColours: time periods\nObjective: assign colours (time periods) to nodes (exams),adjacent nodes with different colour, minimising time periods used\n\n\n\nSelect low level heuristics\n\n\n\n\n\n\n\na specific pseudo-code (i.e. only one specific way) of Tabu Search graph based hyper-heuristic\n\n(note:â€˜eventsâ€™=â€˜examsâ€™)\n\n12345678910111213141516171819initial heuristic list hl = &#123;h1, h2, h3,..., hk&#125;// Begin of Tabu Searchfor i = 0 to i = (5 * the number of events) // number of iterations\th = change two heuristic in hl // a move in Tabu search\tif h does not match a heuristic list in &#x27;failed list&#x27;\t\tif h is not in the tabu list // h is not recently visited\t\tfor j = 0 to j = k // h is used to construct a complete solution\t\t\tschedule the first 2 events in the event list ordered using hj\t\t\tif no feasible solution can be obtained\t\t\t\tstore h into &#x27;failed list&#x27; // update &quot;failed list&quot;\t\t\telse if cost of solution c &lt; the best cost c_g obtained\t\t\t\tsave the best solution, c_g = c //keep the best solution\t\t\t\tadd h into the tabu list\t\t\t\tremove the first item from the tabu list if its length &gt; 9\t\thl = h\t//end if\tDeepest descent on the complete solution obtained//end of Tabu searchoutput the best solution with cost of c_g\n\nNeighborhood operator: randomly change two heuristics in the heuristic list\nObjective function: quality of solutions built by the corresponding heuristic list\nTabu list: visits to the same heuristic lists forbidden\n\n\n\nother high-level search strategies: steepest descent, Variable neighborhood search -&gt; best performing, iterated steepest descent, â€¦\n\n\n Generation Hyper-heuristics\n Genetic programming(GP)\n\n\nGP provides a method for automatically creating a working computer program from a high level problem statement of the problem\n\ni.e. program synthesis / program induction\n\n\n\nGP iteratively transforms a population of computer programs into a new generation of programs via evolutionary process\n\n\nrepresent a computer program as a parse tree\n\n\n\nGP is an evolutionary algorithm containing the same algorithmic components , including\n\nRandom generation of the initial population of possible solutions (programs)\n\nRandomly generate a program that takes two (or more) arguments and uses basic arithmetic to return an answer\n\nFunction set = {+,âˆ’,âˆ—/}\\{+, -, * /\\}{+,âˆ’,âˆ—/}\nTerminal set = {integers,X,Y}\\{integers, X, Y\\}{integers,X,Y}\n\n\nRandomly select either a function or a terminal to represent our program\nIf a function was selected, recursively generate random programs to act as arguments\n\n\nGenetic crossover of two promising solutions to create new possible solutions (programs)\n\nPick a random node in each program\nSwap the two nodes\n\n\nMutation of promising solutions to create new possible solutions (programs)\n\nFirst pick a random node\nDelete the node and its children, and replace with a randomly generated program\n\n\nFitness measure that is used to evaluate a given evolved program\n\nGP is often operated in a train and test fashion training on selected sample instances could take long time while application to unseen instances is generally fast\nEach tree generated by GP can be evaluated using an indicator showing how good it is in building high quality solutions to the sample problem instances, such as, mean quality of solutions over the sample instances\n\n\nTermination criteria\n\n\n\nCase study: Genetic Programming for Packing\n\nError: GP at a specific time (note: â€˜%â€™ should be â€˜/â€™)\n\n      \n\nNow\n\nNow putting the item of size 70 into the bin\ncalculate the fitness measure based on the program,\nfind the max fitness value\nNext put item of size 70 into bin 4\nNext start search for item of size 85\n\n\nafter putting all items in bins (i.e. the program finishes on the test instance)\n\nrecord number of bins used and compare with the best\nthen do crossover / mutation â€¦for new programs\n\n\n\n\n\n\n Advanced topics\n Case study: Policy Matrix Evolution for Generation of Heuristics on 1D Online Bin Packing Problem\n Index Policy\n\n\n\n\n\n\n\n\n\nEach choice option is given a score, or â€œindex valueâ€ independently of the other options\n\nindex policies for 1D online bin packing\n\nscore of bin is f(r,s)f(r, s)f(r,s) where rrr is the remaining capacity of bin and sss is item size\nGiven a new item of size then place into bin with largest value of f(r,s)f(r, s)f(r,s)\n\n\n\n Open / Close\n\nA new empty bin is always available ( open )\nA bin is closed if it can take no more items\n\ne.g. if residual space is smaller than size of any item\n\n\n\n Potential General Method for 1D Online Bin Packing\n\n\n(i.e. method to assign a new item upon its arrival to one of the open bins)\n\n\non arrival of new item, inspect the current set of open bins, and simultaneously use the entire set of residual spaces in the open bins to pick where to place the new item\n\n\ndifficult expensive (in general)\n\n\n Generating Heuristic\n\nWithin search methods, often have score functions, â€œindex functionsâ€ to help make some choice\n\ndifficult to invent successful ones; want to automate this\n\n\nGP approach: evolve arithmetic score functions\n\nUse Genetic Programming to learn f(r,s)f(r,s)f(r,s)\nf(r,s)f(r,s)f(r,s) is represented as arithmetic function tree\nAutomatically creates functions that at least match FF, BF\n\n\nChallenge:\n\nis hard to understand\npotentially biased because of the choice of representation\nsome perfectly good functions might have â€œbloatedâ€ representations\n\n\n\n Matrix View of Policies/Heuristics\n\n\nSince all item sizes (sss) and residual capacities (rrr) are integer,then f(r,s)f(r,s)f(r,s) is simply a large (Câˆ—CC*CCâˆ—C) matrix M(r,s)M(r,s)M(r,s)of parameter values\n\n\n\nUniform (random) Instances\n\nproblem generator on Uniform Bin Packing Problems: UBP(C,smin,smax,N)UBP(C,s_{min}, s_{max}, N )UBP(C,sminâ€‹,smaxâ€‹,N)\nCCC - bin capacity\nNNN number of items with integer sizes taken randomly from range [smin,smax][s_{min}, s_{max}][sminâ€‹,smaxâ€‹]\n\n\n\nCreating Heuristics via Many Parameters (CHAMP)\n\n\nBasic idea:\n\nTake values in matrix M(r,s)M(r,s)M(r,s) to be integers\nDo (meta-)heuristic search to find good choices for M(r,s)M(r,s)M(r,s): Evaluation is by simulation\n\n\n\nOriginal Expectation\n\nthe matrix will tweak the functions from GP and might slightly improve performance\n\n\n\nPotential expected disadvantages\n\nmatrices can be much more verbose than functions\nthey fail to take into account of the good structure captured by functions\n\n\n\n\n\n\nConclusion\n\nPolicies exist that out-perform standard heuristics\nFinding the policies is easier than expected\nThere are many different policies with similar performance\nThe policies are â€œweirderâ€ than expected\n\nThe good policies could have â€œrandomâ€ structures\nNot necessarily easy to capture with an algebraic function of GP\n\n\nThe results can be â€œanalysedâ€ (inspected) to produce simple policies that out-perform standard ones\n\nand that then scale to larger problems\n\n\n\n\n\n A Data Science Improved Hyper-heuristic\n\n\n\n\n\n\n\n\n\nMany real-world data are multidimensional\n\nVery high-dimensional (big) with a large amount of redundancy\n\n Proposed Approach â€“ Ideas\n\nThe balance between exploration and exploitation is crucial\nMix move acceptance methods\nUse machine learning to partition the low level heuristics associated with each method\n\n Summary\n\nHyper-heuristic research originated from a job shop scheduling application and has been rapidly growing since then.\nGeneration hyper-heuristics are commonly used in the area\n\nTrain and test fashion\n\nDoes the selected subset of training instances is sufficiently representative of the test set?\nTraining is time-consuming (delta/incremental evaluation, surrogate functions)\n\n\nThe generated/evolved heuristics might not be easy to interpret, yet they can outperform human designed heuristics\n\n\nThere is empirical evidence that machine learning/analytics/ data science help to improve the hyper-heuristic search process\n\nProblem features vs solution/state features\nOffline versus online learning â€“ Life long learning\n\n\nThere is still a lack of benchmarks\nAutomated design of search methodologies is extremely challenging\n\nAddressed in almost complete absence of a mathematical and theoretical understanding\n\n\n\n","slug":"aim","date":"2024-06-09T22:37:56.000Z","categories_index":"Notes","tags_index":"AI,Heuristics,Optimization","author_index":"Huaji1hao"},{"id":"e36df18b7817d5ce63296b0aa4c51d03","title":"Algorithms, Data structure & Efficiency","content":" Algorithms, Data structure &amp; Efficiency\n\nAlgorithm: step-by-step procedure for solving a problem in a finite amount of time.\n\nNo â€œMAGIC stepsâ€ allowed! (No â€œZeno machinesâ€)\n\n\nData structures: lists, binary trees, heaps, Hash maps, graphs, etc.\nEfficiency: We will start with methods to analyse, classify and describe the efficiency of algorithms\n\nNeeded in order to be able to select â€œbest algorithmsâ€\n\n\n\n Big-Oh family\n Big-Oh Notation\n\n\n\n\n\n\n\n\n\nDefinition: Given positive functions f(n)f(n)f(n) and g(n)g(n)g(n), then we say that\nf(n)f(n)f(n) is O(g(n))O( g(n) )O(g(n))\nif and only if there exist positive constants ccc and n0n_0n0â€‹ such that\nf(n)â‰¤cg(n)f(n)\\leq c g(n)f(n)â‰¤cg(n) for all nâ‰¥n0n \\geq n_0nâ‰¥n0â€‹\n\n\ni.e. â€œexists-exists-forallâ€ structure:\n\nâˆƒc&gt;0,âˆƒn0,suchâ€…â€Šthatâ€…â€Šâˆ€nâ‰¥n0,f(n)â‰¤cg(n)\\exist c&gt;0,\\exist n_0,such \\;that\\;\\forall n \\geq n0, f(n) \\leq c g(n)âˆƒc&gt;0,âˆƒn0â€‹,suchthatâˆ€nâ‰¥n0,f(n)â‰¤cg(n)\n\n\n\nâ€˜OOOâ€™ expresses â€œgrows at most as fast asâ€\n\n\n Big-Oh as a set\n\nBig Oh as a binary relation is reflexive and transitive but not symmetric\n\nIt behaves like â€œâŠ†âŠ†âŠ†â€, â€œâˆˆâˆˆâˆˆâ€ or â€œâ‰¤â‰¤â‰¤â€, not like â€œ===â€\nOne might say nâˆˆO(n)n âˆˆO(n)nâˆˆO(n), and 2n+3âˆˆO(n)2n+3 âˆˆO(n)2n+3âˆˆO(n), etc\n\n\n\nSo may help to think of â€œO(n)O(n)O(n)â€ as a set of functions, with each function f in the set, fâˆˆO(n)f âˆˆO(n)fâˆˆO(n), satisfying â€œfff is O(n)O(n)O(n)â€.\n\nOr can say: {f}âŠ†O(n)\\{f\\} âŠ†O(n){f}âŠ†O(n)\nSo then O(1)âŠ†O(n)O(1) âŠ†O(n)O(1)âŠ†O(n)\n\n\n Rules for Finding big-Oh\n â€œMultiplication Ruleâ€ for big-Oh\n\nSuppose\n\nf1(n)f_1(n)f1â€‹(n) is O(g1(n))O( g_1(n ) )O(g1â€‹(n))\nf2(n)f_2(n)f2â€‹(n) is O(g2(n))O( g_2(n) )O(g2â€‹(n))\n\n\nThen, from the definition, there exist positive constants c1,c2,n1,n2c_1,c_2,n_1,n_2c1â€‹,c2â€‹,n1â€‹,n2â€‹ such that\n\nf1(n)â‰¤c1g1(n)f_1(n) \\leq c_1g_1(n)f1â€‹(n)â‰¤c1â€‹g1â€‹(n) for all nâ‰¥n1n â‰¥ n_1nâ‰¥n1â€‹\nf2(n)â‰¤c2g2(n)f_2(n) \\leq c_2g_2(n)f2â€‹(n)â‰¤c2â€‹g2â€‹(n) for all nâ‰¥n2n â‰¥ n_2nâ‰¥n2â€‹\n\n\nLet n0=max(n1,n2)n_0= max(n1, n2)n0â€‹=max(n1,n2), then multiplying gives\nf1(n)f2(n)â‰¤c1c2g1(n)g2(n)f_1(n) f_2(n) \\leq c_1 c_2g_1(n) g_2(n)f1â€‹(n)f2â€‹(n)â‰¤c1â€‹c2â€‹g1â€‹(n)g2â€‹(n) for all nâ‰¥n0n â‰¥ n_0nâ‰¥n0â€‹\nSo f1(n)f2(n)f_1(n) f_2(n)f1â€‹(n)f2â€‹(n) is O(g1(n)g2(n))O( g_1(n) g_2(n) )O(g1â€‹(n)g2â€‹(n))\n\n Big-Oh Rules: Drop smaller terms\n\nIf f(n)=(1+h(n))f(n)= ( 1 + h(n) )f(n)=(1+h(n)) with h(n)â†’0h(n) â†’0h(n)â†’0 as nâ†’âˆn â†’âˆnâ†’âˆ\nThen f(n)f(n)f(n) is O(1)O(1)O(1)\n(The utility will be to combine with the multiplication rule).\n\n Summary for finding big-Oh\nIf f(n)f(n)f(n) is a polynomial of degree ddd, (with positive largest term) then f(n)f(n)f(n) is O(nd)O(n^d)O(nd), i.e.,\n\nDrop lower-order terms\nDrop constant terms\n\nNote: degree of a polynomial is the highest power e.g. 5n4+3n25 n^4+ 3 n^25n4+3n2 is degree 444 and so will be O(n4)O(n^4)O(n4)\n Big-Omega\n\n\n\n\n\n\n\n\n\nDefinition: Given functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is Î©(g(n))Î©( g(n) )Î©(g(n))\nif there are (strictly) positive constants ccc and n0n_0n0â€‹ such that\nf(n)â‰¥cg(n)f(n)â‰¥c g(n)f(n)â‰¥cg(n) for all nâ‰¥n0n \\geq n_0nâ‰¥n0â€‹\n\n\nNote that need c&gt;0c &gt; 0c&gt;0, and we not allowed c=0c=0c=0\n\n\nNote that ccc must be constant (cannot depend on nnn)\n\n\nf(n)f(n)f(n) is Î©(g(n))Î©(g(n))Î©(g(n)) says that â€œf(n)f(n)f(n) grows at least as fast as g(n)g(n)g(n) at large nnnâ€\n\n\n Linking big-Oh and Big-Omega\n\nThat is: ğ‘“âˆˆğ‘‚(ğ‘”)â†’ğ‘”âˆˆÎ©(f)ğ‘“âˆˆğ‘‚(ğ‘”)â†’ğ‘”âˆˆÎ©(f)fâˆˆO(g)â†’gâˆˆÎ©(f)\nSimilarly: ğ‘“âˆˆÎ©(ğ‘”)â†’ğ‘”âˆˆO(f)ğ‘“âˆˆÎ©(ğ‘”)â†’ğ‘”âˆˆO(f)fâˆˆÎ©(g)â†’gâˆˆO(f)\nNote: similar to: x&lt;=yâ†’y&gt;=xx &lt;= y â†’y &gt;= xx&lt;=yâ†’y&gt;=x\n\n Big-Theta\n\n\n\n\n\n\n\n\n\nDefinition: Given functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is Î˜(g(n))Î˜( g(n) )Î˜(g(n))\nif there are positive constants câ€™câ€™câ€™, câ€™â€™c â€™â€™câ€™â€™ and n0n_0n0â€‹ such that\nf(n)â‰¤câ€™g(n)â‰¤câ€™â€™g(n)f(n)â‰¤câ€™g(n) \\leq c â€™â€™g(n)f(n)â‰¤câ€™g(n)â‰¤câ€™â€™g(n)\nfor all nâ‰¥n0n \\geq n_0nâ‰¥n0â€‹\n\nÎ˜Î˜Î˜ expresses â€œgrows â€˜exactlyâ€™ as fast as\n\n Î˜ is an equivalence relation\nAny relation that is Reflexive &amp; Symmetric &amp; Transitive is an â€œequivalence relationâ€\n\nRoughly speaking: it behaves like a â€œequalityâ€:\n\nIt is reasonable to write â€œğ‘“=Î˜(ğ‘”)ğ‘“=Î˜(ğ‘”)f=Î˜(g)â€\n Little-Oh\n\n\n\n\n\n\n\n\n\nDefinition: Given (positive) functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is o(g(n))o( g(n) )o(g(n))\nif for all positive (real) constants c&gt;0c &gt; 0c&gt;0\nthere exists n0n_0n0â€‹ such that\nf(n)&lt;cg(n)f(n)&lt;c g(n)f(n)&lt;cg(n) for all nâ‰¥n0n  \\geq n_0nâ‰¥n0â€‹\n\n\nSpot the difference from big-Oh?\n\nâ€œfor all c&gt;0c &gt; 0c&gt;0â€ rather than â€œthere exists c&gt;0c &gt; 0c&gt;0â€\n(The change of â€œâ‰¤â‰¤â‰¤â€ toâ€œ&lt;&lt;&lt;â€œ is much less important â€“ see later.)\n\n\n\nSays (roughly) that the ratio f(n)/g(n)â†’0f(n)/g(n) â†’0f(n)/g(n)â†’0 as nâ†’âˆn â†’\\infinnâ†’âˆ\n\n\nNote that n0n_0n0â€‹ is allowed to depend on ccc\n\n\nooo expresses â€œstrictly less thanâ€\n\n\n\n Linked list\n Singly Linked List\n\n\n\n\n\n\n\n\n\nA singly linked list is a concrete data structure consisting of a sequence of nodes\nEach node stores\n\nelement e.g.\n\nReference to an Object\nA primitive date type (int,â€¦)\n\n\nâ€œlinkâ€: a reference (pointer) to the next node\n\n\n Inserting at the Head\n\n\nAllocate a new node\n\n\nInsert new element\n\n\nHave new node point to old head\n\n\nUpdate head to point to new node\n\n\nWhat is the complexity (with n elements in list)?\n\nAnswer: O(1)\nVery efficient!\n\n Removing at the Head\n\nUpdate head to point to next node in the list\nAllow garbage collector to reclaim the former first node\n\nOr do explicit free in C/C++\n\n\n\nAgain, the operation is O(1), and so efficient.\n Inserting at the Tail\n\nAllocate a new node\nInsert new element\nHave new node point to null\nHave old last node point to new node\nUpdate tail to point to new node\n\nComplexity: O(1)\n Removing at the Tail\nTo find new tail we must walk the list from the head\n\nThere is no constant-time way to update the tail to point to the previous node\n\nComplexity: O(n)\n Doubly Linked List\n\n\n\n\n\n\n\n\n\nA doubly linked list provides a natural implementation of a List\n\nNodes implement â€œPositionâ€ and store:\n\nelement\nlink to the next node\nlink to the previous node\n\n\nDeletion at the tail is now O(1)\nBut uses more memory\n\n\n Insertion Algorithm\n12345678Algorithm addAfter(p,e):    Create a new node v    v.setElement(e)    v.setPrev(p) //link v to its predecessor    v.setNext(p.getNext()) //link v to its successor    (p.getNext()).setPrev(v) //link pâ€™s old successor to v    p.setNext(v) //link p to its new successor, v    return v //the position for the element e\n\n Simple Sorting Algorithm\n Bubble Sort\n Basic Idea\n\nOuter loop:\n\nRepeated scans through array\n\n\nInner loop: on each scan do comparison with immediate neighbour\n\nthink of air bubbles rising in water\ndo swaps to make sure that the largest number â€œbubbles upâ€ to the end of the array\n\n\n\n Pseudocode\n12345678910111213void bubbleSort(int arr[])&#123;    int i;    int j;    int temp;    for(i= arr.length - 1; i &gt; 0; i--)&#123;        for(j = 0; j &lt; i; j++)&#123;            if(arr[j] &gt; arr[j+1])&#123;            temp = arr[j];            arr[j] = arr[j+1];            arr[j+1] = temp;        &#125;// end inner loop    &#125;//end outer loop&#125;// end bubble sort\n Complexity of bubble sort\nAll together: t((nâˆ’1)+(nâˆ’2)+...+1)+k+t1(nâˆ’1)t ((n-1) + (n-2) + ... + 1)+ k + t_1(n-1)t((nâˆ’1)+(nâˆ’2)+...+1)+k+t1â€‹(nâˆ’1)\n\n\nwhere ttt is the time required to do one comparison, one swap, check the inner loop condition and increment jjj.\n\n\nWe also spend constant time kkk declaring i,j,tempi,j,tempi,j,temp and initialising iii. Outer loop is executed nâˆ’1n-1nâˆ’1 times, suppose the cost of checking the loop condition and decrementing iii is t1t_1t1â€‹.\n\n\n(worst-case) complexity O(n2)O(n^2)O(n2) by taking c=t+t1+kc = t + t_1+ kc=t+t1â€‹+k and n0=1n_0= 1n0â€‹=1.\n\n\n Selection Sort\n Basic Idea\nInstead of always try to move the â€œgreatest element so farâ€ immediately, we just remember its location and move it at end of scan\n Why one want to do this?\n\nSuppose that the entries are large then a swap operation might be quite expensive\nSo might want to reduce the number of swaps by directly moving entries to â€œthe right placeâ€\n\n Pseudocode\n123456789101112131415void selectionSort(int arr[])&#123;    int i, j, temp, pos_greatest;    for( i = arr.length-1; i &gt; 0; i--)&#123;        pos_greatest = 0;        for(j = 0; j &lt;= i; j++)&#123;        \tif( arr[j] &gt;= arr[pos_greatest])        \t\tpos_greatest = j;        &#125;//end inner for loop        if ( i != pos_greatest ) &#123;            temp = arr[i];            arr[i] = arr[pos_greatest];            arr[pos_greatest] = temp;         &#125;    &#125;//end outer for loop&#125;//end selection sort\n Complexity of selection sort\nCompared to bubble sort:\n\nSame number of iterations\nSame number of comparisons in the worst case\nfewer swaps (one for each outer loop = n-1)\nHence, also O(n2)O(n^2)O(n2)\n\n Insertion Sort\n Basic Idea\nKeep the front of the list sorted, and as we move through the back, elements we insert them into the correct place in the front\n Pseudocode\n1234567891011void insertionSort(int arr[])&#123;for(int j = 1; j &lt; arr.length; j++)&#123;    int temp = arr[j];    int i = j; // range 0 to j-1 is sorted    while(i &gt; 0 &amp;&amp; arr[i-1] &gt; temp)&#123;        arr[i] = arr[i-1];        i--;    &#125;    arr[i] = temp;    &#125; // end outer for loop&#125; // end insertion sort\n Complexity of insertion sort\n\nIn the worst case, has to make n(nâˆ’1)/2n(n-1)/2n(nâˆ’1)/2 comparisons and shifts to the right\nalso O(n2)O(n^2)O(n2) worst case complexity\nBest case: array already sorted\n\nBackwards walk of inner loop stops immediately; no shifts.\nBecomes O(n)O(n)O(n)\n\n\n\n Adaptive Sort\nThis is a (bizarre) name for what asking what happens to the complexity when the lists are already â€œnearly sortedâ€.\nIn many applications lists might already be close to being sorted.\n\nE.g. maybe they were from a list that was sorted and then some corrections were made.\nIt is then natural to ask, for each algorithm, whether the efficiency improves.\n\n Sorting â€œStabilityâ€\n\n\ncompare(o1,o2)==0 means objects o1 and o2 are\n\nequal with respect to the desired ordering\nbut not necessarily that they have the same contents\n\ni.e. are not identical !\n\n\n\n\n\nIf sorting a spreadsheet, then might sort by one column then another.\n\n\nDo not want the sorting to unnecessarily change the order of the rows, as this can be annoying and confusing.\n\n\nâ€œSort by column A, followed by a stable sort on column Bâ€ means that still will have a secondary sort on column A\n\n\n Sorting on Lists\n\n\nBubble sort is just as efficient (or rather inefficient) on linked lists.\n\nWe can easily bubble sort even for a singly linked list.\n\n\n\nSelection sort on linked lists implementation similar to bubble sort; also O(n2)O(n^2)O(n2)\n\n\nInsertion sort is a suitable sorting method (only) for doubly linked lists â€“ as need to walk backwards\n\n\n\n Tree\n Tree Terminology\n\n\nRoot: node without parent (A)\n\n\nInternal node: node with at least one child (A, B, C, F)\n\n\nExternal node (a.k.a. leaf ): node without children (E, I, J, K, G, H, D)\n\n\nAncestors of a node: parent, grandparent, grand-grandparent, etc.\n\n\nDepth of a node: number of ancestors (not counting itself)\n\n\nHeight of a tree: maximum depth of any node = length of longest path from root to a leaf\n\nHeight of tree below = 3\n\n\n\nDescendant of a node: child, grandchild, grand-grandchild, etc.\n\n\nSubtree: tree consisting of a node and its descendants\n\n\n\n Traversals\nGiven a data structure, a common task is to traverse all elements\n\nvisit each element precisely once\nvisit in some systematic and meaningful order\nNote â€œvisitâ€ means â€œprocess the contentsâ€ but does not include just â€œpassing through using the linksâ€\n\n Preorder Traversal\n\nIn a preorder traversal, a node is visited before its descendants\nApplication: print a structured document\n\n1234Algorithm preOrder(v)    visit(v)    for each child w of v        preorder (w)\n Postorder Traversal\n\nIn a postorder traversal, a node is visited after its descendants\nApplication: compute space used by files in a directory and its subdirectories\n\n1234Algorithm postOrder(v)    for each child w of v    \tpostOrder (w)    visit(v)\n Inorder Traversal\n\n\nIn an inorder traversal a node is visited after its left subtree and before its right subtree\n\n\nApplication: draw a binary tree by (x,y) coords:\n\nx(v) = inorder rank of v\ny(v) = depth of v\n\n\n\n123456Algorithm inOrder(v)    if hasLeft (v)    \tinOrder (left (v))    visit(v)    if hasRight (v)    \tinOrder (right (v))\n Binary Trees\n Properties\n\nEach internal node has at most two children\nThe children of a node are an ordered pair - though one might be â€œmissingâ€\n\n Consensus\n\nWe call the children of an internal node left child and right child\nAlternative recursive definition: a binary tree is either\n\na tree consisting of a single node, or\na tree whose root has an ordered pair of â€œchildrenâ€, each of which is missing (a null) or is the root of a binary tree\n\n\nApplications: searching\n\n\n Proper Binary Trees\n Properties\n\nEach node has either two children or no children\nThe children of a node are an ordered pair\n\n Applications:\n\narithmetic expressions\ndecision processes\n\n\n Application details\n\n Print Arithmetic Expressions\nSpecialization of an inorder traversal\n\n\nprint operand or operator when visiting node\n\n\nprint â€œ(â€ before traversing left subtree\n\n\nprint â€œ)â€ after traversing right subtree\n\n\n1234567Algorithm printExpression(v)    if hasLeft (v) print(&quot;(&quot;)        printExpression (left(v))        print(v.element ())    if hasRight (v)        printExpression(right(v))        print (&quot;)&quot;)\n Evaluate Arithmetic Expressions\nSpecialization of a postorder traversal:\n\nrecursive method returning the value of a subtree\nwhen visiting an internal node, combine the values of the subtrees\n\n12345678Algorithm evalExpr(v)    if isExternal (v)        return v.element ()    else        x â† evalExpr(leftChild (v))        y â† evalExpr(rightChild (v))        â—Š â† operator stored at v    return x â—Š y\n Abstract Data Types (ADTs)\n\n\n\n\n\n\n\n\n\nAn abstract data type (ADT) is an abstraction of a data structure\nAn ADT specifies:\n\nData stored\nOperations on the data\nError conditions associated with operations\n\nAn ADT does not specify the implementation itself -hence â€œabstractâ€\n Example: ADT modeling a simple stock trading system\n\nThe data stored are buy/sell orders\nThe operations supported are\n\norder buy(stock, shares, price)\norder sell(stock, shares, price)\nvoid cancel(order)\n\n\nError conditions:\n\nBuy/sell a non existent stock\nCancel a non existent order\n\n\n\n Concrete Data Types (CDTs)\n\n\n\n\n\n\n\n\n\nThe actual date structure that we use\nAn ADT might be implemented using different choices for the CDT\n\nThe choice of CDT will not be apparent from the interface: â€œdata hidingâ€ â€œencapsulationâ€\n\ne.g. see â€˜Object Oriented Methodsâ€™\n\n\nThe choice of CDT will affect the runtime and space usage\n\n ADT &amp; Efficiency\n\n\nOften the ADT comes with efficiency requirements expressed in big-Oh notation, e.g.\n\n\nâ€œcancel(order) must be O(1)â€\n\n\nâ€œsell(order) must be O(log( |orders| ) )â€\n\n\n\n\nHowever, such requirements do not automatically force a particular CDT.\n\nThe underlying implementation is still not specified\n\n\n\nThis is typical of many â€œlibrary functionsâ€\n\n\n Tree ADT\nWe can use â€œpositionsâ€, p, to abstract nodes\n Generic methods:\n\ninteger size()\nboolean isEmpty()\nIterator iterator()\nIterator positions()\n\n Accessor methods:\n\nposition root()\nposition parent(p)\nIterator children(p)\n\n Query methods:\n\nboolean isInternal(p)\nboolean isExternal(p)\nboolean isRoot(p)\n\n Update method:\n\n\nobject replace (p, o)\n\n\nAdditional update methods may be defined by data structures implementing the Tree ADT\n\n\n Array-Based Representation of Binary Trees\nlet rank(node) be defined as follows:\n\n\nrank(root) = 1\n\n\nif node is the left child of parent(node),\nrank(node) = 2*rank(parent(node))\n\n\nif node is the right child of parent(node),\nrank(node) = 2*rank(parent(node))+1\n\n\n Implementation\nRemember that if think of the rank, r(n) of node n, as a binary number then\n\n\nr(n) = r(par(n)) &lt;&lt; 1 + 0 for left\n\n\nr(n) = r(par(n)) &lt;&lt; 1 + 1 for right\n\n\nE.g. r(par(n)) = 101 gives children at\n\nâ€œ101â€+â€0â€ = 1010\nâ€œ101â€+â€œ1â€ = 1011\n\n\n\nGoing to the parent is a right shift\n\nr(par(n)) = r(n) &gt;&gt; 1\n\n Advantages of the tree-as-array structure:\n\nSaves space as do not have to store the pointers â€“ they are replaced by fast computations\nThe storage can be more compact â€“â€œbetter memory localityâ€ and this can be good because of cache and memory hierarchies, when an array element is accessed then other entries can be pulled into the cache, and so access becomes faster.\n\n Perfect binary trees\n Properties\n\n\nA binary tree is said to be â€œproperâ€ (a.k.a. â€œfullâ€) if every internal node has exactly 2 children\n\n\nIt is â€œperfectâ€ if it is proper and all leaves are at the same depth; hence all levels are full.\n\n\n\nCounting suggests numbers of nodes are:\n\n2d2^d2d at level ddd\n2d+1âˆ’12^{d+1}-12d+1âˆ’1 at level ddd or less\n\nHeight (hhh) is logarithmic in size (nnn)\n\n\nheight h=log2(n+1)âˆ’1h = log_2 (n + 1) - 1h=log2â€‹(n+1)âˆ’1 where nnn in the number of nodes.\n\n\nnumber of levels: h+1=log2(n+1)h + 1 = log_2 (n + 1)h+1=log2â€‹(n+1)\n\n\nnodes n=2h+1âˆ’1n = 2^{h+1} - 1n=2h+1âˆ’1\n\n\nFor a general binary tree on nnn nodes, the height is Î©(log(n))\\Omega(log(n))Î©(log(n)) and O(n)O(n)O(n)\n Merge Sort\n Divide-and-Conquer\nDivide-and-conquer is a general algorithm design paradigm:\n\nDivide: divide the input data SSS in two disjoint subsets S1S_1S1â€‹ and S2S_2S2â€‹\nRecur: solve the subproblems associated with S1S_1S1â€‹ and S2S_2S2â€‹\nConquer: combine the solutions for S1S_1S1â€‹ and S2S_2S2â€‹ into a solution for SSS\n\n Merge-Sort\nMerge-sort on an input sequence (array/list) SSS with nnn elements consists of three steps:\n\nDivide: partition SSS into two sequences S1S_1S1â€‹ and S2S_2S2â€‹ of about n/2n/2n/2 elements each\nRecur: recursively sort S1S_1S1â€‹ and S2S_2S2â€‹\nConquer: merge S1S_1S1â€‹ and S2S_2S2â€‹  into a unique sorted sequence\n\n Implementation\n1234567891011public void recMergeSort (int[] arr, int[] workSpace, int l, int r) &#123;    if (l == r) &#123;    \treturn;    &#125; else &#123;        int m = (l+r) / 2;        recMergeSort(arr, workSpace, l, m);        recMergeSort(arr, workSpace, m+1, r);        merge(arr, workSpace, l, m+1, r);    &#125;&#125;// Initial call is with l=0 and r to be end of the array\n Merge-Sort Tree\nAn execution of merge-sort is depicted by a binary tree\n\neach node represents a recursive call of merge-sort and stores\n\nunsorted sequence before the execution and its partition\nsorted sequence at the end of the execution\n\n\nthe root is the initial call\nthe leaves are calls on subsequences of size 0 or 1\n\n\n Analysis of Merge-Sort\nThe height hhh of the merge-sort tree is O(logn)O(log n)O(logn)\n\nat each recursive call we divide in half the sequence,\n\nThe overall amount of work done at all the nodes at depth iii is O(n)O(n)O(n)\n\nwe partition and merge 2i2^i2i sequences of size n/2in/2^in/2i\nwe make 2i+12^{i+1}2i+1 recursive calls\nthe numbers all occur and are all â€œusedâ€ at each depth\nSo, each depth uses O(n)O(n)O(n) work\n\nThus, the total running time of merge-sort is O(nlogn)O(nlog n)O(nlogn)\n\n Summary\n\nFast sorting method for arrays\nGood for sorting data in external memory â€“ because works with adjacent indices in the array (data access is sequential)\n\nIt accesses data in a sequential manner (suitable for sorting data on a disk)\n\n\nNot so good with lists: relies on constant time access to the middle of the sequence\n\n\n Recurrence Relations\n Example\n\n\nHow would we solve T(n)=2â€‰T(n/2)+bT(n) = 2\\,T(n/2) + bT(n)=2T(n/2)+b with T(1)=1T(1)=1T(1)=1\n\n\nWe know T(1)=1T(1)=1T(1)=1, hence\n\nT(2)=2â€‰T(1)+b=2+bT(2) = 2\\,T(1) + b = 2 + bT(2)=2T(1)+b=2+b\nT(4)=2â€‰T(4/2)+b=2â€‰(2+b)+b=4+(2+1)bT(4) = 2\\,T(4/2) + b = 2\\,( 2 + b) + b = 4 + (2+1)bT(4)=2T(4/2)+b=2(2+b)+b=4+(2+1)b\nT(8)=2â€‰(4+(2+1)b)+b=8+(4+2+1)bT(8) = 2\\,(4 + (2+1)b) + b = 8 + (4+2+1) bT(8)=2(4+(2+1)b)+b=8+(4+2+1)b\n\n\n\nIt seems a good guess\nT(2k)=2k+(2(kâˆ’1)+â€¦+1)bT(2k) = 2^k+ (2^{(k-1)}+â€¦+1)bT(2k)=2k+(2(kâˆ’1)+â€¦+1)b\n=2k+(2kâˆ’1)b= 2^k+ (2^k-1) b=2k+(2kâˆ’1)b\n\n\nSo T(n)=n+(nâˆ’1)b=(1+b)nâˆ’bT(n) = n+(n-1) b = (1+b)n-bT(n)=n+(nâˆ’1)b=(1+b)nâˆ’b for nnn in {1,2,4,8â€¦}\\{1,2,4,8â€¦\\}{1,2,4,8â€¦}\n\n\nStill Î˜(ğ‘›)Î˜(ğ‘›)Î˜(n)\n\n\nClaim: T(2k)=2k+(2kâˆ’1)bT(2^k) = 2^k+ (2^k-1) bT(2k)=2k+(2kâˆ’1)b\n\n\nProof by induction:\n\n\nBase case: k=0,T(1)=1+(1âˆ’1)âˆ—b=1k=0, T(1) = 1 + (1-1)*b = 1k=0,T(1)=1+(1âˆ’1)âˆ—b=1\n\n\nStep case: assume true at kkk\n\n\n$T(2^{k+1}) = 2 T(2^k) + b $\n=2(2k+(2kâ€“1)b)+b= 2 (2^k+ (2^kâ€“1) b ) + b=2(2k+(2kâ€“1)b)+b\n=2k+1+(2k+1âˆ’2+1)b= 2^{k+1}+ (2^{k+1}-2 + 1) b=2k+1+(2k+1âˆ’2+1)b\n=2k+1+(2k+1âˆ’1)b= 2^{k+1}+ (2^{k+1}-1) b=2k+1+(2k+1âˆ’1)b\n\n\nQED.\n\n\n Master Theorem (MT)\nFor a given recurrence of the form T(n)=aâ‹…T(n/b)+f(n)T(n)=a \\cdot T(n/b) +f(n)T(n)=aâ‹…T(n/b)+f(n) the M.T. can tell us the growth rate of T(n)T(n)T(n) according to three cases:\n\n\nCase 1: Recurrence dominates (plus special case that f(n)=0f(n)=0f(n)=0)\nIF f(n)f(n)f(n) is O(nc)O(n^c)O(nc) with c&lt;logbac&lt;log_bac&lt;logbâ€‹a,\nTHEN T(n)T(n)T(n) is Î˜(nlogba)\\Theta(n^{log_ba})Î˜(nlogbâ€‹a)\n\n\nCase 2: Neither term dominates\nIF f(n)f(n)f(n) is Î˜(nc(logâ€‰n)k)\\Theta(n^c(log\\,n)^k)Î˜(nc(logn)k) with c=logbac = log_bac=logbâ€‹a and kâ‰¥0k \\geq 0kâ‰¥0,\nTHEN T(n)T(n)T(n) is Î˜(nc(logâ€‰n)k+1)\\Theta(n^c(log\\, n)^{k+1})Î˜(nc(logn)k+1)\n\n\nCase 3: f(n)f(n)f(n) dominates\nIF f(n)f(n)f(n) is Î©(nc)\\Omega(n^c)Î©(nc) with c&gt;logbac &gt; log_bac&gt;logbâ€‹a,\nTHEN T(n)T(n)T(n) is Î˜(f(n))\\Theta(f(n))Î˜(f(n))\n\n\n Quicksort\nQuick-sort is a (randomized) sorting algorithm based on the divide-and-conquer paradigm:\n\nDivide: pick an element xxx(called pivot) and partition SSS into\n\nLLL: elements less than xxx\n\nHave to be careful it is not empty\n\n\nGEGEGE: elements greater than or equal to xxx\nPivot is often picked as a random element\n\n\nRecur: sort LLL and GEGEGE\nConquer: join LLL, GEGEGE\n\n â€œIn-placeâ€ or â€œextra workspaceâ€?\n\nFor sorting algorithms (and algorithms in general) an important issue can be how much extra working space they need besides the space to store the input\nâ€œIn-placeâ€ means they only a â€œlittleâ€ extra space (e.g. O(1)) is used to store data elements.\n\nThe input array is also used for output, and only need a few temporary variables\nbubble-sort is â€œin-placeâ€\nPrevious â€œmergeâ€ used extra O(n) array\n\n(can be made in-place, but messy and so we ignore this option)\n\n\n\n\n\n Implementation\n12345678public void recQuickSort(int[] arr, intleft, intright) &#123;    if (right - left &lt;= 0) return;    else &#123;        intborder = partition(arr, left, right); // pivot position        recQuickSort(arr, left, border-1);        recQuickSort(arr, border+1, right);    &#125;&#125;\n Quick-Sort Tree (3-way split)\n\n Worst-case Running Time\nThe worst case for quick-sort occurs when the pivot is the unique minimum or maximum element\n\nOne of LLL and E+GE+GE+G has size nâˆ’1n -1nâˆ’1 and the other has size 111\nThe running time is proportional to the sum\nn+(nâˆ’1)+â€¦+2+1n+(n-1) + â€¦ +2 + 1n+(nâˆ’1)+â€¦+2+1\nThus, the worst-case running time of quick-sort is O(n2)O(n^2)O(n2)\n\n Best-case  and average-case Running Time\nThe best case for quick-sort occurs when the pivot is the median element\n\nThe LLL and GGG parts are equal â€“the sequence is split in halves, like in merge sort\nThus, the best-case running time of quick-sort is O(nlogn)O(n log n)O(nlogn)\n\nThe average case for quick-sort: half of the times, the pivot is roughly in the middle\n\nThus, the average-case running time of quick-sort is O(nlogn)O(n log n)O(nlogn) again\n\n Motivations for quicksort\nit can be done â€œin-placeâ€\n\nUses a small amount of workspace\nBecause the â€œmergeâ€ step is now a lot easier!!\n\nThe â€œsplitâ€ is more complicated, and the merge â€œmuchâ€ easier â€“ but turns out that the quick-sort split is easier to do in-place than the merge-sort merge\n Comparison-based sorting\nAll the algorithms we have seen so far are comparison based\n\nIf you inspect the code then the decision as to swaps\n\netc. is based only of statements like â€œif ( A[i] &lt;= A[j] )â€\n\n\n\nNot all sorts are comparison-based:\n\nbucket sort: used the actual values to rearrange the items\nRuns in O(n), but relies on knowing the range of values in the sequence\n\n(e.g.â€œintegers between 1 and 1000â€).\n\n\n\n\n\n\n\n\n\n\n\n\nComparison-based sorting cannot do better than O(nlogn)O(n log n)O(nlogn)\n\n Vector\n Vector ADT\nAn element can be accessed, inserted or removed by specifying its rank\nMain vector operations:\n\nobject elemAtRank(integer r):\n\nreturns the element at rank r without removing it\n\n\nobject replaceAtRank(integer r, object o):\n\nreplace the element at rank with o and return the old element\n\n\ninsertAtRank(integer r, object o):\n\ninsert a new element o to have rank r\n\n\nremoveAtRank(integer r):\nremoves and returns the element at rank r\n\nAdditional operations size() and isEmpty()\n Applications of Vectors\n\nThere is not an automatic limit on the storage size\n\nunlike arrays of a fixed size\n\n\nDirect applications\n\nSorted collection of objects (elementary database)\n\n\nIndirect applications\n\nAuxiliary data structure for many algorithms\nComponents of other data structures\n\n\n\n Array-based Vector\n Performance\nIn the array-based implementation of a Vector\n\nThe space used by the data structure is O(n)O(n)O(n)\nsize, isEmpty, elemAtRankand, replaceAtRankrun in O(1)O(1)O(1) time\ninsertAtRankand removeAtRankrunin O(n)O(n)O(n) time\npush runs in O(1)O(1)O(1) time, as do not need to move elements\n\nunless need to resize the array\n\n\npop runs in O(1)O(1)O(1) time\n\n Comparison of the Strategies\nIn an insertAtRankoperation, when the array is full, instead of throwing an exception, we can replace the array with a larger one\n\nincremental strategy: increase size by a constant ccc\ndoubling strategy: double the size\n\nWe call amortized time of a push operation the average time taken by a push over the series of operations, i.e., T(n)/nT(n)/nT(n)/n\n Why is amortised analysis different from the average case analysis?\n\nâ€œAmortisedâ€: (long) real sequence of dependent operations\nâ€œAverageâ€: Set of (possibly independent) operations\n\nWe have different measures of the runtime cost:\n\nWorst-case â€œcost per operation of a sequenceâ€\nnot just\nâ€œWorst case of a single operationâ€\n\n Incremental Strategy Analysis\nAverage, per push operation, is O(n)O(n)O(n)\n\nWe replace the array $k = n/c $ times\nEach â€œreplaceâ€ costs the current size\nThe total time T(n)T(n)T(n) of a series of nnn push operations is proportional to\nn+c+2c+3c+4c+â€¦+kc=n+ c + 2c + 3c + 4c +â€¦ + kc =n+c+2c+3c+4c+â€¦+kc=\nn+c(1+2+3+â€¦+k)=n+ c(1 + 2 + 3 + â€¦ + k) =n+c(1+2+3+â€¦+k)=\nn+ck(k+1)/2n+ ck(k + 1)/2n+ck(k+1)/2\nSince ccc is a constant, T(n)T(n)T(n) is O(n+k2)O(n+k^2)O(n+k2),i.e., O(n2)O(n^2)O(n2)\nThe amortized time of a push operation is O(n)O(n)O(n)\nThis is bad as the normal cost of a push is O(1)O(1)O(1).\n\n Doubling Strategy Analysis\nGives an average of O(1)O(1)O(1) per operation\n\nWe replace the array k=log2nk = log_2nk=log2â€‹n times\nThe total time T(n)T(n)T(n) of a series of nnn push operations is proportional to\nn+1+2+4+8+â€¦+2kâˆ’1=n+ 1 + 2 + 4 + 8 + â€¦+ 2^{k-1}=n+1+2+4+8+â€¦+2kâˆ’1=\nn+2kâˆ’1=2nâˆ’1n+2^k-1 = 2n -1n+2kâˆ’1=2nâˆ’1\nT(n)T(n)T(n) is O(n)O(n)O(n)\nAmortized time of a single push operation is O(1)O(1)O(1)\nThat is, no worse than if all the needed memory was pre-assigned!\n\n(Big-Oh hides the constant factor â€˜2â€™ extra cost.)\n\n\n\n\n Priority Queues &amp; Heaps\n Priority Queue ADT\n\nA priority queue stores a collection of entries\nEach entry is a pair(key, value)\nMain methods of the Priority Queue ADT\n\ninsert(k, v) inserts an entry with key k and value v\nremoveMin() removes and returns the entry with smallest key\nNote: unlike Map (seen later), there is no requirement of a method find(k).\n\n\nAdditional methods\n\nmin() returns, but does not remove, an entry with smallest key\nsize(), isEmpty()\n\n\nApplications:\n\nStandby passengers\nAuctions\nStock market\nPrinter queues\n\n\nTotal Order Relations\n\nJust need a â€œcompareâ€ operation between the keys and that behaves like &lt;= or &lt; as needed\nTwo distinct entries in a priority queue can have the same key\n\n\n\n Heaps\nA heap is a binary tree storing key-value pairs at its nodes and satisfying the following properties:\n\n\nHeap-Order: for every internal node vvv other than the root,\nkey(v)â‰¥key(parent(v))key(v)\\geq key(parent(v))key(v)â‰¥key(parent(v))\n\n\nComplete Binary Tree: let hhh be the height of the heap\n\nfor i=0,â€¦,hâˆ’1i= 0, â€¦ , h -1i=0,â€¦,hâˆ’1, there are 2i2^i2i nodes of depth iii\nThe last node of a heap is the rightmost node of depth hhh\n\n\n\n Insertion into a Heap\n\nFind the insertion node zzz (the new last node)\nStore kkk at zzz\nRestore the heap-order property\n\nAfter the insertion of a new key kkk, the heap-order property may be violated\nAlgorithm upheap restores the heap-order property by swapping kkk along an upward path from the insertion node\nUpheap terminates when the key kkk reaches the root or a node whose parent has a key smaller than or equal to kkk\nSince a heap has height O(logâ€‰n)O(log\\, n)O(logn), upheap runs in O(logâ€‰n)O(log\\, n)O(logn) time\n\n\n\n Removal from a Heap\n\nReplace the root key with the key of the last node www\nRemove www\nRestore the heap-order property\n\nAlgorithm downheap restores the heap-order property by swapping key kkk along a particular downward path from the root\nDownheap terminates when key kkk reaches a leaf or a node whose children have keys greater than or equal to kkk\nSince a heap has height O(logâ€‰n)O(log \\,n)O(logn), downheap runs in O(logâ€‰n)O(log\\, n)O(logn) time\n\n\n\n Heap-Sort\nConsider a priority queue with nnn items implemented by means of a heap\n\nthe space used is O(n)O(n)O(n)\nmethods insert and removeMin take O(logâ€‰n)O(log \\, n)O(logn) time\nmethods size, isEmpty, and min take time O(1)O(1)O(1) time\n\nUsing a heap-based priority queue, we can sort a sequence of nnn elements in O(nâ€‰logâ€‰n)O(n\\,log\\, n)O(nlogn) time\n\nInsert all elements into the heap one by one.\n\nThis takes nnn insertions with O(logâ€‰n)O(log\\,n)O(logn) each for a total of O(nâ€‰logâ€‰n)O(n \\,log \\,n)O(nlogn)\n\n\nRemove all elements one by one, using removeMin()removeMin()removeMin(), hence obtaining them in sorted order.\n\nThis takes nnn removals with O(logâ€‰n)O(log\\, n)O(logn) each for a total of O(nâ€‰logâ€‰n)O(n \\,log \\,n)O(nlogn).\n\n\n\n Map\n The Map ADT over pairs &lt;K,V&gt;\n\n\n\n\n\n\n\n\n\nA map models a collection of key-value entries that is searchable `by the keyâ€™\nThe main operations of a map are for searching, inserting, and deleting items.\nMultiple entries with the same key are not allowed.\nMap ADT methods:\n\n\nV get(K k):\nif the map M has an entry with key k, return its associated value;\nelse, return null\n\n\nV put(K k, V v): insert entry (k, v) into the map M;\nif key k is not already in M, then return null;\nelse, return old value associated with k\n\n\nV remove(K k):\nif the map M has an entry with key k, remove it from M and return its associated value;\nelse, return null\n\n\nint size(), boolean isEmpty()\n\n\n&#123;K&#125; keys(): return an iterablecollection of the keys in M\n\n\n&#123;V&#125; values(): return an iterablecollection of the values in M\n\n\n&#123;&lt;K,V&gt;&#125; entries(): return an iterablecollection of the entries in M\n\n\n A Simple List-Based Map\nIt is straightforward to implement a map using a list â€“ either singly or doubly-linked\n\n\nThe get(k) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nThe put(k,v) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nThe remove(k) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nOverall: the operations are O(n)O(n)O(n) because of needing to traverse the list\n\n\nList version of a Map is â€œsimple but inefficientâ€\n\n\nEasier to be correct\n\n\nEven though slow, they can be useful\n\nallow the rest of the project to proceed without waiting for the efficient version\ncan be used for a regression test of a â€œfaster but trickierâ€ later version\n\ne.g. use in a â€œshadow modeâ€ where the â€œslow-obviously-correctâ€ version is used together with the â€œfast-possibly-buggyâ€ version and the results checked against each other.\n\n\n\n\n\n Hash Tables\n\n\n\n\n\n\n\n\n\nHash tables are a concrete data structure which is suitable for implementing maps.\nBasic idea: convert each key into an index into a (big) array.\nLook-up of keys and insertion and deletion in a hash table usually runs in O(1)O(1)O(1) time.\n\nNot guaranteed, and design of the table needs to be done carefully if want the access to be â€œreliably O(1)O(1)O(1)â€\n\n Basic Hash Functions\n\nA hash function hhh maps keys of a given type to integers in a fixed interval [0,Nâˆ’1][0, N - 1][0,Nâˆ’1]\n\nExample: h(k)=kâ€…â€Šmodâ€…â€ŠNh(k) = k \\;mod\\; Nh(k)=kmodN is a hash function for integer keys\n\n\nThe integer h(k)h(k)h(k) is called the hash value of key kkk\n\n Collision Handling\nCollisions occur when different elements are mapped to the same cell\n Hash Functions\n\n\nA hash function is usually specified as the composition of two functions:\n\n\nHash code:\nh1h_1h1â€‹: keys â†’ integers\n\n\nCompression function:\nh2h_2h2â€‹: integers â†’ [0,Nâˆ’1][0, N - 1][0,Nâˆ’1]\n\n\nDivision:\n\n\nh2(y)=yâ€…â€Šmodâ€…â€ŠNh_2 (y) = y\\; mod \\;Nh2â€‹(y)=ymodN\n\n\nThe size NNN of the hash table is usually chosen to be a prime\n(hash codes will tend to spread better)\n\n\n\n\nMultiply, Add and Divide (MAD):\n\nh2(y)=(ay+b)â€…â€Šmodâ€…â€ŠNh_2 (y) = (ay + b)\\; mod\\; Nh2â€‹(y)=(ay+b)modN\naaa and bbb are nonnegative integers\nsuch that aâ€…â€Šmodâ€…â€ŠN=Ì¸0a\\; mod\\; N \\not= 0amodNî€ =0\nOtherwise, every integer would map to the same value bbb\n\n\n\n\n\n\n\nThe hash code is applied first, and the compression function is applied next on the result,\ni.e., h(x)=h2(h1(x))h(x) = h_2(h_1(x))h(x)=h2â€‹(h1â€‹(x))\n\n\nThe goal of the hash function is to â€œdisperseâ€ the keys in an â€œapparently randomâ€ way\n\nWhy disperse?\n\nto reduce numbers of collisions\n\n\nWhy random?\n\nrandom means â€˜no patternâ€™\nif there is an obvious pattern then the incoming data might have a matching pattern that leads to many collisions\nâ€œsometimes â€˜no patternâ€™ is the only safe patternâ€ (e.g. rock-paper-scissors game)\n\n\n\n\n\n Separate Chaining\nlet each cell in the table point to (e.g.) a linked list of entries that map there\n\nNote: In practice, should use a more efficient Map;\ne.g. a Binary Search Tree (BST)\n\n\n\n\n1234V get(k)&#123;    return A[h(k)].get(k); \t// Simply delegates the â€œgetâ€ to the list-based map at A[h(k)]&#125;\n\n\n123456789101112V put(k,v)&#123;   /*    * If there is an existing entry in our map with key equal to k,     * then we return its value (replacing it with v);     * otherwise, we return null    */    t = A[h(k)].put(k,v); // t is new value or null    // Simply delegates the put to the list-based map at A[h(k)]    if (t = null) n = n + 1; // if k is a new key    return t;&#125;\n\n\n123456V remove(K k)&#123;\tt = A[h(k)].remove(k);    // Simply delegates the remove to the list-based map at A[h(k)]    if (t != null) n = n - 1 // k was found    return t&#125;\n\n\nSeparate chaining is simple and fast, but requires additional memory outside the table.\n\n\nWhen memory is critical then we try harder to remain within the existing memory\n\n\n Open addressing\n\n\n\n\n\n\n\n\n\nthe colliding item is placed in a different cell of the table\n\n\nLinear probing handles collisions by placing the colliding item in the next (circularly) available table cell\n\nâ€œCircular arrayâ€ â€“ once get to the right-hand end, then start again at the beginning of the array\n\n\n\nDisadvantage: Colliding items lump together, causing future collisions to cause a longer sequence of probes\n\n\nV get(k)\n\nWe start at cell h(k)\nWe probe consecutive locations until one of the following occurs\n\nAn item with key k is found, or\nAn empty cell is found, or\nN cells have been unsuccessfully probed\n\n\n\n\n\nsafely remove an element x\n\n\nFind x using `getâ€™ and set the entry back to blank\n\n\nâ€œLazy deletionâ€: donâ€™t mark the entry as a blank, but as a â€˜deletedâ€™ and fix the entries later\n\n\nMove such entries by removing them and then re-inserting them all\n\n\n\n\n Double Hashing\nDouble hashing uses a secondary hash function d(k)d(k)d(k) and handles collisions by placing an item in the first available cell of the series\n\n\n(h(k)+jâ‹…d(k))â€…â€Šmodâ€…â€ŠN(h(k) + j\\cdot d(k))\\; mod \\;N(h(k)+jâ‹…d(k))modN for j=0,1,â€¦,Nâˆ’1j = 0, 1, â€¦ , N - 1j=0,1,â€¦,Nâˆ’1\n\n\nThe secondary hash function d(k)d(k)d(k) cannot have zero values\n\nCommon choice for the secondary hash function:\nd(k)=qâˆ’(kâ€…â€Šmodâ€…â€Šq)d(k) = q - (k \\;mod\\; q)d(k)=qâˆ’(kmodq) where\n\nq&lt;Nq &lt; Nq&lt;N\nqqq is a prime\n\n\n\n\n\nLinear probing is just d(k)=1d(k)=1d(k)=1\n\n\nThe table size NNN must be a prime to allow probing of all the cell\n\nWith a prime NNN, then eventually all table positions will be probed\n\n\n\nExample\n\n\n\n Performance of Hashing\n\n\nIn the worst case, searches, insertions and removals on a hash table take O(n)O(n)O(n) time\n\n\nThe worst case occurs when all the keys inserted into the map collide\n\n\nThe load factor a=n/Na = n/Na=n/N affects the performance of a hash table\n\n\nIn Java, maximal load factor is 0.75 (75%) â€“ after that, rehashed\n\n\n Summary\nThe expected running time of all the map ADT operations in a hash table is O(1)O(1)O(1)\n\nIn practice, hashing is very fast provided the load factor is not close to 100%\n\n Re-Hashing\nWhen the table gets too full then â€œre-hashâ€: Create a new larger table and new hash function.\n\n\nNeed to (eventually) transfer all the entries from the old table to the new one\n\n\nIf do so immediately, then\n\none can amortise the cost over many entries (as for Vector) and so get an average cost of O(1)O(1)O(1) again\nbut the worst case might be O(n)O(n)O(n) when the table is rehashed, and this might be bad for a real time system\nOption:\ndo not transfer all entries â€œin one goâ€ but do â€œa few at a timeâ€\nKeep both tables until the transfer is complete; but only do insertions into the new table\n\n\n\n Applications of Hashing\n\nDirect applications of hash tables:\n\nsmall databases\ncompilers\nbrowser caches\n\n\nHash tables as an auxiliary data structure in a program\n\n\n Binary Search Trees\n\n\n\n\n\n\n\n\n\nA binary search tree is a binary tree storing key-value entries at its internal nodes and satisfying the following â€œsearch treeâ€ property:\nLet u,vu, vu,v, and www be any three nodes such that\nuuu is in the left subtree of vvv and\nwww is in the right subtree of $ v$.\n\nThen we must have\nkey(u)â‰¤key(v)â‰¤key(w)key(u) \\leq key(v) \\leq key(w)key(u)â‰¤key(v)â‰¤key(w)\nor, as we will assume there are no duplicate keys:\nkey(u)&lt;key(v)&lt;key(w)key(u) &lt; key(v) &lt; key(w)key(u)&lt;key(v)&lt;key(w)\n\n Search Algorithm and Basic operation\n Pseudocode\n123456789Node TreeSearch(Key k, Node n)    if n.isExternal () // or, â€œif n == nullâ€    \treturn null    if k &lt; n.key()    \treturn TreeSearch(k, n.left())    else if k = n.key()    \treturn n    else // k &gt; n.key()    \treturn TreeSearch(k, n.right())\n Fundamental Property of Search Tree\n\n\nAn in-order traversal of a (binary) search trees visits the keys in increasing order\n\n\nNote that to access the minimum key, we just need to â€˜always go leftâ€™\n\n\n Insertion\n\nWe search for key k (using Tree Search)\nIf k is already in the tree then just replace the value\nOtherwise, k is not already in the tree, and let w be the leaf reached by the search\n\nWe â€œinsert k at node w and expand w into an internal nodeâ€\n\n\nAgain, only follows a path from the root and so is O(h)O(h)O(h)\n\n Deletion\nFour cases\n\n\nk is not present, nothing to do\n\n\nn has no children, straightforward remove\n\n\nn has one child\n\n\nn has two children\n\n\n Deletion â€“ with one child\nExample: remove 4\n\nTo perform operation remove(4), we search for key 4.\nLet nnn be the node storing 4.\nNode nnn has a null left child, and a real child 5\nWe remove nnn from the tree and connect 5 back to the parent of nnn\n\n      \n Deletion â€“ with two children\nExample: remove 3\nThe key node nnn has two internal children\n\n\nwe find the internal node www that follows nnn in an in-order traversal\n\n\nwe copy key(w)key(w)key(w) into node nnn\n\n\nwe remove node www by means of same procedure as before for â€œone childâ€\n\n\n      \n Balanced Trees\nBinary search trees: if all levels filled, then search, insertion and deletion are O(logâ€‰n)O(log \\, n)O(logn).\n\nAs they are all O(height)O( height )O(height)\n\n Performance\nThe height hhh is O(n)O(n)O(n) in the worst case and O(logâ€‰n)O(log \\, n)O(logn) in the best case\nCould make trees balanced using a â€œtotal rebuildâ€\n\nBut would require O(n)O(n)O(n), and so very inefficient compared to the desired O(logâ€‰n)O( log\\, n)O(logn)\nRe-balancing needs to be O(logâ€‰n)O(log \\, n)O(logn) or O(height)O( height )O(height)\n\n\n Dynamic Programming\nThere are various general methods (â€œparadigmsâ€) for finding solutions to problems:\n\nBrute force â€“ â€œgenerate and testâ€\nDivide-and-conquer\nHeuristics\nDynamic Programming\n\n Brute Force\n\n\n\n\n\n\n\n\n\nThis is roughly â€œgenerate and testâ€\n\nGenerate all potential solutions\nTest for which ones are actual solutions\n\n\n\nExample: we could do â€œsortingâ€ by\n\nGenerate all possible permutations\nTest to see which one is correctly ordered\n\nExtremely inefficient, as is O(n!)O(n!)O(n!)\n\n\n\n\n\nCan be useful in some (small) cases\n\nE.g. Due to the simplicity\n\n\n\n Divide and Conquer\n\n\n\n\n\n\n\n\n\nRecursively, break the problem into smaller pieces, solve them, and put them back together\nMerge-sort and Quicksort were classic examples\n Heuristics\n\n\n\n\n\n\n\n\n\nâ€œHeuristicâ€ = â€œrule of thumbâ€\n\nGenerally, meant to mean something that gives better decisions, than the naive methods, but still not necessarily optimal\n\n Heuristics in exact methods\nThese are general methods that works in an algorithm that does give exact or optimal answers\n\nBut need the heuristics to decrease the (average/typical) runtime\n\nExamples:\n\nâ€œAdmissible heuristicâ€ in A* search â€“ decreases the search time compared to plain search\nâ€œpick a random pivotâ€ in quicksort\n\n Heuristics in inexact methods\nThese are general methods that (generally) are not be guaranteed to give the best possible answers, but that can give good answers quickly\nUsed on problems when the exact methods are too slow\n\nTimetabling and scheduling and many design problems\n\n Greedy algorithms\nA common â€œheuristicâ€ is to be â€œgreedyâ€\n\nTake the decision that looks best in the short term â€“ without looking ahead\n\nSometimes greedy algorithms can still give optimal answers\n\nE.g. Primâ€™s algorithm for constructing a Minimal Spanning Tree is a greedy algorithm\n\nUsually greedy algorithms cannot guarantee to give optimal answers\n\n\nbut often still give (nearly) optimal answers in practice\n\n\nExample: â€œChange-givingâ€:\n\nProblem: given a collection of coins (a multi-set, that allows repeated elements), and a desired target for the change. Supply the change in as few coins as possible\nPick the largest coin which is still available and does not cause to exceed the target\n\n\n\n Dynamic Programming (DP)\nDP is a general method that can be suitable when the optimal solutions satisfy a â€œdecomposition propertyâ€\nThe general idea is roughly: solve small sub-problems first, then build up towards the full solution.\n Subset-Sum\nGiven (multi-)set S of positive integers x[i] and a target K. Is there a subset of S that sums to exactly K?\n\n\nBoolean Array, Y, for [0,â€¦,K]\n\nY[m] = true iff some subset has been found that sums to m\n\n\n\nMain idea: if some subset summed to m, then with the inclusion x[i], we can also find a subset that sums to m+x[i]\n\n\n Pseudocode:\n123456789101112Input: x[0],â€¦,x[n-1] and KInitialise all Y[m] = false for m=1,â€¦, KY[0] = true; // As can always provide no changefor (int i=0 ; i&lt;n ; i++) &#123; // consider effect of x[i]    for (int m=K-x[i] ; m&gt;=0 ; m--) &#123; // â€œscan downâ€        if (Y[m]==true) &#123;                     // m was achievable with x[0]â€¦ x[i-1]            if (m+x[i] == K ) return success; // hence now also m+x[i] is achievable            if (m+x[i] &lt; K ) Y[ m+x[i] ] = true;        &#125;    &#125;&#125;\n Complexity\n\nOuter loop has to consider all the coins, hence O(n)O(n)O(n)\nInner loop scans the entire array Y, hence O(K)O(K)O(K)\nOverall is O(nK)O( n K )O(nK)\n\nHowever, â€œKâ€ has the â€œhidden exponentialâ€ if it is represented in binary:\nThe relevant input size is the number of bits BBB that are needed to represent, B=O(log(K))B=O(log(K))B=O(log(K))\n\n\nThe complexity in terms of the size of the binary input is O(nâ€‰2B)O(n\\,2^B )O(n2B), which is called â€œpseudo-polynomialâ€\n\n Min-Coins version\nPrevious just asked if it is possible to do the change. But want to minimise the coins.\n\n\nAlgorithm: Inspect the coins one at a time keeping track of the best answers obtained so far\n\n\nMain data structure:\n\nInteger Array, Y, for [0,â€¦,K]\n\nY[m] = -1 if have not found any sum for m as yet\nY[m] = c &gt;= 0 means that have found that can achieve the sum m with c coins.\n\n\n\n\n\nAim: when the algorithm finishes then Y[K] will be the minimum number of coins\n\nâ€œSide-effectâ€: All the values of Y[m] m &lt; K, will also be the minimum number for a value of m.\n\n\n\nMain idea: if some set summed to m, then with the inclusion x[i] we can also find a subset that sums to m+x[i]\n\n\n1234567891011121314151617Input: x[0],â€¦,x[n-1] and KInitialise: Y[0] = 0,and Y[m] = -1 for m &gt; 0 // 0 coins can give a change of 0for (i=0 ; i&lt;n ; i++) &#123; // consider effect of x[i]    for (m=K-x[i] ; m&gt;=0 ; m--) &#123; // scan array        if (Y[m] &gt;= 0 ) &#123;            // value m was achievable with x[0]â€¦x[i-1] using Y[m] coins,            // so, m+x[i] is now achievable with Y[m]+1 coins            // but might already have found a better answer            // stored as Y[m + x[i] ] so then take the best            if (Y[m + x[i] ] == -1 )            \tY[ m + x[i] ] = Y[m] + 1;            else                Y[ m + x[i] ] = min( Y[m + x[i] ] , Y[m] + 1 );        &#125;    &#125;&#125;\n Worked example\nInput: x[] = {5,2,2,2,1} and K=6\n\nk=0, Y[] = [0,-1,-1,-1,-1,-1,-1], Y[0]=0 for change {}\nk=1, Y[] = [0,-1,-1,-1,-1,1,-1], Y[5]=1 for change {5}\nk=2, Y[] = [0,-1,1,-1,-1,1,-1], Y[2]=1 for change {2}\nk=3, Y[] = [0,-1,1,-1,2,1,-1], Y[4]=2 for change {2,2}\nk=4, Y[] = [0,-1,1,-1,2,1,3], Y[6]=3 for change {2,2,2}\nk=5, Y[] = [0,-1,1,-1,2,1,2], Y[6]=min(3, 1+1)=2 for change {5,1}\nFinished: so optimal answer is 2 coins.\n\n Minimum Spanning Trees\n Spanning Tree\n\nInput: connected, undirected graph\nOutput: a tree which connects all vertices in the graph using only the edges present in the graph\n\n Minimum Spanning Tree\n\n\nInput: connected, undirected, weighted graph\n\n\nOutput: a spanning tree\n\n(connects all vertices in the graph using only the edges present in the graph)\nand is minimum in the sense that the sum of weights of the edges is the smallest possible for any spanning tree\n\n\n\nUsages: Gas and water pipelines, optic fibers networksâ€¦\n\n\n Why MST is a tree\n\nWe really want a minimum spanning sub-graph\n\na subset of the edges that is connected and that contains every node\n\n\nIf a graph is connected and acyclic then it is a tree\n\n Primâ€™s algorithm\nTo construct an MST:\n\n\nStart by picking any vertex M\n\n\nChoose the shortest edge from M to any other vertex N\n\n\nAdd edge (M, N) to the MST\n\n\nLoop:\n\n\nContinue to add at every step a shortest edge from a vertex in MST to a vertex outside,\nuntil all vertices are in MST\n\n\n(If there are multiple shortest edges, then can take any arbitrary one)\n\n\n\n\n Why is this optimal!?\nArgument by contradiction\n\n\nlet V1 and V2 be a partition of the vertices of G into two disjoint non-empty sets\n\n\nSuppose that some minimum spanning tree T that containing e is better than all other trees\n\n\nThen can add edge e to T and remove some other edge between V1 and V2 and obtain a better MST\n\n\nThe algorithm adds a minimum weight edge between V1 and V2,\nand so this edge must be part of some MST\n\n\nHence, the construction cannot make a â€œfatal mistakeâ€ - at no point can it add an edge not part of an MST\n\n\n Binary Search Trees: Balance and Rotations\n      \n\n\nBoth trees are valid BSTs with the same content\n\n\nBoth have in-order traversal: 1,2,3,4,5. But â€¦\n\nThe left has height 4\nThe right has height 2\n\n\n\nThis matters because the vital BST algorithms are O(height)O( height )O(height)\n\n\nA tree is said to be â€œbalancedâ€ if the heights of left and right subtrees of any node are (close to) equal, and so the height is O(logâ€‰n)O(log \\,n)O(logn)\n\n\n The BST Imbalance problem\n\nAIM: Do â€œsmall local rebuildsâ€ during insert/delete operations, to maintain the balance, and so retain the O(logâ€‰n)O( log \\,n )O(logn) cost\n\n Example of a â€œrotationâ€\n\n\n\nBoth trees have in-order traversal: In(T1), a, In(T2), b, In(T3)\n\n\nDepths:\n\nâ€˜aâ€™ and T1 sink down,\nâ€˜bâ€™ and T3 rise up\n\n\n\na single rotation only has O(1)O(1)O(1) in cost\n\n\n Simple Example\n\n\nâ€œRaising bâ€ was the good choice because it is the median value and so should be the root\n\n Specific Example: Double works\n\n\n\nRotation on the edge b-c\n\n\nThen can rotate on edge a-b\n\n\nThe median node is â€˜bâ€™, needs to drop from 2 to 0\n\nEach rotation can only move â€˜bâ€™ up by 1, hence need two rotations\n\n\n\n Summary\nThe goal is then to control the usage of rotations to reduce the overall height of the tree\nAdvanced:\n\nAVL trees\n2-4 trees &amp; red-black trees\n\nThey guarantee O(logâ€‰n)O( log \\,n )O(logn) â€“ unlike Hash maps\n Shortest Paths: Floyd-Warshall (FW)\n Floyd-Warshall: All-Pairs Shortest Paths\nSuppose that wanted to find the shortest path between all pairs of start and end nodes\n Basic method\n\nBuild the optimal answers using a subset of the nodes.\nThen add the effects of other nodes one at a time\n\n Data structure\n\n\nd(i,j,k)d(i,j,k)d(i,j,k) = shortest distance between nodes iii and jjj,\nbut using only the nodes 1,â€¦,k{1,â€¦,k}1,â€¦,k as potential allowed intermediary points\n\n\nd(2,5,3)d(2,5,3)d(2,5,3) = shortest distance from n2n_2n2â€‹ to n5n_5n5â€‹ using only {n1,n2,n3}\\{n_1,n_2,n_3\\}{n1â€‹,n2â€‹,n3â€‹} as potential intermediate points\n\n\nwe will assume that d(i,i,k)=0d( i, i, k ) = 0d(i,i,k)=0 for all iii\n\n\n Initialisation\n\n\nd(i,j,0)d(i,j,0)d(i,j,0) = best distance between nodes iii and jjj, but not using any intermediate nodes,\n\n\nd(i,j,0)=w(i,j)â€…â€ŠÂ ifÂ thereÂ isÂ anÂ edgeÂ iÂ toÂ jd(i,j,0) = w(i,j) \\;\\text{ if there is an edge i to j}d(i,j,0)=w(i,j)Â ifÂ thereÂ isÂ anÂ edgeÂ iÂ toÂ j\n=âˆâ€…â€Šâ€…â€Šâ€…â€Šâ€…â€Šâ€…â€Šâ€…â€Šâ€…â€ŠÂ otherwise= \\infin \\;\\;\\;\\;\\;\\;\\;\\text{ otherwise}=âˆÂ otherwise\n\n\n FW equations\n\n\nNow suppose that we add the node â€˜n1n_1n1â€‹â€™ to the set of nodes that can be intermediates,\ni.e. consider k = 1\n\n\nBest path is now the best of â€œeither direct, or via n1n_1n1â€‹.â€\n\n\nd(i,j,1)=min(â€‰d(i,j,0),â€…â€Šd(i,n1,0)+d(n1,j,0))d(i,j,1) = min (\\, d(i,j,0), \\;d(i,n_1,0) + d(n_1,j,0) )d(i,j,1)=min(d(i,j,0),d(i,n1â€‹,0)+d(n1â€‹,j,0))\n\n\n\n\n\nNow suppose that we add the new node â€œ(k+1)â€ to the set of â€œvia nodesâ€ that can be intermediates, but have already considered k of them\n\n\nBest path is now either direct using only the k â€˜via nodesâ€™ already accounted for,\nor else also via node â€˜k+1â€™ (and using the previous k viaâ€™s)\n\n\nd(i,j,k+1)=min(â€‰d(i,j,k),â€…â€Šd(i,k+1,k)+d(k+1,j,k))d(i,j,k+1) = min ( \\,d(i,j,k), \\;d(i,k+1,k) + d(k+1,j,k) )d(i,j,k+1)=min(d(i,j,k),d(i,k+1,k)+d(k+1,j,k))\n\n\n\n\n FW code &amp; complexity\nThe main loop after initialisation is:\n1234foreach k = 1,â€¦,|V| // size of V\tforeach i âˆˆ V\t\tforeach j âˆˆ V\t\t\td(i,j,k+1) = min( d(i,j,k), d(i,k+1,k) + d(k+1,j,k) );\n\n\nHave 3 nested loops, of ranges |V|. Hence is O(âˆ£Vâˆ£3)O( |V|^3 )O(âˆ£Vâˆ£3)\n\n\nIf the graph is sparse, then âˆ£Eâˆ£&lt;&lt;âˆ£Vâˆ£2|E| &lt;&lt; |V|^2âˆ£Eâˆ£&lt;&lt;âˆ£Vâˆ£2, so then â€œall-starts Dijkstraâ€ may be better.\n\nâ€œ&lt;&lt;â€ means â€œmuch less thanâ€\n\n\n\n FW on digraphs\nFW also works the same on directed graphs\n\nThe initial matrix d(i,j,0)d(i,j,0)d(i,j,0) need not be symmetric, but then the remaining calculations use exactly the same formulas\n\n FW with negative edges\nFW even works if some (directed) edge weights are negative\n\nBUT it is essential that there are no cycles of total negative weight\nOtherwise simply repeatedly following around the negative cycle may reduce lengths to be as negative as desired, so there is no shortest path\n\n Key Idea\nUses that shortest path does satisfy a nice decomposition of\n\n\n\n\n\n\n\n\n\nIf P(A,B)P(A,B)P(A,B) is a shortest path, and goes via MMM, then P(A,M)P(A,M)P(A,M) is optimal for AAA to MMM and P(M,B)P(M, B)P(M,B) is optimal for MMM to BBB\n\nHence uses a version of â€œdynamic programmingâ€\n\n","slug":"ade","date":"2024-06-09T22:34:56.000Z","categories_index":"Notes","tags_index":"Algorithms,Data structure,Efficiency","author_index":"Huaji1hao"},{"id":"c69b9ac1223f91f4483d13b90656a509","title":"IELTS Writing Task 2","content":" Writing Task 2\n Understanding and Analysis\n The 5 types of questions:\n\nOpinion (Agree or Disagree)\nDiscussion (Discuss both view)\nAdvantages and Disadvantages\nProblem/Causes and Solution\nDouble Question\n\n Question Analysis\nWhen analyzing a question we have to think about 3 things:\n\nTopic\nKeywords\nInstruction words\n\n\n\n\n\n\n\n\n\n\nSome experts believe that it is better for children to begin a foreign language at primary school  rather than secondary school.\nDo the advantages of this outweigh the disadvantages?\n Think like an examiner!\n Task Response Dos\n\n\nAnswer the specific question being asked, Not the general topic\n\n\nMake sure your ideas are relevant\n\n\nFully address each part of the question\n\n\nState your opinion in introduction and use the supporting paragraphs to support this opinion\n\n\nReiterate your opinion in the conclusion\n\n\n Task Response Donâ€™ts\n\nSpend lots of time on just one part of the question\nGive very general examples\nLeave opinion until the last sentence\nRepeat the same points over and over\nWrite under 250 words\n\n Coherence and Cohesion Dos\n\nUse four paragraph structure\nOutline your main ideas and opinion in the introduction\nHave clear topic sentences in your supporting paragraphs\nSkip a line between paragraphs\nUse cohesive devices appropriately and accurately\n\n Coherence and Cohesion Donâ€™ts\n\nInclude background statement in the introduction\nHave lots of ideas in one paragraph\nUse cohesive devices at the start of every sentence\nUse cohesive devices inaccurately\n\n Vocabulary Dos\n\nBe careful with spelling ad grammar\nBe aware of collocations (match the words)\nUse â€˜less-commonâ€™ words (specific description)\nUse topic specific words\nFollow the 100% rule\n\n Vocabulary Donâ€™ts\n\nRepeat the same words again and again\nForce complex words into your essay without knowing them 100%\nUse synonyms that are wrong\nLearn lists of â€˜academicâ€™ words out of context\n\n Grammar Dos\n\nTry to write as many error-free sentences as possible\nUse â€˜complexâ€™ sentences (two classes rather than single)\nUse a variety of structures\nCheck work when writing and at the end\nFollow the 100% rule\n\n Grammar Donâ€™ts\n\nTry to use as many different structures as possible\nTry to impress the examiner with complex grammar\nWrite sentences that stop meaning being conveyed\n\nCLEARITY IS KING\n Planning\n Generate ideas\n 6 Questions\n\nWho? What? Why? Where? How? When?\n\n Bonus Idea Generation\n\nIf you asked 100 people, what would be the common answer?\nIf you were trying to win an argument, what idea would you use?\n\n Structure Planning\n Introduction\n\n\nParaphrase Question\n\n\nOutline\n\n\n Supporting Paragraph 1\n\nMain point\nExplanation\nExample\n\n Supporting Paragraph 2\n\nMain point\nExplanation\nExample\n\n Conclusion\n\nSummarize main points\n\n Pre-think Vocabulary (Synonyms)\n\nStudents = graduates = undergraduates\nUniversity = College = third-level education = tertiary education\nConsume = use\nPeople = human\nTeenagers = adolescents\nGovernments = States\nAdvantages = benefits\n\n Timing\n\n40 minutes to complete task2\n10 minutes planning\nMost essays are around 12 sentences long\n2 minutes per sentence = 24 minutes\n6 minutes reviewing and checking our work\n\n Introduction\n Common Mistake\n\nWriting long general background statements or hooks\nNo opinion or outline of main ideas\n\n Combining opinion and outline\nThe continued rise in the worldâ€™s population is the greatest threat faced by humanity at the present time.\nDo you agree?\n\nrun out of resources\ndamage to the planet\n\n\n\n\n\n\n\n\n\n\nIncreasing overpopulation is the biggest thread human beings face today. This essay totally agrees with this statement because this will lead to a serious depletion of resources and pollution\n Examples\n Opinion\nIn some countries an increasing number of people are suffering from health problems as a result of\neating too much fast food. It is therefore necessary for governments to impose a higher tax on this kind of food.\nDo you agree?\n\n\n\n\n\n\n\n\n\nIt is argued that states should charge fast food companies more tax because of the growing amount of men and women with health conditions associated with this type of food. This essay totally agrees with that statement because these illnesses cost the health service too much money and increasing the price of junk food would reduce the demand for it.\n Discussion\nSome people work for the same organisation all their working life.Others think that it is better to work for different organisations.\nDiscuss both views and give your own opinion.\n\n\n\n\n\n\n\n\n\nSome say that it is more beneficial to be employed with the same company all their lives, while others would argue that it is better to work for a variety of companies. This essay will argue that although working for just one employer gives you more financial and other benefits,working at lots of different places provides an employee with more experience.\n Advantages and Disadvantages\nOne of the consequences of improved medical care is that people are living longer and life expectancy is increasing.\nDo you think the advantages of this development outweigh the disadvantages?\n\n\n\n\n\n\n\n\n\nOne of the results of modern medicine is that men and women are able to live longer. This essay will argue that despite the strain this might cause on the pension system,the alleviation of suffering means that the advantages far outweigh the drawbacks.\n Problem and Solution\nSmartphones are becoming a common sight in the primary school classroom.\nWhat problem does this cause and what is a viable solution?\n\n\n\n\n\n\n\n\n\niPhones and other devices are being used more often by primary school children. This essay will suggest that they are very distracting and that the best solution is to ban them completely in class.\n Double Questions\nCar ownership has increased so rapidly over the past thirty years that many cities in the world are now â€˜one big traffic jamâ€™.\nHow true do you think this statement is?\nWhat can governments do to discourage people from using their cars?\n\n\n\n\n\n\n\n\n\nLots of city centers are highly congested due to an increase in the ownership of cars in the last three decades. This essay will argue that this is true only during peak times and that improving public transport will reduce this.\n Body paragraphs\n Common Mistake\n\nToo many ideas\nUndeveloped ideas\nNo/Poor explanations or examples\nFirstly, secondly, thirdly, finally\nPoor grammar and vocabulary\n\n Checklist\n\nRelevant ideas\nFully address all parts of the task\nClear position throughout\nFully extended and well supported ideas\n\n Structure - 3 Key Elements\n\nTopic Sentence\nExplanation Sentences\nExample\n\n\n\n\n\n\n\n\n\n\nIt is argued that states should charge fast food companies more tax because of the growing amount of men and women with health conditions associated with this type of food. This essay totally agrees with that statement because these illnesses cost the health service too much money and increasing the price of junk food would reduce the demand for it.\nObesity related illnesses cost the taxpayer billions of dollars every year. People who eat too much Food are more likely to suffer from costly diseases associated with being overweight. As a result, they have to go to hospital more regularly for treatment and this puts a strain on the health service. For example,a large proportion of the United Kingdomâ€™s National Health Service budget is spent on preventable, obesity-related diseases, such as heart disease, hypertension and diabetes.\n Topic Sentence\n\nShort clear statement about what the paragraph is about\nDirectly answers the question\nNot much detail.\nMakes your main ideas clear\nCanâ€™t write one unless you have clear ideas already\n\n\n\n\n\n\n\n\n\n\nIntroduction:\nincreasing the price of junk food would reduce the demand for it.\nâ†“\nTopic Sentence:\nAnother reason why fast food restaurants should pay extra tax is to raise the cost in order to decrease demand.\n Explanation\n\n\nPretend that you are writing to someone with on knowledge of the subject\n\n\nClear explain\n\n\nwhat you topic sentence means\n\n\nhow it answers the question\n\n\nwhat the result is\n\n\n\n\nShould be around 2-4 sentences\n\n\nUseful language\n\nThat is to sayâ€¦ = In other wordsâ€¦\nThis is becauseâ€¦ = The reason isâ€¦\nAs a resultâ€¦ = Thereforeâ€¦\n\n\n\n\n\n\n\n\n\n\n\n\nAnother reason why fast food restaurants should pay extra tax is to raise the cost in order to decrease demand. Any extra tax would be added to the normal price of the food, and as prices go up fewer people will be able to afford to buy fatty food. Therefore, this would reduce the amount of junk food people eat and the result would be a healthier nation.\n Examples\n Better Examples\n\nReal examples\nConverting personal examples\n\n How to think of examples\n\n\nTake examples from your own life experience\n\n\nConvert them into more general examples\n\n\nThink about\n\nWhere are you form?\nWhere do you live\nWhat is  your job?\nWhat about your family and friends?\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal:\nFor example, I used to smoke 20 cigarettes everyday, but the government kept increasing the tax and now I canâ€™t afford to smoke.\nâ†“\nGeneral:\nFor example, when the duty on cigarettes is raised each year in the UK, more people quit smoking and this has a knock on effect on the number of people dying from lung cancer and other smoking related diseases.\n\n\n\n\n\n\n\n\n\nYour own personal experience:\nMy sisterâ€™s bookshop closed down because it couldnâ€™t compete with Amazon.\nâ†“\nGeneral:\nFor instance, many bookshops in the UK closed down because of competition from online.\n\n\n\n\n\n\n\n\n\nYour own personal experience:\nI canâ€™t afford to buy a laptop for my child.\nâ†“\nGeneral:\nFor example, in Ireland laptops are very expensive which would prevent most parents from being able to purchase them for their children.\n Conclusion\n Big Mistakes\n\n\nNew ideas\n\n\nTrying to be entertaining\n\n\nBeing too vague\n\n\nRepeating exactly the same thing as in the rest of your essay\n\n\nUsing the wrong cohesive devices\n\n\n Why are conclusions important?\n\nIt is the last thing the examiner reads\nIt shows the examiner that you can summarise\nOpinion is clear the whole way through\nMakes essay cohesive and coherent\n\n Appropriate Cohesive Devices\n\nIn conclusion,\nTo conclude,\n\n Structure\n\n\nTwo essential things are\n\nSummary of main points\nOpinion\n\n\n\nOne sentences\n\n\n Example\n\n\n\n\n\n\n\n\n\nQuestion:\nThe continued rise in the worldâ€™s population is the greatest threat faced by humanity at the present time. Do you agree?\nIntroduction:\nIncreasing overpopulation is the biggest threat human beings face today. This essay totally agrees with this statement because this will lead to a serious depletion of resources and pollution.\nConclusion:\nIn conclusion, there are too many people in the world and this is a huge threat to everyone because\nessential resources are running out and we are also polluting our planet.\n\n\n\n\n\n\n\n\n\nIntroduction:\niPhones and other devices are being used more often by primary school children. This essay will\nsuggest that they are very distracting and that the best solution is to ban them completely in class.\nConclusion:\nIn conclusion, mobiles are not a good idea for young children because they interfere with the learning process and this should be stopped by telling parents that they are not allowed in schools.\n Review\n\n\nGrammar\n\nPrepositions\nArticles\nVerb-subject agreement\nCountable/Uncountable nouns\nTenses\nCapital letters\n\n\n\nVocabulary\nFirst you should scan for any words repeated that you can easily change using synonyms.\nFocus on\n\nCollocations\nMeaning\nSpelling\nWord form\n\n\n\nChecklist\n\nDoes the essay answer all parts of the question?\nIs the opinion clear in the introduction?\nDoes each supporting paragraph have a clear topic sentence?\nAre ideas fully developed with explanations and examples?\nIs your opinion clear throughout the whole essay?\nIs there a suitable conclusion?\nOver 250 words?\nRange of complex and simple sentences.\nAnything confusing or unclear?\n\n\n\n 1. Opinion\n Question\n\nDo you agree or disagree?\nTo what extent do you agree or disagree\n\nalways state â€˜strongly agreeâ€™ or â€˜strongly disagreeâ€™\n\n\n\n Alternative wording of opinion questions\n\nIs this a positive or negative development?\nTo what extent is this a positive or negative development\n\n Common Mistakes\n\nNot giving your opinion\nLeaving opinion until the conclusion\nDiscussing someone elseâ€™s opinion\nDiscussing both sides of the argument\n\n Band 9 Checklist\n\nGive a clear opinion in the introduction\nThink of two main points supporting your opinion\nDevelop these main points with explanations and examples\nShort conclusion summarizing your main points and reiterating your opinion\n\n Deciding Opinion\n\nWastes time\nPersonal opinion does not matter\nPick one side only\nPick the side  you can easily write about\n\n Language\nGood in introduction:\n\nThis essay completely agrees thatâ€¦\nThis essay totally disagrees thatâ€¦\nIn my opinion,\nI totally agree/disagree\n\n Structure\nParagraph Introduction\n\nParaphrase Question\nGive opinion and outline main points\n\nSupporting Paragraph I\n\nTopic Sentence\nExplanations\nExample\n\nSupporting Paragraph II\n\nTopic Sentence\nExplanations\nExample\n\nConclusion\n\nSummary\n\n Planning- Idea Generation\n\n\n\n\n\n\n\n\n\nIn many countries it is now illegal to advertise alcohol. Do you agree or disagree?\nThoughts:\n\n\nWhy do I agree? (Coffee shop method)\n\n\nAlcohol is a problem both socially and for our health\n\n\nTaking advantage of alcoholics\n\n\nAdvertisers can influence people to drink alcohol\n\n\nYoung people are exposed to persuasive ads\n\n\nIntroduction\n\nParaphrase Question\nAgree and outline main points\n\nSupporting Paragraph 1\n\nTopic Sentence - health and social problems\nExplanations - addiction and bad behaviour\nExample - Russia\n\nSupporting Paragraph 2\n\nTopic Sentence - influence\nExplanations - glamorise and young people\nExample - Specific advert\n\nConclusion\n\nSummary\n\nVocabulary:\n\nIllegal-banned, ban, prohibited, not allowed.\nAdvertise-TV, radio, print, ads, commercials, promoting.\nAlcohol-drinking, alcoholic drinks/beverages,beer, wine and spirits.\nAddiction-alcoholic, addicted to alcohol, alcohol dependence, dependent on alcohol.\nBad behavior -anti-social behavior, violence,vandalism.\nInfluence-sway,persuade,convince.\n\n\n\n\n\n\n\n\n\n\nMore and more people are now working remotely from home. Is this a positive or negative development?\nIntroduction\n\nParaphrase Question\nPositive and outline main points\n\nSupporting Paragraph 1\n\nTopic Sentence - less stress\nExplanations - no commute in heavy traffic / timeable adaptable to personal needs\nExample - workers can make timetable to incorporate regular breaks in order to relax\n\nSupporting Paragraph 2\n\nTopic Sentence - less environmental impact\nExplanations - lower carbon footprint due to less travel and energy to power office buildings\nExample - Pandemic meant working from home, which meant lower levels of pollution\n\nConclusion\n\nSummary\n\n Review\n\nNow donâ€™t forget to review your essay\nMake sure that:\n\nGive a clear opinion in the introduction\nThink of two main points supporting your opinion\nDevelop these main points with explanations and examples\nShort conclusion summarizing your main points and reiterating your opinion\n\n\n\n","slug":"Writing_Task_2","date":"2024-06-09T20:53:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"79276e65a56422ae2094bb3bd778a530","title":"Configure Namesilo domain for Github page","content":" use Namesilo domain for Github page / ä½¿ç”¨ Namesilo åŸŸåé…ç½® Github Pages\n 1. buy a domain in namesilo\n 2. manage the domain DNS\n1234567891011121314151617TYPE A    HOSTNAME: leave blank here    Address(add four times):     185.199.108.153    185.199.109.153    185.199.110.153    185.199.111.153TYPE CNAME    HOSTNAME: www    Address: xxx.github.ioTYPE AAAA(optional, for ipv6)    HOSTNAME: retain empty    Address:     2606:50c0:8000::153    2606:50c0:8001::153    2606:50c0:8002::153    2606:50c0:8003::153\n 3. verify domain in user site\n\n\nprofile-&gt;settings-&gt;Code, planning, and automation-&gt;Pages-&gt;Add a domain\n\n\nenter the domain name\n\n\nopen Namesilo DNS management and add\n 123TYPE TXT\tHOSTNAME: _github-pages-challenge-....(your TXT record)\tAddress: asdd324fsdf(your code)\n\n\nVerify until success\n\n\n 4. add custom domain in repository\n\nxxx.github.io-&gt;settings-&gt;Pages-&gt;Custom domain\nadd your domain and save\nclick enforce HTTPS\n\n ä½¿ç”¨ Namesilo åŸŸåé…ç½® Github Pages\n 1. åœ¨ Namesilo è´­ä¹°åŸŸå\n 2. ç®¡ç†åŸŸåçš„ DNS\nåœ¨ Namesilo çš„åŸŸåç®¡ç†ç•Œé¢ï¼Œæ·»åŠ ä»¥ä¸‹ DNS è®°å½•ï¼š\n1234567891011121314151617TYPE A    HOSTNAME: è¿™é‡Œç•™ç©º    Addressï¼ˆæ·»åŠ å››æ¬¡ä¸åŒçš„åœ°å€ï¼‰:     185.199.108.153    185.199.109.153    185.199.110.153    185.199.111.153TYPE CNAME    HOSTNAME: www    Address: xxx.github.ioï¼ˆå°† xxx æ›¿æ¢ä¸ºä½ çš„ GitHub ç”¨æˆ·åï¼‰TYPE AAAAï¼ˆå¯é€‰ï¼Œç”¨äº IPv6ï¼‰    HOSTNAME: ä¿æŒç©ºç™½    Address:     2606:50c0:8000::153    2606:50c0:8001::153    2606:50c0:8002::153    2606:50c0:8003::153\n 3. åœ¨ GitHub ç”¨æˆ·ç«™ç‚¹éªŒè¯åŸŸå\n\n\nè¿›å…¥ GitHub ä¸ªäººèµ„æ–™é¡µï¼Œä¾æ¬¡ç‚¹å‡»ï¼šSettingsï¼ˆè®¾ç½®ï¼‰ -&gt; Code, planning, and automationï¼ˆä»£ç ã€è®¡åˆ’å’Œè‡ªåŠ¨åŒ–ï¼‰ -&gt; Pagesï¼ˆé¡µé¢ï¼‰ -&gt; Add a domainï¼ˆæ·»åŠ åŸŸåï¼‰\n\n\nè¾“å…¥ä½ çš„åŸŸå\n\n\næ‰“å¼€ Namesilo çš„ DNS ç®¡ç†ç•Œé¢ï¼Œæ·»åŠ ä»¥ä¸‹è®°å½•ï¼š\n123TYPE TXT\tHOSTNAME: _github-pages-challenge-....(ä½ çš„ TXT è®°å½•)\tAddress: asdd324fsdf(ä½ çš„ä»£ç )\n\n\nç­‰å¾…å¹¶éªŒè¯ï¼Œç›´åˆ°æˆåŠŸ\n\n\n 4. åœ¨ GitHub ä»“åº“ä¸­æ·»åŠ è‡ªå®šä¹‰åŸŸå\n\nè¿›å…¥ä½ çš„ GitHub Pages ä»“åº“ï¼ˆä¾‹å¦‚ xxx.github.ioï¼‰ï¼Œä¾æ¬¡ç‚¹å‡»ï¼šSettingsï¼ˆè®¾ç½®ï¼‰ -&gt; Pagesï¼ˆé¡µé¢ï¼‰ -&gt; Custom domainï¼ˆè‡ªå®šä¹‰åŸŸåï¼‰\næ·»åŠ ä½ çš„åŸŸåå¹¶ä¿å­˜\nå‹¾é€‰ Enforce HTTPSï¼ˆå¼ºåˆ¶ HTTPSï¼‰\n\nè¿™æ ·ï¼Œä½ å°±æˆåŠŸåœ°å°† Namesilo åŸŸåé…ç½®åˆ°ä½ çš„ GitHub Pages ä¸Šäº†ã€‚\n","slug":"Namesilo_GithubPage","date":"2024-06-04T17:49:36.000Z","categories_index":"Article","tags_index":"Domain,Github","author_index":"Huaji1hao"},{"id":"156564d90d95b8548776617619625b2a","title":"Something Interesting","content":" QAQ\n\nä½ çœŸçš„ä¼šç™¾åº¦å—ï¼Ÿ\nä½ è°·æ¨¡æ¿\nå‰ªè´´æ¿è¿·å®«\n2048\n2048æœä»£ç‰ˆ\nç»™ç¥çŠ‡æ¨èä¸€ä¸ªæ„å›¾ç½‘ç«™\n ä¸‡èƒ½å¤´æ–‡ä»¶ï¼ˆæ‰‹åŠ¨æ»‘ç¨½ï¼‰ï¼š\n1include &lt;bugs/stdc--.h&gt;\n\n å¿«è¯»\n123456inline int read()&#123;    register int x=0,f=1;char c=getchar();    while(c&lt;&#x27;0&#x27;||c&gt;&#x27;9&#x27;)&#123;if(c==&#x27;-&#x27;)f=-1;c=getchar();&#125;    while(c&gt;=&#x27;0&#x27;&amp;&amp;c&lt;=&#x27;9&#x27;) x=(x&lt;&lt;3)+(x&lt;&lt;1)+(c^48),c=getchar();    return x*f;&#125;\n\n å¿«æ’\n12345678910111213141516void qsort(int a[], int l, int r)&#123;    // sort a[] in the range of [l, r].\tif(l == r) return;\tint i = l, j = r;\tint pivot = a[l + (r - l) / 2];\tdo&#123;\t\twhile(a[i] &lt; pivot) i++;\t    while(a[j] &gt; pivot) j--;\t    if(i &lt;= j)&#123;\t    \tint tmp = a[i];a[i] = a[j];a[j] = tmp;\t        i++;j--;\t\t&#125;\t&#125;while(i &lt;= j);\tif(l &lt; j) qsort(a, l, j);\tif(i &lt; r) qsort(a, i, r);&#125;\n\n å¿«é€Ÿå¹‚\n12345678910int quickPower(int a,int b)&#123;    // calculate a ^ b.\tint base=1;\twhile(b)&#123;\t\tif(b&amp;1)base*=a;//base%=mod;\t\ta*=a;//a%=mod;\t\tb&gt;&gt;=1;\t&#125;\treturn base;&#125;\n\n ç§¦ä¹éŸ¶\n123456789double QinJiushao(double x)&#123;    // f(x) = a_1 * x^n + ... + a_n-1 * x^2 + a_n * x + a_n+1    double sum = indexNum[1];    for(int i = 1; i &lt;= n; ++i)&#123;        sum *= x;        sum += indexNum[i + 1];    &#125;    return sum;&#125;\n\næœ¬äººç²¾é€šCSSã€JavaScriptã€PHPã€ASPã€Cã€Cï¼‹ï¼‹ã€C#ã€Javaã€Rubyã€Perlã€Lispã€pythonã€Objective-Cã€ActionScriptã€Pascalç­‰å•è¯çš„æ‹¼å†™\nç†Ÿæ‚‰Windowsã€Linuxã€Macã€Androidã€IOSã€WP8ç­‰ç³»ç»Ÿçš„å¼€å…³æœº\n\n\n","slug":"Luogu_profile","date":"2024-06-01T17:49:36.000Z","categories_index":"Memes","tags_index":"Fun","author_index":"Huaji1hao"}]