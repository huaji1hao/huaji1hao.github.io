[{"id":"0f33ba01d60d1829677c62e2b86ee042","title":"High Output Management Reading Notes","content":" High Output Management Reading Notes\n Introduction\n 1. Context of the Book (1983)\n\nGrove wrote the book based on two decades of experience, mainly focusing on the essentials of management relevant to middle managers.\nDespite the passage of time, Grove finds that most fundamental managerial principles remain effective.\n\n 2. Significant Changes in the 1980s\nTwo major shifts impacted management practices:\n\nJapanese Competition in DRAM: Japanese companies became dominant in the memory (DRAM) industry due to superior manufacturing capabilities, challenging American firms like Intel.\nRise of E-mail: E-mail transformed communication, reducing information delays and enhancing coordination within organizations.\n\n 3. Intel’s Response to Competition\n\n\nIntel, initially a major DRAM producer, struggled against Japan’s aggressive pricing and high-quality production.\nLesson:\n\n“being second best in a tough environment is just not good enough”\n\n\n\nThis competition forced Intel to pivot, focusing on microprocessors instead, which was a painful yet crucial strategic shift.\nLesson:\n\nAdaptation is essential in a competitive environment;\nsticking to old strengths may no longer be viable.\n\n\n\n 4. Globalization’s Impact\n\nGlobalization means that both capital and labor can easily move across borders.\nThis results in global competition, where employees must compete with others worldwide, making time and efficiency critical differentiators.\n\n 5. Japanese Management Model\n\nJapanese firms were noted for their quick decision-making, often sitting in proximity for instant communication.\nHowever, American companies adapted to the digital age more quickly, leveraging e-mail for rapid, far-reaching communication.\n\n 6. The New Managerial Environment\n\nBusinesses must now operate in an increasingly unpredictable environment due to globalization and rapid information flows.\nManagers are encouraged to “let chaos reign, then rein in chaos”—embracing disorder as a temporary state to adapt and innovate.\n\n 7. Middle Managers as “Micro CEOs”\n\nMiddle managers should view themselves as CEOs of their teams, responsible for performance and productivity, independent of higher-level company-wide decisions.\n\n 8. Core Ideas of the Book\n\n\nOutput-Oriented Management:\nApplying manufacturing principles to management, focusing on measurable output.\n\n\nManagerial Leverage:\nIncreasing a manager’s effectiveness by influencing the productivity of their team or organization.\n\n\nPeak Performance in Teams:\nStriving to elicit the best performance from team members consistently.\n\n\n 9. Individual Competitive Advantage\n\nIn a globalized, competitive landscape, every employee must think of themselves as a sole proprietor, responsible for maintaining their own “individual competitive advantage.”\nCareer success today requires continuous learning, value creation, and adaptability, unlike in past decades where stable corporations took care of their employees’ careers.\n\n 10. Key Questions for Self-Assessment\nGrove encourages managers to reflect on three main questions to evaluate their effectiveness:\n\nAre you adding real value? – Consider if you’re truly enhancing the output of those you manage or merely moving information along.\nAre you plugged in? – Stay connected with developments in your organization and industry; don’t rely on others to keep you informed.\nAre you trying new ideas? – Actively test new techniques and technologies rather than waiting for others to innovate.\n\n 11. An Optimistic Yet Realistic Outlook\n\nGrove sees potential for increased productivity and wealth but acknowledges that people often resist change.\nsurvival requires adding value continually. He believes this adaptability is key to thriving amidst rapid change.\n\n 12. Grove’s Philosophy on Management\n\nBased on his experience at Intel, Grove argues that using methods of:\n\nProduction – Applying structured, output-oriented approaches to management.\nManagerial Leverage – Maximizing the impact of managerial actions on the team’s productivity.\nPeak Performance – Motivating teams to consistently perform at their best.\n\n\nThese principles are universally applicable across professions, from lawyers to engineers, and not limited to traditional business managers.\n\n","slug":"High Output Management Reading Notes","date":"2024-10-28T00:41:45.000Z","categories_index":"Notes","tags_index":"Management","author_index":"Huaji1hao"},{"id":"35e0a70213c605a9cb06172ffb21a27c","title":"IELTS Reading, Listening and Speaking","content":" IELTS Reading, Listening and Speaking\n 阅读技巧\n 判断题\n 思路树:\n\n 总结:\n\n\n\n\n\n\n\n\n\n总的来说，先看原文和问题命题是否可共存的，不可共存则选False，或者\n\n同义替换后，其中一部分短语被替换为不同意思的词\n时间顺序相反，或者时间与发生的事情不匹配\n\n若意义相同或者原文表达的范围更小，则选True，\n其他都是Not Given\n 错误总结\n\n全部完成后，先检查填空单词拼写\n选择题先找同义替换，再理解选择，选项读不懂可以尝试拆分成短语\n非常明显一样的不一定是答案，同义替换正确的概率更高\n坚定自己的选择，不要为了凑顺序而修改答案\n\n 听力技巧\n 选择题\n\n\n\n\n\n\n\n\n\n更加关注某一个人否定某一话题之后提出的东西，那更大概率是正确的\n 匹配题\n 人名或地点\n\n\n\n\n\n\n\n\n\n\n第一感觉不一定对，最好听完找同义替换\n相信自己听到的短语的偏好选项\n尽量不要修改前面听到的答案，除非很确定\n大概率是按顺序的，因此应该以不变的专有名词(人名，地点等)为基准，听到的专有名词顺序有大概率可能就是题目的答案的排序(除了可能重复的那个专有名词需要更多加关注)\n\n 填空题(Part 4)\n 特点：\n\n答案词会被读得很清晰\n不出题的句子很少被替换\n填空的句子前或后大概率有替换\n\n\n\n\n\n\n\n\n\n\n通过不出题的句子来定位\n 口语技巧\n Part 1 示例回答\nAre you studying or working?\nI’m still a student. I’m doing my graduates in University of Nottingham.\nWhat subjects are you studying?\nI’m majoring in computer science with a focus on artificial intelligence.\nDo you like your subject?\nYes, I really enjoy it. I have been fascinated by artificial intelligence since I was young. Studying this subject helps me understand the principles behind intelligence, how it interacts with humans, and its future developments.\nWhy did you choose to study computer science with artificial intelligence?\nInitially, I was studying computer science without a focus. In my second year, I had the option to specialize in many directions. My mother consulted with her boss, a successful executive, who advised that artificial intelligence has great potential for future development. Based on this advice, I decided to focus on AI.\nDo you think your subject is popular in your country?\nYes, it is very popular, especially with the recent rise of technologies like ChatGPT. Many companies in China are developing their own AI models, which has significantly impacted the business sector\n Part 2\nThank you so much for your patience, I’m all set now.\nToday, based on this question xxxx, I am going to delve into the story of xxx.\n 口语句型场景\n 时间，亲情，友情 (priceless)\nWell I think time is priceless and invaluable, it transends the value itself. Because it cannot be measured by money or finacial success, instead it is the yardstick of how meaningful your life is.\n 有氧运动\nI would like to play basketball with my friends coz you know that’s a great aerobic exercise\n 兴趣\nSomething adds a new dimension to my life\n 有收获的经历/穿戴贵重物品/帮助他人\nWhen I xxx, it is also a really rewarding experience for me. I can also feel a sense of achievement and self worth and that feeling can just boost my confidence and self esteem.\nYou know that would be really helpful for my mental health.  / That would be good on sb’s personal development and growth.\n 框架：\n\n情景： I was with sb doing Sth\n特定事实：that’s when I found\n印象：I was like: “….”\n产生交集：so decided to\n验证印象：it turns out that\n\n","slug":"IELTS_Reading_and_Writing","date":"2024-06-13T23:03:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"dd102e7597f052d8f60bce0e3074afe4","title":"IELTS Writing Task 1","content":" Writing Task 1\n\n\n\n\n\n\n\n\n\n核心：用你的语言呈现图中的信息\n(尽量避免总结，个人观点和思考)\n注意：\n\n第三人称单数的s！\n单词拼写的正确性\n冠词an和a\n\n 数据类模板\n 第一段，介绍图表：\nThe given xxx chart/graph illustrates/demonstrates the details concerning xxx (time…)\n 第二段，明显内容：\nIn general/Generally, it could be obviously witnessed that xxx.\nWhat is more, xxx\nMoreover, it is noteworthy that …\n 第三段，细节数据：\nUpon closer examination of the data, it is discernible that …\n 第四段，细节数据：\nFurthermore, xxx.\nMoreover, it is palpable and noteworthy that xxx.\n 词汇\n\n上升：\n\nsurge 急速上升\nmodest rise 适度的上升\nincrease 增加\nclimb 攀升\ngrow 增长\nescalate 上升\nspike 猛增\nsoar 飙升\njump 跳跃式上升\n\n\n\n\n下降：\n\ndecline 下降\ndecrease 减少\ndrop 下跌\nfall 下降\nplummet 暴跌\nreduce 减少\nsink 下沉\nslide 滑落\ndip 轻微下降\n\n\n\n\n稳定：\n\nrelatively stable 相对稳定的\nsteady 稳定的\nexhibit minimal fluctuations 波动很少\nremain constant 保持不变\nunchanged 不变的\nmaintain 保持\nflat 稳定的\nlevel off 趋于平缓\nplateau 达到稳定期\n\n\n\n\n波动：\n\nshow considerable volatility 展现了相当大的波动性\nfluctuate 波动\noscillate 震荡\nswing 摇摆\nvary 变化\nundulate 波动\nebb and flow 起伏\ninstability 不稳定\nerratic 变化无常的\n\n\n\n 第二段具体句式(总体)\n\n持续地上升，下降：\n\nThe number/percentage/xxx of A exhibits a persistent upward/downward trajectory from 2000 to 2005.\nThe data reveals a consistent rise in xxx.\nThe xxx exhibited a consistent growth pattern during the period between 2000 and 2005\nThe graph depicts a continuous upward trend in xxx.\n\n\n\n\n急剧下降:\n\nThe figure plummeted dramatically in xxx.\n\n\n\n\n稳定:\n\n\nThe figures plateaued at xxx.\n\n\nThere has been little to no fluctuation in xxx.\n\n\n\n\n\n正负相关:\n\n\nThe number/percentage/xxx is positively/negatively related to xxx.\n\n\nThe two sets of figures move in tandem/opposite directions.\n\n\n\n\n\n中途超过:\n\n\nThe number/percentage/xxx of A surpassed that of B from 2000 to 2005.\n\n\nA outpaces B in terms of xxx.\n\n\nA surpass B but a noticeable margin.\n\n\n\n\n 第三段具体句式（细节）\n\n数据是多少\n\nThere was a reported figure of 100 for xxx in 2000.\n\n\n\n\n数据比谁高多少\n\nThe number of A was higher than that of B by 100.\nThe data indicates that A significantly outstripped B, with A’s figure reaching xxx compared to B’s xxx.\n\n\n\n\n倍数关系\n\nThe quantity of A was threefold that of B, which an accounting for xxx, a tripling compared to B’s xxx.\nThe changes of A generally mirrored those of B, except …\n\n\n\n\n数据变到了多少\n\nThe number of A experienced a remarkable surge, elevating sharply from 100 to 200.\nThere is a significant surge in the number of A, escalating from xxx in 2000 to xxx in 2005.\nxxx began with a significant increase of 6% in January, followed by a sharp decline to -3% in June. (转折)\n\n\n\n\n数据波动\n\nThe data for A demonstrated considerable fluctuations, oscillating between xxx and xxx throughout the 10 years.\n\n\n\n\n数据稳定\n\n\nThe figures for A remained stable, consistently hovering around from xxx to xxx.\n\n\nThe xxx remained relatively constant, with an average of roughly …\n\n\n\n\n 连接词\n顺承: subsequently, additionally\n对比: in contrast, conversely\n强调: indeed, notably\n举例: for instance\n原因 : owing to, in view of\n结果: consequently\n 范文\n\nLine Graph\nThe graph below gives information about the percentage of the population in four Asian countries living in cities from 1970 to 2020, with predictions for 2030 and 2040. Summarise the information by selecting and reporting the main features, and make comparisons where relevant.\nWrite at least 150 words.\n\nThe line graph illustrates the percentage of the population living in cities in four Asian countries from 1970 to 2020, with predictions for 2030 and 2040.\nIn general, it is evident that the urban populations in all four countries followed persistent upward trajectories from 1970 to 2020, and are predicted to continue growing steadily. Moreover, the trends for the Philippines and Thailand follow a comparable pattern, while Indonesia and Malaysia exhibit similar increasing patterns.\nUpon closer examination, the urban populations of both Malaysia and the Philippines began at approximately 31 percent in 1970. However, while Malaysia’s urban population is predicted to surge to 81 percent by 2040, the Philippines is expected to see a more modest rise, reaching 53 percent.\nFurthermore, Thailand’s urban population rate started at 18 percent, and Indonesia’s at 14 percent in 1970. However, by 2020, Indonesia had surpassed Thailand, and it is projected to continue growing rapidly to 60 percent by 2040, while Thailand’s is predicted to reach 48 percent.\n\nThe graph below gives information on the numbers of participants for different activities at one social centre in Melbourne, Australia for the period 2000 to 2020.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\nThe line graph provides information on the number of participants in various activities at a social center in Melbourne, Australia, from 2000 to 2020.\nIn general, the film club consistently had the largest proportion of participants compared to other activities. Additionally, table tennis was moderately popular before 2010, but the number of participants surged significantly in the following decade.\nUpon closer examination, the number of participants in martial arts remained relatively stable, hovering between 32 and 38 from 2000 to 2020. In contrast, the number of participants in amateur dramatics started at around 27 before 2005, followed by a dramatic decline to only 6 participants by 2020.\nIn addition, the musical performances club had no participants until 2005, after which the number steadily grew to 19 by 2020. The number of table tennis participants started at 17 in 2000 and surpassed amateur dramatics in 2010, eventually reaching 53 participants by 2020.\n\nThe line graph shows the sales of children’s books, adult’s fictions and educational books between 2002 and 2006 in one country.\n\nThe given line graph depicts the figures for three kinds of books in a certain nation between the years 2002 and 2006.\nIn general, it is observable that the sales of children’s book exhibited a consistent growth pattern during the aforementioned period. Moreover, it is noteworthy that the sales of educational books remained lower than those of others from 2002 to 2005.\nUpon closer examination of the data, it is discernible that the sales of children’s book began at approximately 32 million dollars, escalated to around 46 million dollars in 2005, and culminated at its highest point in 2006. Conversely, the sales of educational books remained relatively constant, with an average of roughly 28 million dollars.\nFurthermore, the sales of adults’ fiction commenced at about 45 million dollars in 2002 and underwent a reduction of about 9 million dollars in 2003. Then, a marginal increase was observed followed by a drop of approximately 10 million dollars in the subsequent years.\n\nThe graph below shows the average monthly change in the prices of three metals during 2014. Summarise the information by selecting and reporting the main features,and make comparisons where relevant.\n\nThe given line graph illustrates the average monthly percentage change in the prices of copper, nickel, and zinc during 2014.\nIn general, it is evident that the average monthly percentage changes in the prices of nickel and zinc fluctuated throughout the year, while copper prices remained relatively stable. Specifically, the percentage change in zinc prices rose towards the end of the year, whereas nickel prices experienced a decline. Moreover, the stability in the percentage changes of copper prices is noteworthy compared to the other two metals.\nUpon closer examination of the data, it is clear that the price of nickel began with a significant increase of 6% in January, followed by a sharp decline to -3% in June. After maintaining a steady trajectory for three months, there was a rapid increase to 1% in November. Furthermore, the changes in zinc prices generally mirrored those of nickel, except in January when zinc showed a modest rise of 1%. Additionally, the price of copper exhibited minimal fluctuations, hovering between -0.5% and 2% throughout the year.\nIn conclusion, while nickel and zinc prices showed considerable volatility in 2014, copper prices remained relatively stable, highlighting the distinct price behaviors of these metals.\n\n\n\n\nBar Chart\nThe chart below shows the number of households in the US by their annual income in 2007, 2011 and 2015. Summarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\nThe bar chart provides information about the number of US households categorized by different annual income brackets in the years 2007, 2011, and 2015.\nIn general, it can be observed that lower-income groups had a higher number of households compared to wealthier groups, with the exception of the highest income bracket. Additionally, the second-highest income group consistently had the fewest households throughout the period.\nLooking more closely at the data, the number of households in the lowest income group (less than $25,000) and the second-lowest income group ($25,000-$49,999) were quite similar, with both standing at around 26 million in 2007. By 2015, both groups saw a modest increase, reaching approximately 28 million. Meanwhile, households with an annual income between $50,000 and $74,999 remained relatively stable, hovering around 22 million over the same period.\nFurthermore, the second-highest income group ($75,000-$99,999) experienced little fluctuation, remaining steady at approximately 14 million households between 2007 and 2015. Households earning $100,000 or more represented the largest group in 2015, increasing from 29 million in 2007 to 33 million by 2015.\n\n\n 流程图\n 第一段，介绍流程\nThe diagram demonstrates the sequence of events in the xxx process.\nThe flowchart exhibits the various steps required to complete the xxx process.\nThe diagram demonstrates the intricate details of xxx, providing a comprehensive overview of its integral components.\n 第二段，宏观信息\nGenerally, it could be witnessed that totally 18 steps in relation to the whole process.\nWhat is more, xxx\nThe process can be meticulously broken down into a total xxx distinct phrases, each of which plays a pivotal role in the overall sequence.\n 第三第四段，具体流程\nUpon closer examination towards the process, xxx.\nMoreover, xxx.\n\n直线类\n流程从A开始，到B结束\n\n\nStarting from A, the process moves through several steps and ends at B.\n\n\nCommencing with the initiation of A, the progression then methodically transitions through a series of intricately interwoven stages, culminating in the finalization at B.\n\n\n在进入下一个步骤之前，xxx发生了\n\nBefore advancing to the next phase, xxx occurs.\n\nA结束后，步骤就到了B\n\nOnce A is finished, the process moves on to B\n\n\n\n\n分支类\n\n\nIn the mean time, xxx is happening alongside xxx.\n\n\nOnce reaching the juncture at xxx, the process splits into two divergent pathways: one leading towards xxx and the other navigating towards xxx.\n\n\n\n\n\n循环类\nA cyclic pattern is evident, with xxx recurrently looping back to xxx, thereby perpetuating a continuous cycle of operations.\n\n\n\nArticles\nThe diagram below shows how a biofuel called ethanol is produced. Summarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\nThe given chart illustrates the process of producing ethanol from plants and trees. This cycle for biofuel production involves three main stages: harvesting and preparing plant material for processing, producing ethanol by chemical processing, and then using the fuel for transportation before the cycle begins again.\nIn the first stage, plants and trees absorb sunlight and capture carbon dioxide from atmosphere to grow. Once mature, those plants are harvested using agricultural machinery, after which they undergo pre-processing equipment where they are converted into cellulose, which is a crucial component for ethanol production.\nIn the following stage, the cellulose is transported into a chemical factory for further processing, and then the cellulose is degraded into sugars which are subsequently transformed into bioethanol with the addition of microbes. The produced ethanol can be utilized as fuel for various vehicles, including cars, trucks, and airplanes.\nDuring the combustion of ethanol in engines, carbon dioxide is released into the atmosphere. This carbon dioxide is then reabsorbed by plants and trees, continuing the same procedure cycle.\n\n\n 位置图\n 第一段\nThe map highlights the notable changes took place in xxx from 2002 to 2003.\nThe maps illustrates the changes that have taken places in xxx over the years, specially between xxx and xxx.\nThe provided map portrays the evolving landscape of xxx(地区) over the whole period.\n 第三第四段\nIn the central region of A, a new xxx has been constructed.\nxxx被修建在A区域的中间\nDuring the period, the A area has experienced substantial growth and improvement, including the new-building xxx.\nA区域扩张了，包括一个新建的xxx\n\n某物在某位置\n\nNestled in the xxx quadrant of the depicted scheme, a prominent xxx emerges.\nIn the xxx part of the diagram, you can see a noticeable xxx.\nCommanding a central position within the illustration, the xxx is conspicuously situated.\nPerched at the edge of the delineated region, the xxx marks its presence.\n\n\n\n\n旁边有什么\nAbutting the A, B asserts itself.\n\n\n\n面积增大\nA palpable enlargement in the domain of the xxx is observed, suggesting spatial expansion.\nThere is a noticeable increase in the size of the xxx, indicating it has expanded\n\n\n\n某设施更换\nWhere the xxx once stood in xxx, now the xxx makes its mark\n\n\n\n某设施数量增加\nxxx now boasts an increased array of xxx\n\n\n\nArticles\nThe maps below show an industrial area in the town of Norbiton, and planned future development of the site.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\n\nThe maps illustrate the current layout of the Norbiton industrial area and the proposed future development of the region.\nIn general, it is evident that more residential areas and facilities will replace the current factories. Additionally, a bridge will be constructed over the river, linking the northern farmland to the central area.\nUpon closer examination, the main circular road will remain intact, while an additional smaller roundabout will be added to the southern part of the primary road, with a medical center located to its northeast. A road branching off to the right will lead to a school, and another road will extend northwards from the center. A playground will be established in this area, with houses surrounding the street.\nAdditionally, a new road will extend to the northwest from the central roundabout, with three housing units planned for the north of this road. A shopping area will also be constructed to the west.\n\nThe plans below show a harbour in 2000 and how it looks today.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\n\nThe maps illustrate the changes in Porth Harbour between the year 2000 and the present day.\nIn general, the overall layout of the harbour has remained largely unchanged, except for some modifications in land use and the addition of new structures. Moreover, the main dock for passenger ferries remains the same, though an additional dock has been constructed to accommodate more boats.\nUpon closer examination, traveling along the main road from north to south, the first left turn still leads to a car park, with the original showers and toilets to the north and new ones to the south. Additionally, the marina for private yachts and the fishing boats area have swapped positions, with the yachts now located where the fishing boats used to be in the south.\nFurthermore, the road leading to the lifeboat has been renovated. The southern car park is now only accessible from the main road, as the old branch road has been removed. A new southern road has been added, leading to a hotel that has replaced the disused castle. Cafes and shops have also been constructed west of the lifeboat, and the public beach in the south has been converted into a private beach for hotel guests.\n\n\n","slug":"Writing_Task_1","date":"2024-06-11T15:55:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"a5201f72282a8a0869129d066b95cef6","title":"Language and Computing","content":" Turing Problem\nType-0-languages = Recursively Enumerable Languages = Semi-decidable Languages\nDecidable languages = Recursive languages\n\nif there is a Turing Machine that accepts it and always stop on any word\n\nContext-sensitive languages(Type-1-languages): subset of recursive languages\n\nThese are grammars where the left-hand side of a production is always shorter than the right-hand side\n\nHalt problem: the language of encodings of Turing machines that will always stop\nThere are languages that are accepted by a TM (i.e., type 0 languages) but that are undecidable\n DFA\nA deterministic finite automaton (DFA) A=(Q,Σ,δ,q0,F)A = (Q,Σ, δ, q_0, F)A=(Q,Σ,δ,q0​,F) is given by:\n\nA finite set of statesstatesstates QQQ\nA finite set of input symbols, the alphabet,Σalphabet, Σalphabet,Σ\nA transitiontransitiontransition functionfunctionfunction δ∈Q×Σ→Qδ \\in Q \\times Σ \\rightarrow Qδ∈Q×Σ→Q\nAn initialinitialinitial statestatestate q0∈Qq_0 \\in Qq0​∈Q\nA set of finalfinalfinal statesstatesstates F⊆QF \\sube QF⊆Q\n\n The language of DFA\nUsing the extended transition function δ^\\hat{\\delta}δ^, we define the language L(A)L(A)L(A) of a DFA AAA formally:\n\n\nδ^(q,ϵ)=q\\hat{\\delta}(q,\\epsilon) = qδ^(q,ϵ)=q\n\n\nδ^(q,xw)=δ^(δ(q,x),w)\\hat{\\delta} (q, xw) = \\hat{\\delta}(\\delta(q,x),w)δ^(q,xw)=δ^(δ(q,x),w)\n\n\nL(A)={w  ∣  δ^(q0,w)∈F}L(A) = \\{w \\;|\\; \\hat{δ}(q_0,w) \\in F\\}L(A)={w∣δ^(q0​,w)∈F}\n\n\n NFA\nA nondeterministic finite automaton (NFA) A=(Q,Σ,δ,S,F)A = (Q,Σ, δ, S, F)A=(Q,Σ,δ,S,F) is given by:\n\nA finite set of statesstatesstates QQQ,\nA finite set of input symbols, the alphabet,Σalphabet, Σalphabet,Σ\nA transitiontransitiontransition functionfunctionfunction δ∈Q×Σ→P(Q)δ \\in Q \\times Σ \\rightarrow \\mathcal{P}(Q)δ∈Q×Σ→P(Q),\nA set of initialinitialinitial statesstatesstates S⊆QS \\sube QS⊆Q,\nA set of finalfinalfinal (or acceptingacceptingaccepting) states F⊆QF \\sube QF⊆Q.\n\n The language of NFA\nδ^∈P(Q)×Σ∗→P(Q)\\hat{δ} \\in \\mathcal{P}(Q)\\timesΣ^∗ \\rightarrow \\mathcal{P}(Q)δ^∈P(Q)×Σ∗→P(Q), δ^(P,w)\\hat{δ}(P,w)δ^(P,w) is set of states that are reachable from one of states in PPP on word www\n\nδ^(P,ϵ)=P\\hat{\\delta}(P, \\epsilon) = Pδ^(P,ϵ)=P\nδ^(P,xw)=δ^(⋃{δ(q,x)  ∣  q∈P},w)\\hat{\\delta}(P, xw) = \\hat{\\delta}(\\bigcup\\{\\delta(q,x)\\;|\\;q\\in P\\}, w)δ^(P,xw)=δ^(⋃{δ(q,x)∣q∈P},w)\nL(A)={w  ∣  δ^(S,w)∩F≠∅}L(A) = \\{w\\;|\\;\\hat{\\delta}(S,w)\\cap F\\not =\\empty\\}L(A)={w∣δ^(S,w)∩F=∅}\n\n Context-free Grammar\nA context-free grammar G=(N,T,P,S)G = (N, T, P, S)G=(N,T,P,S) is given by\n\nA finite set NNN of nonterminalnonterminalnonterminal symbolssymbolssymbols or nonterminalsnonterminalsnonterminals.\nA finite set TTT of terminalterminalterminal symbolssymbolssymbols or terminalsterminalsterminals.\nN∩T=∅N \\cap T = \\emptysetN∩T=∅; i.e., the sets NNN and TTT are disjoint.\nA finite set P⊆N×(N∪T)∗P \\sube N \\times(N \\cup T)^∗P⊆N×(N∪T)∗ of productions. A production (A,α)(A, α)(A,α), where A∈NA \\in NA∈N and α∈(N∪T)∗α \\in (N \\cup T)^∗α∈(N∪T)∗ is a sequence of nonterminal and terminal symbols. It is written as A→αA \\rightarrow αA→α in the following.\nS∈NS\\in NS∈N: the distinguished start symbol.\n\n The language of a grammar\nL(G)⊆T∗L(G) \\sube T^∗L(G)⊆T∗, consists of all terminal sentential forms:\n\nL(G)={w∈T∗  ∣  S⇒G∗w}L(G) = \\{w\\in T^*\\;|\\;S\\xRightarrow[G]{*}w\\}L(G)={w∈T∗∣S∗G​w}\n\n Pushdown Automaton\nA Pushdown Automaton P=(Q,Σ,Γ,δ,q0,Z0,F)P = (Q,Σ, Γ, δ, q_0,Z_0, F)P=(Q,Σ,Γ,δ,q0​,Z0​,F) is given by the following data\n\nA finite set QQQ of states,\nA finite set ΣΣΣ of input symbols (the alphabet),\nA finite set ΓΓΓ of stack symbols,A\ntransition function\nδ∈Q×(Σ∪{ϵ})×Γ→Pfin(Q×Γ∗)δ \\in Q \\times (Σ \\cup \\{ϵ\\}) \\times Γ \\rightarrow P_{fin}(Q \\times Γ^∗)δ∈Q×(Σ∪{ϵ})×Γ→Pfin​(Q×Γ∗)\nHere Pfin(A)P_{fin}(A)Pfin​(A) are the finite subsets of a set; i.e., this can be defined as\nPfin(A)={X∣X⊆A∧X is finite.}P_{fin}(A) = \\{X | X \\sube A \\wedge X \\text{ is finite.}\\}Pfin​(A)={X∣X⊆A∧X is finite.}\nThus, PDAs are in general nondeterministic because they may have a choice of transitions from any state. However, there are always only finitely many choices.\nAn initial state q0∈Qq_0 \\in Qq0​∈Q,\nAn initial stack symbol Z0∈ΓZ_0 \\in ΓZ0​∈Γ,\nA set of final states F⊆QF \\sube QF⊆Q.\n\n ID(Instantaneous Description)\n\n\n\n\n\n\n\n\n\nSuch a triple (q,w,γ)∈Q×Σ∗×Γ∗(q,w, γ) \\in Q\\timesΣ^∗\\timesΓ^∗(q,w,γ)∈Q×Σ∗×Γ∗ is called an Instantaneous Description (ID)\n Acceptance by final state\n\nL(P)={w  ∣  (q0,w,Zo)⊢∗(q,ϵ,γ)∧q∈F}L(P) = \\{w\\;|\\;(q_0,w,Z_o)\\vdash^*(q,\\epsilon,\\gamma)\\wedge q \\in F\\}L(P)={w∣(q0​,w,Zo​)⊢∗(q,ϵ,γ)∧q∈F}\n\n Acceptance by empty stack\n\nL(P)={w  ∣  (q0,w,Z0)⊢∗(q,ϵ,ϵ)}L(P)=\\{w\\;|\\;(q_0,w,Z_0)\\vdash^*(q,\\epsilon, \\epsilon)\\}L(P)={w∣(q0​,w,Z0​)⊢∗(q,ϵ,ϵ)}\n\n Deterministic PDAs\n\n∣δ(q,x,z)∣+∣δ(q,ϵ,z)∣≤1,q∈Q,x∈Σ,z∈Γ|\\delta(q,x,z)| +|\\delta(q,\\epsilon,z)| \\leq 1, q \\in Q,x\\in \\Sigma,z\\in \\Gamma∣δ(q,x,z)∣+∣δ(q,ϵ,z)∣≤1,q∈Q,x∈Σ,z∈Γ\n\n Turing Machine\nA Turing Machine M=(Q,Σ,Γ,δ,q0,B,F)M = (Q,Σ, Γ, δ, q_0,B, F)M=(Q,Σ,Γ,δ,q0​,B,F) is:\n\nA finite set QQQ of states;\nA finite set ΣΣΣ of symbols (the alphabet);\nA finite set ΓΓΓ of tape symbols s.t. Σ⊆ΓΣ \\sube ΓΣ⊆Γ. This is the case because we use the tape also for the input;\nA transition function\nδ∈Q×Γ→{stop}∪Q×Γ×{L,R}δ \\in Q \\times Γ \\rightarrow \\{stop\\} \\cup Q \\times Γ \\times \\{L, R\\}δ∈Q×Γ→{stop}∪Q×Γ×{L,R}\nThe transition function defines how the machine behaves if is in state qqq and the symbol on the tape is xxx. If δ(q,x)δ(q, x)δ(q,x) = stop then the machine stops otherwise if δ(q,x)=(q′,y,d)δ(q, x) = (q′, y, d)δ(q,x)=(q′,y,d) the machines gets into state q′q′q′, writes yyy on the tape (replacing xxx) and moves left if d=Ld = Ld=L or right, if d=Rd = Rd=R;\nAn initial state q0∈Qq_0 \\in Qq0​∈Q;\nThe blank symbol B∈ΓB \\in ΓB∈Γ but B∉ΣB \\not\\in ΣB∈Σ. Initially, only a finite section of the tape containing the input is non-blank;\nA set of final states F⊆QF \\sube QF⊆Q.\n\n ID(Instantaneous Description)\n\n\n\n\n\n\n\n\n\nAn element (γL,q,γR)∈ID(γ_L, q, γ_R) \\in ID(γL​,q,γR​)∈ID describes a situation where the TM is in state QQQ, the non-blank portion of the tape on the left of the head is γLγ_LγL​ and the non-blank portion of the tape on the right, including the square under the head, is γRγ_RγR​\n The language of a Turing Machine\n\nL(M)={w∈Σ∗  ∣  (ϵ,q0,w)⊢∗(γL,q′,γR)∧q′∈F}L(M) = \\{w\\in \\Sigma^*\\;|\\;(\\epsilon, q_0,w)\\vdash^*(γ_L, q&#x27;, γ_R)\\wedge q&#x27; \\in F\\}L(M)={w∈Σ∗∣(ϵ,q0​,w)⊢∗(γL​,q′,γR​)∧q′∈F}\n\n Predictive parsing\nConsider productions for a nonterminal X\n\nX→α  ∣  βX \\rightarrow \\alpha \\;|\\;\\betaX→α∣β\n\n1234parseX (t : ts) =| t ∈ first(α) -&gt; parse α| t ∈ first(β) -&gt; parse β| otherwise -&gt; Nothing\nSuppose it can be the case that\n\nβ⇒∗ϵ\\beta \\xRightarrow[]{*}\\epsilonβ∗​ϵ\n\n1234parseX (t : ts) =| t ∈ first(α) -&gt; parse α| t ∈ first(β) ∪ follow(X) -&gt; parse β| otherwise -&gt; Nothing\n Disambiguating context-free grammars\n\nE→E+E  ∣  E∗E  ∣  E↑E  ∣  (E)  ∣  NE \\rightarrow E + E \\;|\\; E * E \\;|\\;E \\uparrow E \\;|\\; (E) \\;|\\; NE→E+E∣E∗E∣E↑E∣(E)∣N\nN→0  ∣  1  ∣  2N\\rightarrow0\\;|\\;1\\;|\\;2N→0∣1∣2\n$\\uparrow(right) ;&gt;;*(left);&gt;;+(left) $\n\n The subexpressions of expressions of the highest precedence\n\nE→E1+E1  ∣  E1E\\rightarrow E_1+E_1\\;|\\;E_1E→E1​+E1​∣E1​\nE1→E2∗E2  ∣  E2E_1\\rightarrow E_2*E_2\\;|\\;E_2E1​→E2​∗E2​∣E2​\nE2→E3↑E3  ∣  E3E_2\\rightarrow E_3 \\uparrow E_3\\;|\\;E_3E2​→E3​↑E3​∣E3​\nE3→(E)  ∣  NE_3\\rightarrow (E)\\;|\\;NE3​→(E)∣N\nN→0  ∣  1  ∣  2N\\rightarrow 0\\;|\\;1\\;|\\;2N→0∣1∣2\n\n Make the corresponding productions left- and right-recursive\n\nE→E+E1  ∣  E1E\\rightarrow E+E_1\\;|\\;E_1E→E+E1​∣E1​\nE1→E1∗E2  ∣  E2E_1\\rightarrow E_1*E_2\\;|\\;E_2E1​→E1​∗E2​∣E2​\nE2→E3↑E2  ∣  E3E_2\\rightarrow E_3 \\uparrow E_2\\;|\\;E_3E2​→E3​↑E2​∣E3​\nE3→(E)  ∣  NE_3\\rightarrow (E)\\;|\\;NE3​→(E)∣N\nN→0  ∣  1  ∣  2N\\rightarrow 0\\;|\\;1\\;|\\;2N→0∣1∣2\n\n Elimination of left recursion\n\n","slug":"lac","date":"2024-06-09T15:39:56.000Z","categories_index":"Notes","tags_index":"Language,Automata","author_index":"Huaji1hao"},{"id":"e8690cb5cccdb863caaf29d6fe84aa6a","title":"AI Methods","content":" Introduction, Heuristic search (introduction), Pseudo-random numbers\n Preliminaries\n Decision support\nThis term is used often and in a variety of contexts related to decision making\n System\n\nDegree of dependence of systems on the environment\n\nClosed systems are totally independent\nOpen systems dependent on their environment\n\n\nEvaluations of systems\n\nSystem effectiveness: the degree to which goals are achieved, i.e. result, output\nSystem efficiency: a measure of the use of inputs (or resources) to achieve output, e.g., speed\n\n\n\n Solving problems by searching\n\nSearch for paths to goals\n\ntypical algorithms are the depth firstsearch, breadth first search, uniform cost search, branch and bound, A*\n\n\nSearch for solutions (optimisation)\n\nmore general class than searching for paths to goals\nefficiently finding a solution to a problem in a large space of candidate solutions\nsubsumes the first type, since a path through a search tree can be encoded as a candidate solution\n\n\nSearch in Continuous vs Discrete Space\n\n Solving an (mathematical) optimisation problem - steps\n\nFirst choose a quantity (typically a function of several variables –objective function) to be maximised or minimised, which might be subject to one or more constraints (constraint optimisation)\nNext choose a mathematical or search method to solve the optimisation problem (searching the space of solutions and detecting absolutely the best/optimal solution)\n\n Optimization\n\n\nFundamental problem of optimization is to arrive at the best possible (optimal) decision/solution in any given set of circumstances\n\n\nGlobal Optimization\nGlobal optimization is the task of finding the absolutely best set of admissible conditions to achieve your objective, formulated in mathematical terms\n\n\nIn most cases “the best” (optimal) is unattainable\n\n\nGlobal vs Local Optimum\n\nGlobal Optimum- better than all other solutions (best)\nLocal Optimum- better than all solutions in a certainneighbourhood\n\n\n\n Problem and Problem Instance\n\nProblem refers to the high level question or optimization issue to be solved\nAn instance of this problem is the concrete expression, which represents the input for a decision or optimization problem\n\n Combinatorial optimization problems (COP)\n\nRequire finding an optimal object from a finite set of objects\nFor NP-hard COPs, the time complexity of finding solutions can grow exponentially with instance size\n\n Optimization/Search Methods\n\n\nExact/Exhaustive/Systematic Methods\n\ne.g., Dynamic Programming, Branch&amp;Bound, Constraint Satisfaction, …\n\nlimitations: only work if the problem is structured - in many cases for small problem instances\nquite often used to solve sub-problems\n\n\n\n\n\nInexact/Approximate/Local Search Methods\n\ne.g., heuristics,metaheuristics, hyper-heuristics,…\n\n\n\n Search Paradigms\n\nPerturbative←→Constructive\n\n\nstart from complete solutions\n\n\nstart from partial solutions\n\n\n\ndeterministic ←→ stochastic\n\nprovide the same solution regardless of how many times\ncontain a random component and may return a different solution at each time\n\n\nsystematic ←→ local search\nsequential ←→ parallel\nsingle objective ←→ multi-objective\n\n Heuristic Search/Optimization\n Heuristic Search Methods\n\n\n\n\n\n\n\n\n\nA heuristic is a rule of thumb method derived from human intuition.\n\n\nA heuristic is a problem dependent search method which seeks good, i.e. near-optimal solutions, at a reasonable cost (e.g. speed) without being able to guarantee optimality\n\n\nGood for solving ill-structured problems, or complex well-structured problems (large-scale combinatorial problems that have many potential solutions to explore)\n\n\n Case study: Traveling Salesman Problem (TSP)\n\n\n\n\n\n\n\n\n\n&quot;Given a list of cities and the distances between each pair of cities,what is the shortest possible route that visits each city and returns to the origin city?” – NP hard\nExamples heuristics for TSP\n\nThe nearest neighbour (NN) algorithm - Constructive(Stochastic, Systematic)\nA Constructive Stochastic Local Search Algorithm for TSP(based on NN algorithm)\n\nStep 1: Choose a random city\nStep 2: Apply nearest neighbour to construct a complete solution\nStep 3: Compare the new solution to the best found so far and update the best solution as appropriate\nStep 4: Go-to Step 1 and repeat while the maximum number of iterations is not exceeded (parameter)\nStep 5: Return the best solution\n\n\nPairwise exchange (2-opt) - Perturbative (Stochastic, Local Search)\nA Perturbative Stochastic Local Search Algorithm for TSP(based on 2-opt)\n\nStep 1: Create a random current solution (build a permutation array and shuffle its content)\nStep 2: Apply 2-opt: swap two randomly chosen cities forming a new solution\nStep 3: Compare the new solution to the current solution and if there is improvement make the new solution current solution, otherwise continue\nStep 4: Go-to Step 2 and repeat while the maximum number of iterations is not exceeded (parameter)\nStep 5: Return the current solution\n\n\n\n Drawbacks of Heuristic Search\n\nThere is no guarantee for the optimality of the obtained solutions\nUsually can be used only for the specific situation for which they are designed\nOften, heuristics have some parameters\n\nPerformance of a heuristic could be sensitive to the setting of those parameters\n\n\nMay give a poor solution\n\n Pseudo-random numbers\n Some problems with pseudo-random numbers\n\nShorter than expected periods for some seed states; such seed states may be called ‘weak’ in this context\nLack of uniformity of distribution (e.g., 0.17 appears 100 times in10000 successive numbers while 0.29 appears 5 times more)\nCorrelation of successive values\nThe distances between where certain values occur are distributed differently from those in a random sequence distribution\n\n Components of heuristic search, Hill climbing (HC) , Performance analysis\n Main components of heuristic search methods\n\nRepresentation\nEvaluation function (objective function)\nInitialization (e.g., random)\nNeighborhood relation (move operators)\nSearch process (guideline)\nMechanism for escaping from local optima\n\n Representation\n\n\n\n\n\n\n\n\n\nEncoding of candidate solutions\n Characteristics\n\ncompleteness: all solutions associated with the problem must be represented\nconnexity: a search path must exist between any two solutions of the search space. Any solution of the search space, especially the global optimum solution, can be attained\nefficiency: The representation must be easy to manipulate by the search operators\n\n Example\n\nBinary encoding (e.g. 10110010110010…1011)\n\nGiven a binary string of length N (representing N items), search space size is 2^N\n\n\nPermutation encoding (e.g. for TSP: 1 5 3 2 6 4 7 9 8)\n\nGiven N cities (pubs), search space size is N!\n\n\nInteger encoding (e.g. 1 3 4 5 5 5 4 1 1 … 2 2 1)\n\nFor a general problem with M composite materials to form an N-layer composite structure, search space size is M^N\n\n\nValue encoding (e.g. ATFCTTCGG) (e.g. 1.2324 5.3243 …) (e.g.&lt;back, back, right, forward, left, …&gt;)\nNonlinear encoding (e.g. tree encoding - genetic programming)\nSpecial encodings (e.g. random key encoding)\n\n Evaluation function\n\n\n\n\n\n\n\n\n\nIndicates the quality of a given solution, distinguishing between better and worse solutions\n\nAlso referred to as objective , cost, fitness, penalty, etc.\nServes as a major link between the algorithm and the problem being solved\n\nprovides an important feedback for the search process\n\n\nMany types: (non)separable, uni/multi-modal, single/multi-objective, etc.\nEvaluation functions could be computationally expensive\nExact vs. approximate\n\nCommon approaches to constructing approximate models: polynomials, regression, SVMs, etc.\nConstructing a globally valid approximate model remains difficult, and so beneficial to selectively use the original evaluation function together with the approximate model\n\n\n\n Evaluation Function - Delta (Incremental) Evaluation\n\nKey idea: calculate effects of differences between current search position S and a neighbour S’ on the evaluation function value.\nEvaluation function values often consist of independent contributions of solution components; hence,\nf(S’) can be efficiently calculated from f(S) by differences between S and S’ in terms of solution components\nCrucial for efficient implementation of heuristics/metaheuristics/hyper-heuristics\n\n Neighborhoods\n\n\n\n\n\n\n\n\n\nA neighborhood of a solution is a set of solutions that can be reached from by a simple operator (move operator/heuristic)\n Example neighborhood for binary representation:\nBit-flip operator\n\nflips a bit in a given solution\nHamming Distance\n\nBetween two bit strings (vectors) of equal length is the number of positions at which the corresponding symbols differ.\ne.g. HD(011, 010) = 1\n\n\nIf the binary string is of size n, then the neighborhood size is n\n\nA discrete value is replaced by any other character of the alphabet\n\nIf the solution is of size n and alphabet is of size k , then the neighborhood size is (k - 1) * n\n\nAdjacent pairwise interchange\n\nswap adjacent entries in the permutation (e.g. 5 1 4 3 2 -&gt; 1 5 4 3 2)\nIf permutation is of size n, then the neighborhood size is n - 1\n\nInsertion operator\n\n\ntake an entry in the permutation and insert itin another position (e.g. 5 1 4 3 2 -&gt; 1 4 5 3 2)\n\n\nNeighborhood size: n * (n - 1)\n\n\nExchange operator\n\narbitrarily selected two entries are swapped(e.g. 5 4 3 1 2 -&gt; 1 4 3 5 2)\n\nInversion operator\n\nselect two arbitrary entries and invert the sequence in between them (e.g. 1 4 5 3 2 -&gt; 1 3 5 4 2)\n\n Summary of components\n\nChoosing an appropriate encoding to represent a candidate solution is crucial in heuristic optimisation\nInitialisation could influence the performance of an optimisation algorithm\nEvaluation function guides the search process and fast evaluation is important\n\n Hill climbing algorithms\n Search paradigm\n\n\n\n\n\n\n\n\n\nPerturbative heuristics/operators:\n\nMutational (diversification/exploration) vs.\nHill-climbing (intensification/exploitation)\n\n\n\nMutational heuristics/operator:\nProcesses a given candidate solution and generates a solution which is not guaranteed to be better\n\n\nHill climbing heuristics/operator:\nProcesses a given candidate solution and generates a better or equal quality solution\n\n\n Minimisation problem &amp; Maximisation problem\n\n\nA local search algorithm which constantly moves in the direction of decreasing level/objective value (for a minimisation problem) to find the nadir/the lowest point of the landscape or best/near optimal solution to the problem\n\nThe hill climbing algorithm halts when it detects a nadir value (where no neighbour has a lower value)\n\n\n\nA local search algorithm which constantly moves in the direction of increasing level/objective value (for a maximisation problem) to find the peak/the highest point of the landscape or best/near optimal solution to the problem\n\nThe hill climbing algorithm halts when it detects a peak value (where no neighbour has a higher value)\n\n\n\n Pseudocode\n\n\nPick an initial starting point (current state) in the search space\n\n\nRepeat\n\n\nConsider the neighbors of the current state\n\n\nCompare new point(s) in the neighborhood of the current state with the current state using an evaluation function and choose a new point with the best quality(among them) and move to that state\n\n\n\n\nUntil there is no more improvement or when a predefined number of iterations is reached\n\n\nReturn the current state as the solution state\n\n\n Note of algorithm\n\n\nInitial starting points may be chosen\n\nrandomly\nuse a constructive heuristic/operator(s)\naccording to some regular pattern\nbased on other information (e.g. results of a prior search)\n\n\n\nVariations of hill-climbing algorithms differ in the way for selecting a new solution compared to the current solution\n\n\nimproving vs. non-worsening ( tmpEval &lt; bestEval vs. tmpEval &lt;=bestEval )\n\n\nWhen to stop\n\nIf the target objective is known, then the search can be stopped when that target objective value is achieved\nHill climbing could be applied repeatedly until a termination criterion is satisfied\n\nHowever, there is no point in applying Best Improvement,Next Improvement and Davis’s (Bit) Hill Climbing if there is no improvement after any single pass over a solution\nRandom Mutation Hill Climbing requires consideration\n\n\n\n\n\n Simple hill climbing heuristic\n\n\nSimple Hill Climbing examining neighbors:\n\nBest improvement (steepest descent/ascent)\nFirst improvement (next descent/ascent)\nTrade-off between the number of search steps required for finding a local optimum and the computation time for each search step.\n\nTypically, for First Improvement search steps can be computed more efficiently than when using Best Improvement, since especially as long as there are multiple improving search steps from a current candidate solution, only a small part of the local neighborhood is evaluated by First Improvement. (Best improvement has larger search range)\nHowever, the improvement obtained by each step of First Improvement local search is typically smaller than for Best Improvement and therefore, more search steps have to be applied to reach a local optimum.\nAdditionally, Best Improvement benefits more than First Improvement from the use of caching and updating mechanisms for evaluating neighbors efficiently.\n\n\n\n\n\nStochastic Hill Climbing (randomly choose neighbors)\n\nDavis’s (bit) hill-climbing (DBHC)\nRandom selection/mutation hill climbing\n\n\n\nRandom-restart (shotgun) hill climbing is built on top of hill climbing and operates by changing the starting solution for the hill climbing, randomly and returning the best\n\n\n Hill climbing vs. Random walk\n\nA Hill-climbing method exploits the best available solution for possible improvement but neglect exploring a large portion of the search space\nRandom walk explores the search space thoroughly but misses exploiting promising regions\n\n Advantage:\nVery easy to implement, requiring:\n\na representation;\nan evaluation function;\na measure that defines the neighborhood around a point in the search space.\n\n Disadvantage:\n\nLocal Optimum: If all neighboring states are worse or the same. The algorithm will halt even though the solution may be far from satisfactory\nPlateau (neutral space/shoulder): All neighbouring states are the same as the current state. In other words the evaluation function is essentially flat. The search will conduct a random walk\nRidge/valley: The search may oscillate from side to side, making little progress. In each case, the algorithm reaches a point at which no progress is being made. If this happens, an obvious thing to do is start again from a different starting point\nAs a result, hill climbing algorithm may not find the optimal solution and may get stuck at a local optimum\nNo information as to how much the discovered local optimum deviates from the global (or even other local optima)\nUsually no upper bound on computation time\nSuccess/failure of each iteration depends on starting point\n\n Question Example\nexample: “Assume that Davis’s Bit Hill Climbing , First Improvement Hill Climbing and Steepest Descent Hill Climbing\nalgorithms are applied to a MAX-SAT problem instance resulting in average objective values of 12.4, 34.3 and 25.7, respectively, over 30 runs.”\n\nDavis’s Bit Hill Climbing would perform the best for solving MAX-SAT problems assuming a minimisation problem formulation (✕ -Any comment for 1 instance is valid only for 1 problem instance,not for the whole algorithm)\nDavis’s Bit Hill Climbing performs the best based on the average objective value on this problem instance (✕ - do not know it’s a maximisation / minimisation problem)\nAssuming that the problem is formulated as a maximisation problem, then First Improvement Hill Climbing performs the best based on the average objective value on this problem instance (✓)\n\n Statistical tests\n\nThe null hypothesis states the results are due to chance and are not significant in terms of supporting the idea being investigated\nA p-value/ probability value, is a number describing how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true)\nApply non-parametric statistical test - one tailed:\n\nGiven two algorithms: X vs. Y, X &gt; Y (X &lt; Y) denotes that X(Y) is better than Y(X) and this performance difference is statistically significant within a confidence interval of 95% and X &gt;= Y ( X &lt;= Y) indicates that X(Y) performs better on average than  Y(X) but no statistical significance\nA stronger conclusion can be provided for one instance\nAlways repeat the experiments more than or equal to 30 times for any given instance for a meaningful statistical comparison\n\n\n\n\n Boxplots\n\n\n\n\n\n\n\n\n\nBoxplots illustrates groups of numerical data through their quartiles\n\n Notched boxplots\n\n\n\n\n\n\n\n\n\nNotched boxplots allows you to evaluate confidence intervals (by default 95% confidence interval) for the medians of each boxplot\n\n\nSince the notches in the boxplots A vs. B vs. C do not overlap,you can conclude that with 95% confidence that the true medians do differ between each pair of those algorithms on current instance: A performs significantly better than B as well as C, and B performs significantly better than C\n\n Progress plot - per instance\n\n\n\n\n\n\n\n\n\nObjective value from a run or mean of objective values from multiple runs per iteration/time unit\n\n Metaheuristic\n\n\n\n\n\n\n\n\n\nA metaheuristic is a high-level problem independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic optimization algorithms\n Components of metaheuristic\n\nRepresentation of candidate solutions\nEvaluation function\nInitialisation: E.g., initial candidate solution may be chosen\n\nrandomly use a constructive heuristic\naccording to some regular pattern\nbased on other information (e.g. results of a prior search), and more\n\n\nNeighborhood relation (move operators)\nSearch process (guideline)\nStopping conditions\nMechanism for escaping from local optima\n\n\n Mechanism for escaping from local optima\n\nIterate with different solutions, or restart (reinitialise search whenever a local optimum is encountered)\n\nInitialisation could be costly\nrestart could be partial (e.g. change 10% of previous solution)\ne.g. Iterated Local Search (ILS), GRASP\n\n\nChange the search landscape\n\nChange the objective function (e.g. Guided Local Search)\nUse (mix) different neighborhoods (e.g. Variable Neighbourhood Search, Hyper-heuristics)\n\n\nUse Memory\n(e.g. tabu search (TS))\nAccept non-improving moves\nallow search using candidate solutions with equal or worse evaluation function value than the one in hand\n\nCould lead to long walks on plateaus (neutral regions) during the search process, potentially causing cycles – visiting of thesame states\n\n\nNone of the mechanisms is guaranteed to always escape effectively from local optima\n\n Stopping conditions (examples)\n\nStop if a fixed maximum number of iterations, or moves, or objective function evaluations, or a fixed amount of CPU time is exceeded\nStop if consecutive number of iterations since the last improvement in the best objective function value is larger thana specified number\nStop if evidence can be given than an optimum solution has been obtained (i.e. optimum objective value is known)\nStop if no feasible solution can be obtained for a fixed number of steps/time (a solution is feasible if it satisfies all constraints in an optimisation problem)\n\n Deal with (in)feasible solution\n\n\nsimply reject infeasible solution\n\n\nUse a problem domain specific repair operator\n\ne.g. for 0/1 Knapsack Problem with constraints of 15kg, randomly flip a bit to 0 until the solution in hand feasible:1 1 0 1 0: $16 (18 kg, ✕) -&gt; 1 0 0 1 0: $14 (16 kg, ✕) -&gt; 1 0 0 0 0: $4 (12 kg, ✓)\n\n\n\nPenalise each constraint violation for the infeasible solutions such that they can’t be better than the worst feasible solution for a given instance\n\nSet a fixed (death) penalty value poorer than the worst\n\ne.g., f′(s)f&#x27;(s)f′(s)= if s is infeasible, then min{pi,∀i}/2min\\{p_i,\\forall i\\}/2min{pi​,∀i}/2, pip_ipi​ is the profit from the i th item\n\n\nDistinguish the level of infeasibility of a solution with the penalty\n\ne.g., f′(s)f&#x27;(s)f′(s)= if s is infeasible, then min{pi,∀i}/(2∗(total_weight−capacity))min\\{p_i,\\forall i\\}/(2*(total\\_weight-capacity))min{pi​,∀i}/(2∗(total_weight−capacity))\n\n\n\n\n\n Single Point Based Iterative Search - Local Search Metaheuristics  - Stochastic Local Search\n Pseudocode\n12345678910s0; // starting solutions* = initialise(s0) // e.g., improve s0 or use the same repeatRepeat\t// generate a new solution    s&#x27; = makeMove(s*, memory); // choose a neighbour of s*    accept = moveAcceptance(s*, s&#x27;, memory); // remember s_best    if(accept) s* = s&#x27;; // else reject new solution s&#x27;    Until (termination conditions are satisfied)\n\n\nMove Acceptance decides whether to accept or reject the new solution considering its evaluation/quality\n\n\nAccepting non-improving moves could be used as a mechanism to escape from local optimum\n\n\nEffective search techniques provide a mechanism to balance exploration and exploitation\n\nExploration aims to prevent stagnation of search process getting trapped at a local optimum\nExploitation aims to greedily increase solution quality or probability , e.g., by exploiting the evaluation function\n\n\n\nAim is to design search algorithms/metaheuristics that can\n\nescape local optima\nbalance exploration and exploitation\nmake the search independent from the initial configuration\n\n Iterated Local Search (ILS) - Local Search Metaheuristics - Stochastic Local Search\n1234567891011121314//random or construction heuristics0 = GenerateInitialSolution()s* = LocalSearch(s0) //not always usedRepeat\t// random move    s&#x27; = Perturbation(s*, memory)    // hill climbing    s&#x27; = LocalSearch(s&#x27; )    // remember s_best    s* = AcceptanceCriterion(s*, s&#x27;, memory)     // the conditions that the new local optimum    // must satisfy to replace the current solutionUntil (termination conditions are satisfied)return s*\n Based on visiting a sequence of locally optimal solutions by\n\n\nPerturbing the current local optimum (exploration)\n\n\nA perturbation phase might consist of one or more steps\n\n\nThe perturbation strength is crucial\n\nweak perturbations usually lead to shorter local search phases than strong perturbations, because the iterative improvement algorithm takes less steps to identify a local optimum\nToo small / weak: may generate cycles, fall back into the local optimum just visited leading to a stagnation of the search process\nToo big / strong: good properties of the local optima are lost , similar to a random restart of the search process\n\n\n\n\n\napplying local search/hill climbing(exploitation) after starting from the modified solution\n\n\n Acceptance criteria\n\nExtreme in terms of intensification: accept only improving solutions &lt;-&gt; Extreme in terms of diversification: accept any solution\nOther: deterministic (like threshold), probabilistic (like Simulated Annealing)\n\n Memory\nVery simple use: restart search if for a number of iterations noimproved solution is found\n Guidelines\n\nInitial solution should be to a large extent irrelevant for longer runs\nThe interactions among perturbation strength and acceptance criterion can be particularly important\n\nit determines the relative balance of intensification anddiversification\nlarge perturbations are only useful if they can be accepted\n\n\nAdvanced acceptance criteria may take into account search history,\n\ne.g. by occasionally reverting to incumbent solution\n\n\nAdvanced ILS algorithms may change nature and/or strength of perturbation adaptively during search\nLocal search should be as effective and as fast as possible.\n\nBetter local search generally leads to better ILS performance\n\n\nChoose a perturbation operator whose steps cannot be easily undone by the local search\n\n\n Tabu Search (TS) - Local Search Metaheuristics - Stochastic Local Search\n Basic idea\n\n\n\n\n\n\n\n\n\nuses history (memory structures) to escape from local minima/ maxima\n Pseudocode\n1234567determine initial candidate solution sWhile termination criterion is not satisfied\tdetermine set N&#x27; of non-tabu neighbours of s\tchoose a best improving candidate solution s&#x27; in N&#x27;\tupdate tabu attributes based on s&#x27;\ts = s&#x27;\n\n\nIn each step, move to ‘non-tabu’ best neighbouring solution (admissible neighbours), although it may be worse than current one\n\n\nTo avoid cycles, TS tries to avoid revisiting previously seen solutions\n\n\nTo avoid storing complete solutions, TS bases the memory on attributes of recently seen solutions\n\n\nTabu solution attributes are often defined via local search moves\n\n\nTabu-list contains moves which have been made in the recent past\n\n\nTabu tenure/tabu list length:\nthe length of time/number of steps ttt for which a move is forbidden\n\nttt too low - risk of cycling\nttt too high - may restrict the search too much\nttt = 7 has often been found sufficient to prevent cycling\nt=nt = \\sqrt{n}t=n​\nnumber of tabu moves: 5 ~ 9\n\n\n\nSolutions which contain tabu attributes are forbidden for a certain number of iterations\n\n\n\n\nOften, an additional aspiration criterion is used: this specifies conditions under which tabu status may be overridden (e.g. if considered step leads to improvement in incumbent solution)\n\nIf a tabu move is smaller than the aspiration level then we accept the move (use of aspiration criteria to override tabu status)\n\n\n\n 3 main components\n\nForbidding strategy: control what enters the tabu list\nFreeing strategy: control what exits the tabu list and when\nShort-term strategy: manage interplay between the forbidding strategy and freeing strategy to select trial solutions\n\n Memory\nheavily relies on the use of an explicit memory of the search process\n\nsystematic use of memory to guide search process\nmemory typically contains only specific attributes of previously seen solutions\nsimple tabu search strategies exploit only short term memory\nmore complex tabu search strategies exploit long term memory\n\n Introduction to Scheduling\n\n\n\n\n\n\n\n\n\nScheduling deals with the allocation of resources to tasks over given time periods and its goal is to optimize one or more objectives. The resources and tasks in an organization can take many different forms.\n Framework &amp; Notation\n\n\njobs j=1,2,...,nj = 1, 2, ..., nj=1,2,...,n (number of jobs are assumed to be finite)\n\n\nmachines i=1,2,...,mi = 1, 2, ..., mi=1,2,...,m (number of machines are assumed to be finite)\n\n\n(i,j)(i, j)(i,j)- processing step, or operation of job j on machine i\n\n\nscheduling problem - α | β | γ\n\nα - machine characteristics (environments)\nβ - processing/job characteristic\nγ - optimality criteria (objective to be minimised)\n\n\n\n Sample Machine Characteristics (α)\n\n\n111 Single machine\n\n\nPmPmPm Identical machines in parallel\n\nmmm machines in parallel\nJob jjj requires a single operation and may be processed on any of the m machines\n\n\n\nQmQmQm Machines in parallel with different speeds\n\n\nRmRmRm Unrelated machines in parallel machines have different speeds for different jobs\n\n\n Sample Job Characteristics (β)\n\nProcessing time pijp_{ij}pij​ - processing time of job jjj on machine iii (if a single machine then pjp_jpj​)\nDue date djd_jdj​ - committed shipping or completion (due) date of job jjj\nWeight wjw_jwj​ - importance of job jjj relative to the other jobs in the system\nRelease date rjr_jrj​ - earliest time at which job jjj can start its processing\nPrecedence precprecprec – Precedence relations might be given for the jobs. If kkk precedes lll, then starting time of lll should be not earlier than completion time of kkk.\nSequence dependent setup times sjks_{jk}sjk​ - setup time between jobs jjj and kkk\nBreakdowns brkdwnbrkdwnbrkdwn - machines are not continuously available\n\n Sample Optimality Criteria (γ)\n\n\nCijC_{ij}Cij​ completion time of the operation of job jjj on machine iii\n\n\nCjC_jCj​ time when job jjj exits the system\n\n\nCmaxC_{max}Cmax​ makespan is the time difference from the start (often, t=0) to\nfinish when the last job exits the system\n\n\nLj=Cj−djL_j = C_j - d_jLj​=Cj​−dj​ lateness of job jjj\n\n\nTj=max(Cj−dj,0)T_j = max(C_j - d_j , 0)Tj​=max(Cj​−dj​,0) tardiness of job jjj\n\n\nU_j = \n\\begin{cases}\n1 &amp; \\text{if  } C_j &gt; d_j\\\\\n0 &amp; otherwise\\\\\n\\end{cases}$$**unit penalty** of job $j$\n\n\n\n\n\n\n1∣prec∣Cmax−A1 | prec | C_{max} - A1∣prec∣Cmax​−A\nA single machine, general precedence constraints, minimising makespan (maximum completion time)\n\n\nP3∣dj,sjk∣∑Lj−3P3 | d_j, s_{jk} | \\sum L_j - 3P3∣dj​,sjk​∣∑Lj​−3\n3 identical machines, each job has a due date and sequence dependent setup times between jobs,minimising total lateness of jobs\n\n\nR∣∣∑CjR||\\sum C_jR∣∣∑Cj​\nvariable number of unrelated parallel machines, no constraints, minimising total completion time\n\n\n1∣dj∣∑wjTj1|d_j|\\sum w_jT_j1∣dj​∣∑wj​Tj​\nGiven nnn jobs to be processed by a single machine, each job jjj with a due date djd_jdj​ , processing time pjp_jpj​ , and a weight wjw_jwj​ , find the optimal sequencing of jobs producing the minimal weighted tardiness wjTjw_jT_jwj​Tj​\n\n\n Move Acceptance in Local search Metaheuristics, Parameter Setting issues\n Move Acceptance Methods of Local Search Metaheuristics\n\n Parameter setting mechanisms in Move Acceptance\n\n\nStatic - either there is no parameter to set or parameters are set toa fixed value (e.g. IoM=5 )\n\n\nDynamic - parameter values vary with respect to time/iteration count. Given the same candidate and current solutions at the same current elapsed time or iteration count, the acceptance threshold or acceptance probability would be the same irrespective of search history\n(e.g. IoM=round(1+(itercurrent/itermax)∗4)IoM = round(1 +(itercurrent / itermax) * 4)IoM=round(1+(itercurrent/itermax)∗4))\n\n\nAdaptive - Given the same candidate and current solutions at the same current elapsed time or iteration count, the acceptance threshold or acceptance probability is not guaranteed to be the same as one or more components depend on search history\n(e.g. if for 100 steps best solution found so far cannot be improved, then IoM++, and after any improvement, reset IoM=1)\n\n\n Non-stochastic &amp; Basic Move Acceptance Methods\n\nReuse the objective values of previously encountered solutions for the accept/reject decisions\nstatic\n\nall moves f′(s)f&#x27;(s)f′(s)\nimproving moves only f(s′)&lt;f(s)f(s&#x27;) &lt; f(s)f(s′)&lt;f(s)\nimproving and equal f(s′)≤f(s)f(s&#x27;) \\leq f(s)f(s′)≤f(s)\n\n\ndynamic: none\nadaptive\n\nLate Acceptance: compares the quality of the solution with that of the solution accepted/visited LLL iterations previously st−Ls_{t-L}st−L​, and accepts the move if and only if f(s′)≤f(st−L)f(s&#x27;) \\leq f(s_{t-L})f(s′)≤f(st−L​)\n\nInitialisation: assign all elements of the list to be equal to the initial cost (objective value)\nList implementation: List for the history of the objective values of the recent solutions is implemented as a circular queue\n\n\n\n\n\n Non-stochastic &amp; Threshold Move Acceptance Method\nDetermine a threshold which is in the vicinity of a chosen solution quality, e.g. the quality of the best solution found so far or current solution, and accept all solutions below that threshold pseudocode for minimisation\n Pseudocode for minimisation\n12345678910s0 = generateInitialSolution();s, s_best = s0;// initialise other relevant parameters if there is anyREPEAT\ts&#x27; = makeMove(s, memory); // choose a neighbour of s*\tthreshold = moveAcceptance-&gt;getThreshold(s, s&#x27;, memory);\tif(f(s&#x27;) &lt;= threshold) s = s&#x27;; // else reject new solution s&#x27;\ts_best = updateBest(s, s&#x27;); //keep track of s_bestUNTIL(termination conditions are satisfied):RETURN s_best\n\n\nstatic\n\nAccept a worsening solution if the worsening of the objective value is no worse than a fixed value\n\n\n\ndynamic\n\n\nGreat Deluge\n\n\n\nFlex Deluge\n\n\n\n\nAdaptive\n\nRecord to record travel (RRT)\nExtended Great Deluge\n\nbased on Great Deluge\nFeedback is received during the search and decay-rate is updated/reset accordingly whenever there is no improvement for a long time\n\n\nModified Great Deluge\n\n\n\n Stochastic Move Acceptance\n Psuedocode\n12345678910s0 = generateInitialSolution();s, s_best = s0;REPEAT\ts&#x27; = makeMove(s, memory); // choose a neighbour of s\tP = moveAcceptance-&gt;getAcceptanceProbability(s, s&#x27;, memory);\tr = getRandomValue(); // a uniform random value between [0, 1]\tif(f(s&#x27;).isBetterThan(f(s)) || r &lt; P) s = s&#x27;; // else reject new solution s&#x27;\ts_bset&lt;-updateBest(s, s&#x27;); // keep track of s_bestUNTIL(termination conditions are satisfied);RETURN s_best;\n\n\nstatic\n\nNaive Acceptance: P is fixed, e.g. if improving P = 1.0, else P = 0.5\n\n\n\ndynamic\n\n\nSimulated Annealing : P changes in time with respect to the difference in the quality of current and previous solutions\n\n\nadvantages:\n\neasy to implement\nachieves good performance given sufficient running time\n\n\n\ndrawbacks:\n\nrequires a good parameter setting for improved performance\nHas interesting theoretical properties (convergence),but these are of very limited practical relevance\n\n\n\nPseudocode\n123456789101112131415INPUT: 𝑇0, 𝑇𝑓𝑖𝑛𝑎𝑙𝑠0 ← 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝐼𝑛𝑖𝑡𝑖𝑎𝑙𝑆𝑜𝑙𝑢𝑡𝑖𝑜𝑛();𝑇 ← 𝑇0; // initialise temperature to 𝑇0S_𝑏𝑒𝑠𝑡 ← 𝑠0; 𝑠 ← 𝑠0; // set 𝑠 and S_𝑏𝑒𝑠𝑡 to initial solutionREPEAT    𝑠′ ← 𝑝𝑒𝑟𝑡𝑢𝑟𝑏𝑎𝑡𝑖𝑜𝑛(𝑠) ; // choose a neighbouring solution of 𝑠    Δ = 𝑓(𝑠′) − 𝑓(𝑠);    𝑟 ← 𝑟𝑎𝑛𝑑𝑜𝑚 ∈[0,1]; // get a uniform random number in the range [0,1)    if(Δ &lt; 0 || 𝑟 &lt; 𝑃(Δ,𝑇) ) &#123; // if solution is non-worsening or in Boltzmann probability        s ← 𝑠′;    &#125;    S_best ← 𝑢𝑝𝑑𝑎𝑡𝑒𝐵𝑒𝑠𝑡(); // keep track of best solution    𝑇 ← 𝑐𝑜𝑜𝑙𝑇𝑒𝑚𝑝𝑒𝑟𝑎𝑡𝑢𝑟𝑒(); // decrease the temperature according to cooling scheduleUNTIL (Termination conditions are satisfied);Return S_𝑏𝑒𝑠𝑡;\n\n\nAccepting moves\n\nΔ=F(snew)−F(sold)Δ = F(s_{new}) - F(s_{old})Δ=F(snew​)−F(sold​)\nImproving moves (i.e. Δ &lt; 0, assuming minimisation, below are same) are accepted\nWorsening moves are accepted using the Metropolis criterion at a given temperature TTT\n\nfor Δ &gt; 0 accept with a Boltzman probability of P(Δ,T)=e−ΔTP(Δ, T) = e^{\\frac{-Δ}{T}}P(Δ,T)=eT−Δ​\nU(0,1)U(0, 1)U(0,1) generates a random number in [0, 1)\naccept if U(0,1)&lt;P(Δ,T)U(0, 1) &lt; P(Δ, T)U(0,1)&lt;P(Δ,T)\n\n\n\n\n\nCooling / Annealing\n\n\nAs the temperature T decreases, the probability of accepting worsening moves decreases\n\n\nStarting Temperature (T0)\n\nhot enough: to allow almost all neighbors\nnot so hot: random search for sometime\nEstimate a suitable starting temperature:\n\nReduce quickly to 60% of worse moves are accepted\nUse this as the starting temperature\n\n\n\n\n\nFinal Temperature\n\nUsually 0, however in practice, not necessary\nT is low: accepting a worse move is almost the same as T=0\nThe stopping criteria: either be a suitably low T, or “frozen” at the current T (i.e. no worse moves are accepted)\n\n\n\nTemperature Decrement\n\nLinear: $$T = T - x$$\nGeometric: $$T = T * α$$\n\nExperience: α is typically in the interval [0.9, 0.99]\n\n\nLundy Mees: $$T = \\frac{T}{1 + βT}$$\n\nOne iteration at each T, but decrease T very slowly.\nExperience: β is typically a very small value, that is close to 0 (e.g., 0.0001)\n\n\n\n\n\nIterations at each temperature\n\nOne iteration at each TTT\nA constant number of iterations at each TTT\nCompromise\n\nEither: a large number of iterations at a few TTTs, or\nA small number of iterations at many TTTs, or\nA balance between the two\n\n\nDynamically change the no. of iterations\n\nAt higher TTTs: less no. of iterations\nAt lower TTTs: large no. of iterations, local optimum fully exploited\n\n\n\n\n\nReheating\n\nIf stuck at a local optimum for a while, increase the current temperature with a certain rate\n\n\n\n\n\n\n\n\n\nadaptive\n\nSimulated Annealing with reheating:\nP is modified time to time causing partial restart –increasing the probability of acceptance of non-improving solutions\nSimulated Annealing using the best found solution so far(or in a phase) for acceptance with a cooling schedule\n\n\n\n Parameter Setting Issues and Tuning Methods\n Parameter types\n\nCategorical/symbolic/structural parameters - e.g. choice of initialisation method, choice of mutation, …\nOrdinal parameters - e.g. neighborhoods (e.g., small, medium,large),…\nNumerical/behavioural parameters\n\ninteger, real-valued, …\ne.g. population sizes, evaporation rates, …\nvalues may depend on the setting of categorical or ordinal parameters\n\n\n\n\n Parameter Setting Methods\n\n\nParameter tuning (Off-line setting):\nFinding the best initial settings for a set of parameters before the search process starts (off-line)\nE.g., fixing the mutation strength in ILS, mutation probability in genetic algorithms, etc.\nThe initial parameter setting influences the performance of a metaheuristic.\n\nSequential tuning\nDesign of Experiments\nMeta-optimisation\n\n\n\nParameter control (Online setting):\nManaging the settings of parameters during the search process (online) (dynamic, adaptive, self-adaptive)\nE.g., changing the mutation strength in ILS, changing the mutation probability in genetic algorithms during the search process\nControlling parameter setting could yield a system which is not sensitive to its initial setting\n\nDynamic\nAdaptive\n\nSelf-adaptive\n\n\n\n\n\n Parameter Tuning methods\n\n\nTraditional approaches\n\nUse of an arbitrary setting\nTrial&amp;error with settings based on intuition\nUse of theoretical studies\nA mixture of above BOOKMARK\n\n\n\nSequential tuning: fix parameter values successively (e.g., fix A=20 and tune B that is try {0, 0.3, 0.5, 0.8, 1.0 }}, then fixing the best setting for B from the previous trials and tune A that is try {20, 40,50, 60, 80})\n\n\nDesign of experiments (DoE)\n\n\nA systematic method (controlled experiments) to determine the relationship between controllable and uncontrollable factors (inputs to the process, variables) affecting a process(e.g. running of an algorithm), their levels (settings) and the response (output) of that process (e.g. quality of solutions obtained performance of an algorithm)\n\n\nImportant outcomes are measured and analysed to determine the factors and their settings that will provide the best overall outcome\n\n\nFractional Factorial Designs - designed to draw out valuableconclusions from fewer runs (&lt;-&gt; a number of factors is k in an n level factorial design can results in n^k runs for even a single replicate)\n\nKey observation: Responses are often affected by a small number of main effects and lower order interactions,while higher order interactions are relatively unimportant\n\n\n\nSampling - whenever factorial design is not possible, sampling is performed\n\n\nrandom\n\nGenerate each sample point independently (M)\n\n\n\nLatin Hyper-cube\n\nDecide the number of sample points(M) for N variables and for each sample point remember in which row and column the sample point was taken\n\n\n\n\n\n\nOrthogonal\n\nThe sample space is divided into equally probable subspaces. Sample points simultaneously,ensuring they form an ensemble of Latin Hypercube sample\n\n\n\n\n\n\nTaguchi Orthogonal Arrays Method for Parameter Tuning\n\n\n\n\n\n\nMeta-optimisation: use a metaheuristic to obtain “optimal”parameter settings\n\n\nTaguchi Orthogonal Arrays Method for Parameter Tuning\n(Parameter Tuning methods - Design of experiments (DoE) - Sampling - Orthogonal)\n\nAim : make a “product” or “process” less variable (more robust) in the face of variation over which we have little or no control\n\n\n\n      \n Parameter Control\n\n\nStatic:\n\n\n\n\n\n\n\n\n\nfixed  parameter value\nExample:  Accept a worsening solution if the worsening of the objective value is no worse than a fixed value\n\n\nDynamic:\n\n\n\n\n\n\n\n\n\nparameter value vary with respect to time/iteration count\nExample:  Great Deluge,  Flex Deluge, Simulated Annealing\n\n\nAdaptive:\n\n\n\n\n\n\n\n\n\ndepend on search history\nExample:  Record to record travel (RRT), Extended Great Deluge, Modified Great Deluge, Simulated Annealing with reheating\n\n\n\n\n Evolutionary Algorithms (EAs) (I/II): Genetic Algorithms (GAs),Memetic Algorithms (MAs), Benchmark functions\n Evolutionary Algorithms (EAs)\n\n\n\n\n\n\n\n\n\nEAs simulate natural evolution (Darwinian Evolution) of individual structures at the genetic level using the idea of survival of the fittest via processes of selection, mutation, and reproduction (recombination)\n\nEA includes GA, GP, EP\n\n\n\nAn individual/chromosome represents a candidate solution for the problem at hand\n\n\nA collection of individuals currently “alive”, called population (set of individuals/chromosomes) is evolved from one generation (iteration) to another depending on the fitness of individuals in a given environment , indicating how fit an individual is, (how close it is to the optimal solution)\n\n\nHope: last generation will contain the final solution\n\n\n History\n\n\nGenetic Algorithms\n\nMemetic Algorithms\n\n\n\n\n\n\n\n\n\n\nevolves bit strings\n\n\nEvolutionary Programming\n\n\n\n\n\n\n\n\n\nevolves parameters of a program with a fixed structure\n\n\nEvolution Strategies\n\n\n\n\n\n\n\n\n\nvectors of real numbers\n\n\nGenetic Programming\n\n\n\n\n\n\n\n\n\nevolves computer programs in tree form\n\n\nGene Expression Programming\n\n\n\n\n\n\n\n\n\ncomputer programs of different sizes are encoded in linear chromosomes of fixed length\n\n\nGrammatical Evolution\n\n\n\n\n\n\n\n\n\nevolves solutions wrt a specified grammar\n\n\n\n\n Key Features of EAs\n\npopulation based search approaches\n\nBe independent of initial starting point(s) - Start search from many points in the search space\nConduct search in parallel over the search space - implicit parallelism\n\n\nAvoid converging to local optima\n\n Weakness\n\nLimited theoretical and mathematical analyses - this is a growing field of study\nConsidered slow for online applications and even for some large offline problems\n\n Genetic Algorithms(GAs)\n Pseudocode\n1234567891011begingenerate initial population; // initialisecalculate fitness values; //evaluate populationdo&#123;\tperform reproduction; //select parents\trecombine pairs with p_c; // apply corssover\tapply mutation with p_m; // mutate offspring\tcalculate fitness values; // eval. population\treplace current population;&#125; while termination criteria not satisfied;end\n Basic components of GAs\n\nA genetic representation (encoding) for candidate solutions(individuals) to the problem at hand\nAn initialisation scheme to generate the first population (set) of candidate solutions (individuals)\nA fitness (evaluation) function that plays the role of the environment, rating the solutions in terms of their fitness\nA scheme for selecting mates (parents) for recombination\nCrossover (recombination) exchanges genetic material between mates producing offspring children\nMutation perturbs an individual creating a new one\nReplacement strategy to select the surviving individuals for the next generation\nTermination Criteria\nValues for various parameters that GA uses (population size, probabilities of applying genetic operators, etc)\n\n Representation\n\nHaploid structure: each individual contains one chromosome\nchromosome contain a fixed number of genes: chromosome length\neach individual is evaluated and has an associated fitness value\nTraditionally binary encoding is used for each gene: Allele value ∈\\in∈ {0, 1}\na population contains a fixed number of individuals: population size\neach iteration is referred as generation\n\n Initialisation\n\nRandom initialisation\n\nPopulation size number of individuals are created randomly\nEach gene at a locus of an individual is assigned an allele value 0 or 1 randomly\n\n\n\n Fitness calculation\n\nFitness value indicates “how fit the individual is to survive and reproduce under the current conditions”\ni.e. how much the current solution meets the requirements of the objective function\nis obtained by applying the fitness function to the individual’s chromosome (candidate solution)\ni.e. genotype (e.g. 101110) to phenotype (e.g.1) mapping\n\n Reproduction\n\n\nselecting individuals : apply selection pressure considering the fitness of individuals in the population (e.g. roulette wheel selection, tournament selection, rank selection, truncation selection, Boltzmann selection, …)\n\nSelection pressure means the individuals with better fitness have higher chance for being selected\n\n\n\nusually 2 parents (individuals/candidate solutions) are selected using the same method, which will go under the crossover operation\n Roulette Wheel Selection\n\nFitness level is used to associate a probability of selection(probiprob_iprobi​) with each individual chromosome\nExpected number of representatives of each individual in the pool is proportional to its fitness\n“While candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be”\n\n Tournament Selection\n\nThis method involves running a number of &quot;tournaments&quot;among randomly chosen individuals (of tour size)\nselecting the one with best fitness at the end\n\n\n\n Crossover / Recombination\n\n\napplied with a crossover probability pcp_cpc​ which in general is chosen to 1.0\n One Point Crossover(1PTX)\n\nGenerate a random number in [0, 1), if it is smaller than a crossover probability pcp_cpc​ Then\n\nSelect a random crossover site in [1, chromosome length]\nSplit individuals at the selected site\nExchange segments between pairs to form two new individuals\n\n\nElse\n\nCopy the individuals as new individuals\n\n\n\n\n\n\n 2 Point Crossover (2PTX)\n K-point Crossover\n Uniform Crossover (UX)\n\nconsiders each bit in the parent strings for exchange with a probability of 0.5\n\n\n\n\n\n\n Mutation\n\nprovides diversity and allows GA to explore different regions of the search space (escaping)\nLoop through all the alleles of all the individuals one by one, and if that allele is selected for mutation with a given probability you can either change it by a small amount or replace it with a new value (for binary representation, flipping a gene value)\n\nMutation rate is typically chosen to be very small (e.g. 0.001)\nChoosing pmp_mpm​ as (1 / chromosome length) implies on average a single gene will be mutated for an individual\n\n\n\n Replacement\n\n\nGeneration gap (ααα) controls the fraction of the population to be replaced in each generation, where $α \\in [1/N,1.0] $Number of offspring produced at each generation is g=α∗Ng=α*Ng=α∗N\n\n\n(Trans-) Generational GA\nNNN individuals produce αNαNαN offspring, so (N+αN)→N(N + αN) → N(N+αN)→N\n\nαNαNαN replaces worst αNαNαN of NNN\n\nlargest generation gap where α=1.0α=1.0α=1.0 yields g=Ng=Ng=N.\nGA relies on improvement of average objective values from one population to another\n\nIt is always a good idea not to loose the best solution found so far.\n\n\n\n\nsort (N+αN)(N + αN)(N+αN) and choose the NNN best (elitism)\n\n\n\nSteady-State GA (g=2, that is α=2/N)\nTwo offspring replace two individuals from the old generation.\n\nMethod#1: two offspring replace two parents\nMethod#2: two offspring replace worst two of the population\nMethod#3: best two of (parents and offspring) replace two parents (elitism)\nMethod#4: best two of (parents and offspring) replace worst two of the population (strong elitism)\n\n\n\n Termination Criteria\n\nA predefined maximum number of generations is exceeded\nA goal is reached (e.g. Expected fitness is achieved, Population converges)\nBest fitness does not change for a while\nA condition is satisfied depending on a combination of above\n\n Convergence - definition\n\nDefined as the progression towards uniformity (individuals become alike)\nGene convergence: a location on a chromosome is converged when 95% of the individuals have the same gene value for that location\nPopulation (Genotypic) convergence: a population is converged when all the genes have converged (all individuals are alike they might have different fitness)\nPhenotypic Convergence: average fitness of the population approaches to the best individual in the population (all individuals have the same fitness)\n\n Memetic Algorithms (MAs)\n\n\n\n\n\n\n\n\n\nMeme: contagious piece of information\n\nMemes are similar to local refinement/local search\nGene vs. Meme\n\nMemes can change, evolve using rules and time scales other than the traditional genetic ones\n\n\n\n\nMAs aim to improve GAs by embedding local search\n\nMAs are much faster and accurate than GAs on some problems\n\n\nMAs make use of exploration capabilities of GAs and exploitation capabilities of local search (i.e. an explicit mechanism to balance exploration and exploitation)\n\n\n Benchmark functions\n\n\n\n\n\n\n\n\n\nBenchmark functions serves as a testbed for performance comparison of (meta/hyper)heuristic optimisation algorithms\n Why use benchmark functions\n\nTheir global minimum are known\nThey can be easily computed\nEach function is recognised to have certain characteristics potentially representing a different real world problem\n\n Classification\n\n\nContinuity (Differentiability)\n\n\nf(x)=∣x∣f(x)=∣x∣f(x)=∣x∣, continuous but not differentiable\n\n\ndiscontinuous vs. continuous\n\n\n\n\nDimensionality (Scalability)\n\n\nModality\n[The number of ambiguous peaks in the function landscape]\n\nUnimodal\nMultimodal with few local minima\nMultimodal with exponential number of local minima\n\n\n\nSeparability\n[each variable of a function is independent of the other variables; A function of variables is separable if it can be rewritten as a sum of functions of just one variable (note: see examples for comprehensive understanding)]\n\nseparable functions allows delta evaluation\n\n Examples\n\nf(x)=∑i=1nxi2f(x) = \\sum_{i=1}^{n}x_i^2f(x)=∑i=1n​xi2​\n\ncontinuous, differentiable, separable, scalable\n\n\nf(x)=∑i=1n(⌊∣xi∣⌋)f(x) = \\sum_{i=1}^{n}(⌊∣x_i∣⌋)f(x)=∑i=1n​(⌊∣xi​∣⌋)\n\ndiscontinuous, non-differentiable, separable, scalable\n\n\nf(x,y)=−20exp(−0.20.5(x2+y2)−exp(0.5(cos(2πx)+cos(2πy)))+e+20f(x, y) = -20exp(-0.2\\sqrt{0.5(x^2+y^2)}-exp(0.5(cos(2\\pi x) +cos(2\\pi y)))+e+20f(x,y)=−20exp(−0.20.5(x2+y2)​−exp(0.5(cos(2πx)+cos(2πy)))+e+20\n\ncontinuous, differentiable, non-separable, non-scalable\n\n\n\n\n\n Evolutionary Algorithms (EAs) (II/II): MA &amp; GA (cont.), Multimeme Memetic Algorithms (MMAs)\n Case study of GAs/MAs\n Binary Coding vs. Gray Coding\n\nGray encoding ensures a Hamming distance of 1 for the adjacent numbers\nShown to be useful in GAs empowering the algorithm to mutate a solution in the right direction\n\n Binary Representation for Encoding Permutation / Permutation basedGenetic Operators\n\n Partially Mapped Crossover (PMX)\n\n Order Crossover (OX)\n\n\n Cycle Crossover (CX)\n\n Multimeme Memetic Algorithms (MMAs)\n Self Adaptation\n\n\n\n\n\n\n\n\n\nDeciding which operators and settings to use on the fly whenever needed receiving feedback during the evolutionary search process\n\n\ni.e. co-evolve genetic and memetic material\n\nMultimeme Algorithm can indeed learn how to choose an operator and relevant settings through the evolutionary process (co evolution)\nThere is a trade off as learning requires time and a memetic algorithm with a single setting could perform better\n\n\n\nMemes represent instructions for self improvement\n\nSpecify set of rules, programs, heuristics, strategies,behaviors, etc.\n\n\n\nMeme of each operator can be combined under a memeplex\n\n\nGrammar for memeplex - Compound of Memes\n\n\n\nExample\n\n\n\n Mutating Memes during evolution\n\n\nInnovation rate IR∈[0,1]IR\\in [0, 1]IR∈[0,1] : the probability of mutating the memes\n\nIR=0IR = 0IR=0 - no innovation\n\nif a meme option is not introduced in the initial generation, it will not be reintroduced again\n\n\nIR=1IR=1IR=1\n\nAll different strategies implied by the available MMM memes might be equally used\n\n\n\n\n\nConcentration of a meme Ci(t)C_i(t)Ci​(t)\n\ntotal number of individuals that carry the meme iii at a given generation ttt\nCrude measure of a meme success; gives no information about continual usage of a meme\n\n\n\nEvolutionary activity of a meme ai(t)a_i(t)ai​(t)\n\nthe accumulation of meme concentration until a given generation\na_i(t) = \\left\\{\\begin{array}{**lr**}  \\int_0^t c_i(t)dt, &amp; \\text{if } c_i(t) &gt; 0 \\\\ 0, &amp; \\text{otherwise}\\\\  \\end{array}   \\right.\nSlope in a plot represents the rate of increase of a meme concentration\n\n\n\n Hyper-heuristics (I/II): Motivation / Characteristics / Classification /Misconceptions of Hyper-heuristics, Selection Hyper-heuristic controlling Perturbative Heuristics\n Hyper-heuristics\n\n\n\n\n\n\n\n\n\nA hyper-heuristic is a search method or learning mechanism for selecting or generating heuristics to solve computationally difficult problems\nA class of methodologies for cross domain (i.e. no domain knowledge) search\n\n Motivation\nraise the level of generality of search methods / cross domain research\n Characteristics of Hyper heuristics (initial framework)\n\nOperate on a search space of heuristics (neighborhood operators) rather than directly on a search space of solutions\nAim is to take advantage of strengths and avoid weaknesses of each heuristic (operator)\nNo problem specific knowledge is required during the search over the heuristics (operator) space\nEasy to implement, practical to deploy (easy, cheap, fast)\nExisting (or computer generated) heuristics (operators) can be used within hyper heuristics\n\n Classification of Hyper-heuristics\n\n\nGeneration hyper-heuristics\nautomatically construct new heuristics from given components\nSelection hyper-heuristics\nchoose and control a predefined set of heuristics\nOffline learning hyper-heuristics\n\nusually trained on a set of selected instances and then generalise to unseen instances\n\n\nOnline learning hyper-heuristics\n\nreceive feedback during the search process while solving a given instance of a problem\n\n\n\n Misconceptions about Hyper-heuristics\n\nHyper-heuristics do not require parameter tuning (✕)\n\nrequire parameter tuning (unless have a parameter control mechanism)\n\n\nHyper-heuristics are all tested under a fair setting (HyFlex) (✕)\n\nTime allocated (and instances used) for tuning - no one seems to take into account the time spent for parameter tuning, much time on tuning\n\n\nApplying a hyper heuristic to a new domain is easy (✕)\n\nthe hyper-heuristic part itself is easy, but decide who is going to implement these new operators in the new domain / choose of domain-specific operators/heuristics is hard\n\n\nDomain specific information should not be passed to the hyper-heuristics (objective value is not a domain specific information, all others are) (✕)\n\nobjective value is domain specific information, and, the hyper-heuristic, as an interface, should allow such information to be passed, and, with domain-specific information hyper-heuristic can be more capable\n\n\n\n\n Selection Hyper-heuristic controlling Perturbative Heuristics\n A Selection Hyper heuristic Framework - Single Point Search\n1234567generate initial candidate solution pwhile(termination critera not satisfied)&#123;\tselect a heuristic (or subset of heuristic) h from &#123;H1,..., Hn&#125;\tgenerate a new solution (or solutions) s by applying h to p\tif(s is accepted) tehn p = s; // decide whether to accept s or not&#125;return p;\n\n Hyper-heuristics Flexible Interface (HyFlex) - Multi-Point / Population Search\n\n\nHeuristic types: mutational (MU), ruin-recreate (RC), local search(HC), crossover (XO)\nParameters: intensity of mutation, depth of search\n\n Heuristic (Operator) selection methods\n\n\nSimple Random / Random Permutation - with no learning\n\n\nGreedy (GR) - with learning\n\nApply each low level heuristic to the candidate solution and choose the one that generates the best objective value\n\n\n\nReinforcement Learning (RL) - with learning\n\nA machine learning technique\nConcerned with how an agent ought to take actions in an environment to maximize some notion of long term reward\nreward and punishment\nMaintains a score for each heuristic\nIf an improving move then increase (e.g. +1) other wise decrease (e.g. - 1) the score of the heuristic\n\n\n\nChoice Function (CF) - with learning\n\nmaintains a record of the performance of each heuristic with 3 criteria\n\nits individual performance (f1f_1f1​)\nhow well it has performed with other heuristics (f2f_2f2​)\nthe elapsed time since the heuristic has been called (f3f_3f3​)\n\n\nFn(hj)=αnf1(hj)+βnf2(hk,hj)+γnf3(hj)F_n(h_j) = \\alpha _n f_1(h_j) + \\beta _n f_2(h_k, h_j) + \\gamma _n f_3(h_j)Fn​(hj​)=αn​f1​(hj​)+βn​f2​(hk​,hj​)+γn​f3​(hj​)\n\n\n\nAn Iterated Multi-stage Selection Hyper-heuristic\n\n\nCrossover operators are ignored\n\n\n\n\n\nStage 1 Hyper-heuristic (S1HH)\n\n\nselect a low level heuristic iii with probability scorei/∑∀kscorekscore_i / \\sum _{\\forall k} score_kscorei​/∑∀k​scorek​\n\n\napply the chosen heuristic\n\n\nAccept/reject based on an adaptive threshold acceptance method\n\n\n\nStage 1 terminates if a duration of is exceeded without any improvement\n\n\n\n\nStage 2 Hyper-heuristic (S2HH)\n\nGiven NNN LLHs, pair up all (which increase the number of LLHs to N+N2N + N^2N+N2), then reduce the number of LLHs (N+N2→nN+N^2\\rightarrow nN+N2→n)  and assign probabilities\nparameter tuning needed (6 parameters for this specific framework)\n\n\n\n\n\n Hyper-heuristics (II/II): Selection Hyper-heuristic controlling Constructive Heuristics, Generation Hyper-heuristics (Genetic Programming (GP))\n Selection Hyper-heuristic controlling Constructive Heuristics\n A Graph-based Hyper-heuristic (GHH)\n\n\nGraph colouring problem - no two adjacent vertices share the same colour (vertex colouring)\n\n\nminimum colouring problem (an NP-hard problem)\n\n\ndegree of a vertex: number of edges connected to that vertex\n\n\nsaturation degree of a vertex: number of differently coloured vertices already connected to it\n\n(note : 1. coloured; 2.differently)\n\n\n\n\n\n Graph colouring heuristics\n\nlargest degree\n\nsort the vertices from largest degree to smallest\ncolour the first vertex in the list with an colour\n\n(starting with the first) that is different than its neighbours\n\n\ndelete the vertex from the list go to the previous step unless no vertices left\n\n\nsaturation degree\n\nuse saturation degree instead of degree in largest degree\n\n\n\n Case study: examination timetabling problem\n\n\nA general framework employing a set of low level constructive graph colouring heuristics\n\nLow level heuristics: sequential methods that order events by the difficulties of assigning them\n\n\n\nmodel the problem as a graph colouring problem\n\nNodes: exams\nEdges: adjacent exams have common students\nColours: time periods\nObjective: assign colours (time periods) to nodes (exams),adjacent nodes with different colour, minimising time periods used\n\n\n\nSelect low level heuristics\n\n\n\n\n\n\n\na specific pseudo-code (i.e. only one specific way) of Tabu Search graph based hyper-heuristic\n\n(note:‘events’=‘exams’)\n\n12345678910111213141516171819initial heuristic list hl = &#123;h1, h2, h3,..., hk&#125;// Begin of Tabu Searchfor i = 0 to i = (5 * the number of events) // number of iterations\th = change two heuristic in hl // a move in Tabu search\tif h does not match a heuristic list in &#x27;failed list&#x27;\t\tif h is not in the tabu list // h is not recently visited\t\tfor j = 0 to j = k // h is used to construct a complete solution\t\t\tschedule the first 2 events in the event list ordered using hj\t\t\tif no feasible solution can be obtained\t\t\t\tstore h into &#x27;failed list&#x27; // update &quot;failed list&quot;\t\t\telse if cost of solution c &lt; the best cost c_g obtained\t\t\t\tsave the best solution, c_g = c //keep the best solution\t\t\t\tadd h into the tabu list\t\t\t\tremove the first item from the tabu list if its length &gt; 9\t\thl = h\t//end if\tDeepest descent on the complete solution obtained//end of Tabu searchoutput the best solution with cost of c_g\n\nNeighborhood operator: randomly change two heuristics in the heuristic list\nObjective function: quality of solutions built by the corresponding heuristic list\nTabu list: visits to the same heuristic lists forbidden\n\n\n\nother high-level search strategies: steepest descent, Variable neighborhood search -&gt; best performing, iterated steepest descent, …\n\n\n Generation Hyper-heuristics\n Genetic programming(GP)\n\n\nGP provides a method for automatically creating a working computer program from a high level problem statement of the problem\n\ni.e. program synthesis / program induction\n\n\n\nGP iteratively transforms a population of computer programs into a new generation of programs via evolutionary process\n\n\nrepresent a computer program as a parse tree\n\n\n\nGP is an evolutionary algorithm containing the same algorithmic components , including\n\nRandom generation of the initial population of possible solutions (programs)\n\nRandomly generate a program that takes two (or more) arguments and uses basic arithmetic to return an answer\n\nFunction set = {+,−,∗/}\\{+, -, * /\\}{+,−,∗/}\nTerminal set = {integers,X,Y}\\{integers, X, Y\\}{integers,X,Y}\n\n\nRandomly select either a function or a terminal to represent our program\nIf a function was selected, recursively generate random programs to act as arguments\n\n\nGenetic crossover of two promising solutions to create new possible solutions (programs)\n\nPick a random node in each program\nSwap the two nodes\n\n\nMutation of promising solutions to create new possible solutions (programs)\n\nFirst pick a random node\nDelete the node and its children, and replace with a randomly generated program\n\n\nFitness measure that is used to evaluate a given evolved program\n\nGP is often operated in a train and test fashion training on selected sample instances could take long time while application to unseen instances is generally fast\nEach tree generated by GP can be evaluated using an indicator showing how good it is in building high quality solutions to the sample problem instances, such as, mean quality of solutions over the sample instances\n\n\nTermination criteria\n\n\n\nCase study: Genetic Programming for Packing\n\nError: GP at a specific time (note: ‘%’ should be ‘/’)\n\n      \n\nNow\n\nNow putting the item of size 70 into the bin\ncalculate the fitness measure based on the program,\nfind the max fitness value\nNext put item of size 70 into bin 4\nNext start search for item of size 85\n\n\nafter putting all items in bins (i.e. the program finishes on the test instance)\n\nrecord number of bins used and compare with the best\nthen do crossover / mutation …for new programs\n\n\n\n\n\n\n Advanced topics\n Case study: Policy Matrix Evolution for Generation of Heuristics on 1D Online Bin Packing Problem\n Index Policy\n\n\n\n\n\n\n\n\n\nEach choice option is given a score, or “index value” independently of the other options\n\nindex policies for 1D online bin packing\n\nscore of bin is f(r,s)f(r, s)f(r,s) where rrr is the remaining capacity of bin and sss is item size\nGiven a new item of size then place into bin with largest value of f(r,s)f(r, s)f(r,s)\n\n\n\n Open / Close\n\nA new empty bin is always available ( open )\nA bin is closed if it can take no more items\n\ne.g. if residual space is smaller than size of any item\n\n\n\n Potential General Method for 1D Online Bin Packing\n\n\n(i.e. method to assign a new item upon its arrival to one of the open bins)\n\n\non arrival of new item, inspect the current set of open bins, and simultaneously use the entire set of residual spaces in the open bins to pick where to place the new item\n\n\ndifficult expensive (in general)\n\n\n Generating Heuristic\n\nWithin search methods, often have score functions, “index functions” to help make some choice\n\ndifficult to invent successful ones; want to automate this\n\n\nGP approach: evolve arithmetic score functions\n\nUse Genetic Programming to learn f(r,s)f(r,s)f(r,s)\nf(r,s)f(r,s)f(r,s) is represented as arithmetic function tree\nAutomatically creates functions that at least match FF, BF\n\n\nChallenge:\n\nis hard to understand\npotentially biased because of the choice of representation\nsome perfectly good functions might have “bloated” representations\n\n\n\n Matrix View of Policies/Heuristics\n\n\nSince all item sizes (sss) and residual capacities (rrr) are integer,then f(r,s)f(r,s)f(r,s) is simply a large (C∗CC*CC∗C) matrix M(r,s)M(r,s)M(r,s)of parameter values\n\n\n\nUniform (random) Instances\n\nproblem generator on Uniform Bin Packing Problems: UBP(C,smin,smax,N)UBP(C,s_{min}, s_{max}, N )UBP(C,smin​,smax​,N)\nCCC - bin capacity\nNNN number of items with integer sizes taken randomly from range [smin,smax][s_{min}, s_{max}][smin​,smax​]\n\n\n\nCreating Heuristics via Many Parameters (CHAMP)\n\n\nBasic idea:\n\nTake values in matrix M(r,s)M(r,s)M(r,s) to be integers\nDo (meta-)heuristic search to find good choices for M(r,s)M(r,s)M(r,s): Evaluation is by simulation\n\n\n\nOriginal Expectation\n\nthe matrix will tweak the functions from GP and might slightly improve performance\n\n\n\nPotential expected disadvantages\n\nmatrices can be much more verbose than functions\nthey fail to take into account of the good structure captured by functions\n\n\n\n\n\n\nConclusion\n\nPolicies exist that out-perform standard heuristics\nFinding the policies is easier than expected\nThere are many different policies with similar performance\nThe policies are “weirder” than expected\n\nThe good policies could have “random” structures\nNot necessarily easy to capture with an algebraic function of GP\n\n\nThe results can be “analysed” (inspected) to produce simple policies that out-perform standard ones\n\nand that then scale to larger problems\n\n\n\n\n\n A Data Science Improved Hyper-heuristic\n\n\n\n\n\n\n\n\n\nMany real-world data are multidimensional\n\nVery high-dimensional (big) with a large amount of redundancy\n\n Proposed Approach – Ideas\n\nThe balance between exploration and exploitation is crucial\nMix move acceptance methods\nUse machine learning to partition the low level heuristics associated with each method\n\n Summary\n\nHyper-heuristic research originated from a job shop scheduling application and has been rapidly growing since then.\nGeneration hyper-heuristics are commonly used in the area\n\nTrain and test fashion\n\nDoes the selected subset of training instances is sufficiently representative of the test set?\nTraining is time-consuming (delta/incremental evaluation, surrogate functions)\n\n\nThe generated/evolved heuristics might not be easy to interpret, yet they can outperform human designed heuristics\n\n\nThere is empirical evidence that machine learning/analytics/ data science help to improve the hyper-heuristic search process\n\nProblem features vs solution/state features\nOffline versus online learning – Life long learning\n\n\nThere is still a lack of benchmarks\nAutomated design of search methodologies is extremely challenging\n\nAddressed in almost complete absence of a mathematical and theoretical understanding\n\n\n\n","slug":"aim","date":"2024-06-09T15:37:56.000Z","categories_index":"Notes","tags_index":"AI,Heuristics,Optimization","author_index":"Huaji1hao"},{"id":"e36df18b7817d5ce63296b0aa4c51d03","title":"Algorithms, Data structure & Efficiency","content":" Algorithms, Data structure &amp; Efficiency\n\nAlgorithm: step-by-step procedure for solving a problem in a finite amount of time.\n\nNo “MAGIC steps” allowed! (No “Zeno machines”)\n\n\nData structures: lists, binary trees, heaps, Hash maps, graphs, etc.\nEfficiency: We will start with methods to analyse, classify and describe the efficiency of algorithms\n\nNeeded in order to be able to select “best algorithms”\n\n\n\n Big-Oh family\n Big-Oh Notation\n\n\n\n\n\n\n\n\n\nDefinition: Given positive functions f(n)f(n)f(n) and g(n)g(n)g(n), then we say that\nf(n)f(n)f(n) is O(g(n))O( g(n) )O(g(n))\nif and only if there exist positive constants ccc and n0n_0n0​ such that\nf(n)≤cg(n)f(n)\\leq c g(n)f(n)≤cg(n) for all n≥n0n \\geq n_0n≥n0​\n\n\ni.e. “exists-exists-forall” structure:\n\n∃c&gt;0,∃n0,such  that  ∀n≥n0,f(n)≤cg(n)\\exist c&gt;0,\\exist n_0,such \\;that\\;\\forall n \\geq n0, f(n) \\leq c g(n)∃c&gt;0,∃n0​,suchthat∀n≥n0,f(n)≤cg(n)\n\n\n\n‘OOO’ expresses “grows at most as fast as”\n\n\n Big-Oh as a set\n\nBig Oh as a binary relation is reflexive and transitive but not symmetric\n\nIt behaves like “⊆⊆⊆”, “∈∈∈” or “≤≤≤”, not like “===”\nOne might say n∈O(n)n ∈O(n)n∈O(n), and 2n+3∈O(n)2n+3 ∈O(n)2n+3∈O(n), etc\n\n\n\nSo may help to think of “O(n)O(n)O(n)” as a set of functions, with each function f in the set, f∈O(n)f ∈O(n)f∈O(n), satisfying “fff is O(n)O(n)O(n)”.\n\nOr can say: {f}⊆O(n)\\{f\\} ⊆O(n){f}⊆O(n)\nSo then O(1)⊆O(n)O(1) ⊆O(n)O(1)⊆O(n)\n\n\n Rules for Finding big-Oh\n “Multiplication Rule” for big-Oh\n\nSuppose\n\nf1(n)f_1(n)f1​(n) is O(g1(n))O( g_1(n ) )O(g1​(n))\nf2(n)f_2(n)f2​(n) is O(g2(n))O( g_2(n) )O(g2​(n))\n\n\nThen, from the definition, there exist positive constants c1,c2,n1,n2c_1,c_2,n_1,n_2c1​,c2​,n1​,n2​ such that\n\nf1(n)≤c1g1(n)f_1(n) \\leq c_1g_1(n)f1​(n)≤c1​g1​(n) for all n≥n1n ≥ n_1n≥n1​\nf2(n)≤c2g2(n)f_2(n) \\leq c_2g_2(n)f2​(n)≤c2​g2​(n) for all n≥n2n ≥ n_2n≥n2​\n\n\nLet n0=max(n1,n2)n_0= max(n1, n2)n0​=max(n1,n2), then multiplying gives\nf1(n)f2(n)≤c1c2g1(n)g2(n)f_1(n) f_2(n) \\leq c_1 c_2g_1(n) g_2(n)f1​(n)f2​(n)≤c1​c2​g1​(n)g2​(n) for all n≥n0n ≥ n_0n≥n0​\nSo f1(n)f2(n)f_1(n) f_2(n)f1​(n)f2​(n) is O(g1(n)g2(n))O( g_1(n) g_2(n) )O(g1​(n)g2​(n))\n\n Big-Oh Rules: Drop smaller terms\n\nIf f(n)=(1+h(n))f(n)= ( 1 + h(n) )f(n)=(1+h(n)) with h(n)→0h(n) →0h(n)→0 as n→∞n →∞n→∞\nThen f(n)f(n)f(n) is O(1)O(1)O(1)\n(The utility will be to combine with the multiplication rule).\n\n Summary for finding big-Oh\nIf f(n)f(n)f(n) is a polynomial of degree ddd, (with positive largest term) then f(n)f(n)f(n) is O(nd)O(n^d)O(nd), i.e.,\n\nDrop lower-order terms\nDrop constant terms\n\nNote: degree of a polynomial is the highest power e.g. 5n4+3n25 n^4+ 3 n^25n4+3n2 is degree 444 and so will be O(n4)O(n^4)O(n4)\n Big-Omega\n\n\n\n\n\n\n\n\n\nDefinition: Given functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is Ω(g(n))Ω( g(n) )Ω(g(n))\nif there are (strictly) positive constants ccc and n0n_0n0​ such that\nf(n)≥cg(n)f(n)≥c g(n)f(n)≥cg(n) for all n≥n0n \\geq n_0n≥n0​\n\n\nNote that need c&gt;0c &gt; 0c&gt;0, and we not allowed c=0c=0c=0\n\n\nNote that ccc must be constant (cannot depend on nnn)\n\n\nf(n)f(n)f(n) is Ω(g(n))Ω(g(n))Ω(g(n)) says that “f(n)f(n)f(n) grows at least as fast as g(n)g(n)g(n) at large nnn”\n\n\n Linking big-Oh and Big-Omega\n\nThat is: 𝑓∈𝑂(𝑔)→𝑔∈Ω(f)𝑓∈𝑂(𝑔)→𝑔∈Ω(f)f∈O(g)→g∈Ω(f)\nSimilarly: 𝑓∈Ω(𝑔)→𝑔∈O(f)𝑓∈Ω(𝑔)→𝑔∈O(f)f∈Ω(g)→g∈O(f)\nNote: similar to: x&lt;=y→y&gt;=xx &lt;= y →y &gt;= xx&lt;=y→y&gt;=x\n\n Big-Theta\n\n\n\n\n\n\n\n\n\nDefinition: Given functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is Θ(g(n))Θ( g(n) )Θ(g(n))\nif there are positive constants c’c’c’, c’’c ’’c’’ and n0n_0n0​ such that\nf(n)≤c’g(n)≤c’’g(n)f(n)≤c’g(n) \\leq c ’’g(n)f(n)≤c’g(n)≤c’’g(n)\nfor all n≥n0n \\geq n_0n≥n0​\n\nΘΘΘ expresses “grows ‘exactly’ as fast as\n\n Θ is an equivalence relation\nAny relation that is Reflexive &amp; Symmetric &amp; Transitive is an “equivalence relation”\n\nRoughly speaking: it behaves like a “equality”:\n\nIt is reasonable to write “𝑓=Θ(𝑔)𝑓=Θ(𝑔)f=Θ(g)”\n Little-Oh\n\n\n\n\n\n\n\n\n\nDefinition: Given (positive) functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is o(g(n))o( g(n) )o(g(n))\nif for all positive (real) constants c&gt;0c &gt; 0c&gt;0\nthere exists n0n_0n0​ such that\nf(n)&lt;cg(n)f(n)&lt;c g(n)f(n)&lt;cg(n) for all n≥n0n  \\geq n_0n≥n0​\n\n\nSpot the difference from big-Oh?\n\n“for all c&gt;0c &gt; 0c&gt;0” rather than “there exists c&gt;0c &gt; 0c&gt;0”\n(The change of “≤≤≤” to“&lt;&lt;&lt;“ is much less important – see later.)\n\n\n\nSays (roughly) that the ratio f(n)/g(n)→0f(n)/g(n) →0f(n)/g(n)→0 as n→∞n →\\infinn→∞\n\n\nNote that n0n_0n0​ is allowed to depend on ccc\n\n\nooo expresses “strictly less than”\n\n\n\n Linked list\n Singly Linked List\n\n\n\n\n\n\n\n\n\nA singly linked list is a concrete data structure consisting of a sequence of nodes\nEach node stores\n\nelement e.g.\n\nReference to an Object\nA primitive date type (int,…)\n\n\n“link”: a reference (pointer) to the next node\n\n\n Inserting at the Head\n\n\nAllocate a new node\n\n\nInsert new element\n\n\nHave new node point to old head\n\n\nUpdate head to point to new node\n\n\nWhat is the complexity (with n elements in list)?\n\nAnswer: O(1)\nVery efficient!\n\n Removing at the Head\n\nUpdate head to point to next node in the list\nAllow garbage collector to reclaim the former first node\n\nOr do explicit free in C/C++\n\n\n\nAgain, the operation is O(1), and so efficient.\n Inserting at the Tail\n\nAllocate a new node\nInsert new element\nHave new node point to null\nHave old last node point to new node\nUpdate tail to point to new node\n\nComplexity: O(1)\n Removing at the Tail\nTo find new tail we must walk the list from the head\n\nThere is no constant-time way to update the tail to point to the previous node\n\nComplexity: O(n)\n Doubly Linked List\n\n\n\n\n\n\n\n\n\nA doubly linked list provides a natural implementation of a List\n\nNodes implement “Position” and store:\n\nelement\nlink to the next node\nlink to the previous node\n\n\nDeletion at the tail is now O(1)\nBut uses more memory\n\n\n Insertion Algorithm\n12345678Algorithm addAfter(p,e):    Create a new node v    v.setElement(e)    v.setPrev(p) //link v to its predecessor    v.setNext(p.getNext()) //link v to its successor    (p.getNext()).setPrev(v) //link p’s old successor to v    p.setNext(v) //link p to its new successor, v    return v //the position for the element e\n\n Simple Sorting Algorithm\n Bubble Sort\n Basic Idea\n\nOuter loop:\n\nRepeated scans through array\n\n\nInner loop: on each scan do comparison with immediate neighbour\n\nthink of air bubbles rising in water\ndo swaps to make sure that the largest number “bubbles up” to the end of the array\n\n\n\n Pseudocode\n12345678910111213void bubbleSort(int arr[])&#123;    int i;    int j;    int temp;    for(i= arr.length - 1; i &gt; 0; i--)&#123;        for(j = 0; j &lt; i; j++)&#123;            if(arr[j] &gt; arr[j+1])&#123;            temp = arr[j];            arr[j] = arr[j+1];            arr[j+1] = temp;        &#125;// end inner loop    &#125;//end outer loop&#125;// end bubble sort\n Complexity of bubble sort\nAll together: t((n−1)+(n−2)+...+1)+k+t1(n−1)t ((n-1) + (n-2) + ... + 1)+ k + t_1(n-1)t((n−1)+(n−2)+...+1)+k+t1​(n−1)\n\n\nwhere ttt is the time required to do one comparison, one swap, check the inner loop condition and increment jjj.\n\n\nWe also spend constant time kkk declaring i,j,tempi,j,tempi,j,temp and initialising iii. Outer loop is executed n−1n-1n−1 times, suppose the cost of checking the loop condition and decrementing iii is t1t_1t1​.\n\n\n(worst-case) complexity O(n2)O(n^2)O(n2) by taking c=t+t1+kc = t + t_1+ kc=t+t1​+k and n0=1n_0= 1n0​=1.\n\n\n Selection Sort\n Basic Idea\nInstead of always try to move the “greatest element so far” immediately, we just remember its location and move it at end of scan\n Why one want to do this?\n\nSuppose that the entries are large then a swap operation might be quite expensive\nSo might want to reduce the number of swaps by directly moving entries to “the right place”\n\n Pseudocode\n123456789101112131415void selectionSort(int arr[])&#123;    int i, j, temp, pos_greatest;    for( i = arr.length-1; i &gt; 0; i--)&#123;        pos_greatest = 0;        for(j = 0; j &lt;= i; j++)&#123;        \tif( arr[j] &gt;= arr[pos_greatest])        \t\tpos_greatest = j;        &#125;//end inner for loop        if ( i != pos_greatest ) &#123;            temp = arr[i];            arr[i] = arr[pos_greatest];            arr[pos_greatest] = temp;         &#125;    &#125;//end outer for loop&#125;//end selection sort\n Complexity of selection sort\nCompared to bubble sort:\n\nSame number of iterations\nSame number of comparisons in the worst case\nfewer swaps (one for each outer loop = n-1)\nHence, also O(n2)O(n^2)O(n2)\n\n Insertion Sort\n Basic Idea\nKeep the front of the list sorted, and as we move through the back, elements we insert them into the correct place in the front\n Pseudocode\n1234567891011void insertionSort(int arr[])&#123;for(int j = 1; j &lt; arr.length; j++)&#123;    int temp = arr[j];    int i = j; // range 0 to j-1 is sorted    while(i &gt; 0 &amp;&amp; arr[i-1] &gt; temp)&#123;        arr[i] = arr[i-1];        i--;    &#125;    arr[i] = temp;    &#125; // end outer for loop&#125; // end insertion sort\n Complexity of insertion sort\n\nIn the worst case, has to make n(n−1)/2n(n-1)/2n(n−1)/2 comparisons and shifts to the right\nalso O(n2)O(n^2)O(n2) worst case complexity\nBest case: array already sorted\n\nBackwards walk of inner loop stops immediately; no shifts.\nBecomes O(n)O(n)O(n)\n\n\n\n Adaptive Sort\nThis is a (bizarre) name for what asking what happens to the complexity when the lists are already “nearly sorted”.\nIn many applications lists might already be close to being sorted.\n\nE.g. maybe they were from a list that was sorted and then some corrections were made.\nIt is then natural to ask, for each algorithm, whether the efficiency improves.\n\n Sorting “Stability”\n\n\ncompare(o1,o2)==0 means objects o1 and o2 are\n\nequal with respect to the desired ordering\nbut not necessarily that they have the same contents\n\ni.e. are not identical !\n\n\n\n\n\nIf sorting a spreadsheet, then might sort by one column then another.\n\n\nDo not want the sorting to unnecessarily change the order of the rows, as this can be annoying and confusing.\n\n\n“Sort by column A, followed by a stable sort on column B” means that still will have a secondary sort on column A\n\n\n Sorting on Lists\n\n\nBubble sort is just as efficient (or rather inefficient) on linked lists.\n\nWe can easily bubble sort even for a singly linked list.\n\n\n\nSelection sort on linked lists implementation similar to bubble sort; also O(n2)O(n^2)O(n2)\n\n\nInsertion sort is a suitable sorting method (only) for doubly linked lists – as need to walk backwards\n\n\n\n Tree\n Tree Terminology\n\n\nRoot: node without parent (A)\n\n\nInternal node: node with at least one child (A, B, C, F)\n\n\nExternal node (a.k.a. leaf ): node without children (E, I, J, K, G, H, D)\n\n\nAncestors of a node: parent, grandparent, grand-grandparent, etc.\n\n\nDepth of a node: number of ancestors (not counting itself)\n\n\nHeight of a tree: maximum depth of any node = length of longest path from root to a leaf\n\nHeight of tree below = 3\n\n\n\nDescendant of a node: child, grandchild, grand-grandchild, etc.\n\n\nSubtree: tree consisting of a node and its descendants\n\n\n\n Traversals\nGiven a data structure, a common task is to traverse all elements\n\nvisit each element precisely once\nvisit in some systematic and meaningful order\nNote “visit” means “process the contents” but does not include just “passing through using the links”\n\n Preorder Traversal\n\nIn a preorder traversal, a node is visited before its descendants\nApplication: print a structured document\n\n1234Algorithm preOrder(v)    visit(v)    for each child w of v        preorder (w)\n Postorder Traversal\n\nIn a postorder traversal, a node is visited after its descendants\nApplication: compute space used by files in a directory and its subdirectories\n\n1234Algorithm postOrder(v)    for each child w of v    \tpostOrder (w)    visit(v)\n Inorder Traversal\n\n\nIn an inorder traversal a node is visited after its left subtree and before its right subtree\n\n\nApplication: draw a binary tree by (x,y) coords:\n\nx(v) = inorder rank of v\ny(v) = depth of v\n\n\n\n123456Algorithm inOrder(v)    if hasLeft (v)    \tinOrder (left (v))    visit(v)    if hasRight (v)    \tinOrder (right (v))\n Binary Trees\n Properties\n\nEach internal node has at most two children\nThe children of a node are an ordered pair - though one might be “missing”\n\n Consensus\n\nWe call the children of an internal node left child and right child\nAlternative recursive definition: a binary tree is either\n\na tree consisting of a single node, or\na tree whose root has an ordered pair of “children”, each of which is missing (a null) or is the root of a binary tree\n\n\nApplications: searching\n\n\n Proper Binary Trees\n Properties\n\nEach node has either two children or no children\nThe children of a node are an ordered pair\n\n Applications:\n\narithmetic expressions\ndecision processes\n\n\n Application details\n\n Print Arithmetic Expressions\nSpecialization of an inorder traversal\n\n\nprint operand or operator when visiting node\n\n\nprint “(” before traversing left subtree\n\n\nprint “)” after traversing right subtree\n\n\n1234567Algorithm printExpression(v)    if hasLeft (v) print(&quot;(&quot;)        printExpression (left(v))        print(v.element ())    if hasRight (v)        printExpression(right(v))        print (&quot;)&quot;)\n Evaluate Arithmetic Expressions\nSpecialization of a postorder traversal:\n\nrecursive method returning the value of a subtree\nwhen visiting an internal node, combine the values of the subtrees\n\n12345678Algorithm evalExpr(v)    if isExternal (v)        return v.element ()    else        x ← evalExpr(leftChild (v))        y ← evalExpr(rightChild (v))        ◊ ← operator stored at v    return x ◊ y\n Abstract Data Types (ADTs)\n\n\n\n\n\n\n\n\n\nAn abstract data type (ADT) is an abstraction of a data structure\nAn ADT specifies:\n\nData stored\nOperations on the data\nError conditions associated with operations\n\nAn ADT does not specify the implementation itself -hence “abstract”\n Example: ADT modeling a simple stock trading system\n\nThe data stored are buy/sell orders\nThe operations supported are\n\norder buy(stock, shares, price)\norder sell(stock, shares, price)\nvoid cancel(order)\n\n\nError conditions:\n\nBuy/sell a non existent stock\nCancel a non existent order\n\n\n\n Concrete Data Types (CDTs)\n\n\n\n\n\n\n\n\n\nThe actual date structure that we use\nAn ADT might be implemented using different choices for the CDT\n\nThe choice of CDT will not be apparent from the interface: “data hiding” “encapsulation”\n\ne.g. see ‘Object Oriented Methods’\n\n\nThe choice of CDT will affect the runtime and space usage\n\n ADT &amp; Efficiency\n\n\nOften the ADT comes with efficiency requirements expressed in big-Oh notation, e.g.\n\n\n“cancel(order) must be O(1)”\n\n\n“sell(order) must be O(log( |orders| ) )”\n\n\n\n\nHowever, such requirements do not automatically force a particular CDT.\n\nThe underlying implementation is still not specified\n\n\n\nThis is typical of many “library functions”\n\n\n Tree ADT\nWe can use “positions”, p, to abstract nodes\n Generic methods:\n\ninteger size()\nboolean isEmpty()\nIterator iterator()\nIterator positions()\n\n Accessor methods:\n\nposition root()\nposition parent(p)\nIterator children(p)\n\n Query methods:\n\nboolean isInternal(p)\nboolean isExternal(p)\nboolean isRoot(p)\n\n Update method:\n\n\nobject replace (p, o)\n\n\nAdditional update methods may be defined by data structures implementing the Tree ADT\n\n\n Array-Based Representation of Binary Trees\nlet rank(node) be defined as follows:\n\n\nrank(root) = 1\n\n\nif node is the left child of parent(node),\nrank(node) = 2*rank(parent(node))\n\n\nif node is the right child of parent(node),\nrank(node) = 2*rank(parent(node))+1\n\n\n Implementation\nRemember that if think of the rank, r(n) of node n, as a binary number then\n\n\nr(n) = r(par(n)) &lt;&lt; 1 + 0 for left\n\n\nr(n) = r(par(n)) &lt;&lt; 1 + 1 for right\n\n\nE.g. r(par(n)) = 101 gives children at\n\n“101”+”0” = 1010\n“101”+“1” = 1011\n\n\n\nGoing to the parent is a right shift\n\nr(par(n)) = r(n) &gt;&gt; 1\n\n Advantages of the tree-as-array structure:\n\nSaves space as do not have to store the pointers – they are replaced by fast computations\nThe storage can be more compact –“better memory locality” and this can be good because of cache and memory hierarchies, when an array element is accessed then other entries can be pulled into the cache, and so access becomes faster.\n\n Perfect binary trees\n Properties\n\n\nA binary tree is said to be “proper” (a.k.a. “full”) if every internal node has exactly 2 children\n\n\nIt is “perfect” if it is proper and all leaves are at the same depth; hence all levels are full.\n\n\n\nCounting suggests numbers of nodes are:\n\n2d2^d2d at level ddd\n2d+1−12^{d+1}-12d+1−1 at level ddd or less\n\nHeight (hhh) is logarithmic in size (nnn)\n\n\nheight h=log2(n+1)−1h = log_2 (n + 1) - 1h=log2​(n+1)−1 where nnn in the number of nodes.\n\n\nnumber of levels: h+1=log2(n+1)h + 1 = log_2 (n + 1)h+1=log2​(n+1)\n\n\nnodes n=2h+1−1n = 2^{h+1} - 1n=2h+1−1\n\n\nFor a general binary tree on nnn nodes, the height is Ω(log(n))\\Omega(log(n))Ω(log(n)) and O(n)O(n)O(n)\n Merge Sort\n Divide-and-Conquer\nDivide-and-conquer is a general algorithm design paradigm:\n\nDivide: divide the input data SSS in two disjoint subsets S1S_1S1​ and S2S_2S2​\nRecur: solve the subproblems associated with S1S_1S1​ and S2S_2S2​\nConquer: combine the solutions for S1S_1S1​ and S2S_2S2​ into a solution for SSS\n\n Merge-Sort\nMerge-sort on an input sequence (array/list) SSS with nnn elements consists of three steps:\n\nDivide: partition SSS into two sequences S1S_1S1​ and S2S_2S2​ of about n/2n/2n/2 elements each\nRecur: recursively sort S1S_1S1​ and S2S_2S2​\nConquer: merge S1S_1S1​ and S2S_2S2​  into a unique sorted sequence\n\n Implementation\n1234567891011public void recMergeSort (int[] arr, int[] workSpace, int l, int r) &#123;    if (l == r) &#123;    \treturn;    &#125; else &#123;        int m = (l+r) / 2;        recMergeSort(arr, workSpace, l, m);        recMergeSort(arr, workSpace, m+1, r);        merge(arr, workSpace, l, m+1, r);    &#125;&#125;// Initial call is with l=0 and r to be end of the array\n Merge-Sort Tree\nAn execution of merge-sort is depicted by a binary tree\n\neach node represents a recursive call of merge-sort and stores\n\nunsorted sequence before the execution and its partition\nsorted sequence at the end of the execution\n\n\nthe root is the initial call\nthe leaves are calls on subsequences of size 0 or 1\n\n\n Analysis of Merge-Sort\nThe height hhh of the merge-sort tree is O(logn)O(log n)O(logn)\n\nat each recursive call we divide in half the sequence,\n\nThe overall amount of work done at all the nodes at depth iii is O(n)O(n)O(n)\n\nwe partition and merge 2i2^i2i sequences of size n/2in/2^in/2i\nwe make 2i+12^{i+1}2i+1 recursive calls\nthe numbers all occur and are all “used” at each depth\nSo, each depth uses O(n)O(n)O(n) work\n\nThus, the total running time of merge-sort is O(nlogn)O(nlog n)O(nlogn)\n\n Summary\n\nFast sorting method for arrays\nGood for sorting data in external memory – because works with adjacent indices in the array (data access is sequential)\n\nIt accesses data in a sequential manner (suitable for sorting data on a disk)\n\n\nNot so good with lists: relies on constant time access to the middle of the sequence\n\n\n Recurrence Relations\n Example\n\n\nHow would we solve T(n)=2 T(n/2)+bT(n) = 2\\,T(n/2) + bT(n)=2T(n/2)+b with T(1)=1T(1)=1T(1)=1\n\n\nWe know T(1)=1T(1)=1T(1)=1, hence\n\nT(2)=2 T(1)+b=2+bT(2) = 2\\,T(1) + b = 2 + bT(2)=2T(1)+b=2+b\nT(4)=2 T(4/2)+b=2 (2+b)+b=4+(2+1)bT(4) = 2\\,T(4/2) + b = 2\\,( 2 + b) + b = 4 + (2+1)bT(4)=2T(4/2)+b=2(2+b)+b=4+(2+1)b\nT(8)=2 (4+(2+1)b)+b=8+(4+2+1)bT(8) = 2\\,(4 + (2+1)b) + b = 8 + (4+2+1) bT(8)=2(4+(2+1)b)+b=8+(4+2+1)b\n\n\n\nIt seems a good guess\nT(2k)=2k+(2(k−1)+…+1)bT(2k) = 2^k+ (2^{(k-1)}+…+1)bT(2k)=2k+(2(k−1)+…+1)b\n=2k+(2k−1)b= 2^k+ (2^k-1) b=2k+(2k−1)b\n\n\nSo T(n)=n+(n−1)b=(1+b)n−bT(n) = n+(n-1) b = (1+b)n-bT(n)=n+(n−1)b=(1+b)n−b for nnn in {1,2,4,8…}\\{1,2,4,8…\\}{1,2,4,8…}\n\n\nStill Θ(𝑛)Θ(𝑛)Θ(n)\n\n\nClaim: T(2k)=2k+(2k−1)bT(2^k) = 2^k+ (2^k-1) bT(2k)=2k+(2k−1)b\n\n\nProof by induction:\n\n\nBase case: k=0,T(1)=1+(1−1)∗b=1k=0, T(1) = 1 + (1-1)*b = 1k=0,T(1)=1+(1−1)∗b=1\n\n\nStep case: assume true at kkk\n\n\n$T(2^{k+1}) = 2 T(2^k) + b $\n=2(2k+(2k–1)b)+b= 2 (2^k+ (2^k–1) b ) + b=2(2k+(2k–1)b)+b\n=2k+1+(2k+1−2+1)b= 2^{k+1}+ (2^{k+1}-2 + 1) b=2k+1+(2k+1−2+1)b\n=2k+1+(2k+1−1)b= 2^{k+1}+ (2^{k+1}-1) b=2k+1+(2k+1−1)b\n\n\nQED.\n\n\n Master Theorem (MT)\nFor a given recurrence of the form T(n)=a⋅T(n/b)+f(n)T(n)=a \\cdot T(n/b) +f(n)T(n)=a⋅T(n/b)+f(n) the M.T. can tell us the growth rate of T(n)T(n)T(n) according to three cases:\n\n\nCase 1: Recurrence dominates (plus special case that f(n)=0f(n)=0f(n)=0)\nIF f(n)f(n)f(n) is O(nc)O(n^c)O(nc) with c&lt;logbac&lt;log_bac&lt;logb​a,\nTHEN T(n)T(n)T(n) is Θ(nlogba)\\Theta(n^{log_ba})Θ(nlogb​a)\n\n\nCase 2: Neither term dominates\nIF f(n)f(n)f(n) is Θ(nc(log n)k)\\Theta(n^c(log\\,n)^k)Θ(nc(logn)k) with c=logbac = log_bac=logb​a and k≥0k \\geq 0k≥0,\nTHEN T(n)T(n)T(n) is Θ(nc(log n)k+1)\\Theta(n^c(log\\, n)^{k+1})Θ(nc(logn)k+1)\n\n\nCase 3: f(n)f(n)f(n) dominates\nIF f(n)f(n)f(n) is Ω(nc)\\Omega(n^c)Ω(nc) with c&gt;logbac &gt; log_bac&gt;logb​a,\nTHEN T(n)T(n)T(n) is Θ(f(n))\\Theta(f(n))Θ(f(n))\n\n\n Quicksort\nQuick-sort is a (randomized) sorting algorithm based on the divide-and-conquer paradigm:\n\nDivide: pick an element xxx(called pivot) and partition SSS into\n\nLLL: elements less than xxx\n\nHave to be careful it is not empty\n\n\nGEGEGE: elements greater than or equal to xxx\nPivot is often picked as a random element\n\n\nRecur: sort LLL and GEGEGE\nConquer: join LLL, GEGEGE\n\n “In-place” or “extra workspace”?\n\nFor sorting algorithms (and algorithms in general) an important issue can be how much extra working space they need besides the space to store the input\n“In-place” means they only a “little” extra space (e.g. O(1)) is used to store data elements.\n\nThe input array is also used for output, and only need a few temporary variables\nbubble-sort is “in-place”\nPrevious “merge” used extra O(n) array\n\n(can be made in-place, but messy and so we ignore this option)\n\n\n\n\n\n Implementation\n12345678public void recQuickSort(int[] arr, intleft, intright) &#123;    if (right - left &lt;= 0) return;    else &#123;        intborder = partition(arr, left, right); // pivot position        recQuickSort(arr, left, border-1);        recQuickSort(arr, border+1, right);    &#125;&#125;\n Quick-Sort Tree (3-way split)\n\n Worst-case Running Time\nThe worst case for quick-sort occurs when the pivot is the unique minimum or maximum element\n\nOne of LLL and E+GE+GE+G has size n−1n -1n−1 and the other has size 111\nThe running time is proportional to the sum\nn+(n−1)+…+2+1n+(n-1) + … +2 + 1n+(n−1)+…+2+1\nThus, the worst-case running time of quick-sort is O(n2)O(n^2)O(n2)\n\n Best-case  and average-case Running Time\nThe best case for quick-sort occurs when the pivot is the median element\n\nThe LLL and GGG parts are equal –the sequence is split in halves, like in merge sort\nThus, the best-case running time of quick-sort is O(nlogn)O(n log n)O(nlogn)\n\nThe average case for quick-sort: half of the times, the pivot is roughly in the middle\n\nThus, the average-case running time of quick-sort is O(nlogn)O(n log n)O(nlogn) again\n\n Motivations for quicksort\nit can be done “in-place”\n\nUses a small amount of workspace\nBecause the “merge” step is now a lot easier!!\n\nThe “split” is more complicated, and the merge “much” easier – but turns out that the quick-sort split is easier to do in-place than the merge-sort merge\n Comparison-based sorting\nAll the algorithms we have seen so far are comparison based\n\nIf you inspect the code then the decision as to swaps\n\netc. is based only of statements like “if ( A[i] &lt;= A[j] )”\n\n\n\nNot all sorts are comparison-based:\n\nbucket sort: used the actual values to rearrange the items\nRuns in O(n), but relies on knowing the range of values in the sequence\n\n(e.g.“integers between 1 and 1000”).\n\n\n\n\n\n\n\n\n\n\n\n\nComparison-based sorting cannot do better than O(nlogn)O(n log n)O(nlogn)\n\n Vector\n Vector ADT\nAn element can be accessed, inserted or removed by specifying its rank\nMain vector operations:\n\nobject elemAtRank(integer r):\n\nreturns the element at rank r without removing it\n\n\nobject replaceAtRank(integer r, object o):\n\nreplace the element at rank with o and return the old element\n\n\ninsertAtRank(integer r, object o):\n\ninsert a new element o to have rank r\n\n\nremoveAtRank(integer r):\nremoves and returns the element at rank r\n\nAdditional operations size() and isEmpty()\n Applications of Vectors\n\nThere is not an automatic limit on the storage size\n\nunlike arrays of a fixed size\n\n\nDirect applications\n\nSorted collection of objects (elementary database)\n\n\nIndirect applications\n\nAuxiliary data structure for many algorithms\nComponents of other data structures\n\n\n\n Array-based Vector\n Performance\nIn the array-based implementation of a Vector\n\nThe space used by the data structure is O(n)O(n)O(n)\nsize, isEmpty, elemAtRankand, replaceAtRankrun in O(1)O(1)O(1) time\ninsertAtRankand removeAtRankrunin O(n)O(n)O(n) time\npush runs in O(1)O(1)O(1) time, as do not need to move elements\n\nunless need to resize the array\n\n\npop runs in O(1)O(1)O(1) time\n\n Comparison of the Strategies\nIn an insertAtRankoperation, when the array is full, instead of throwing an exception, we can replace the array with a larger one\n\nincremental strategy: increase size by a constant ccc\ndoubling strategy: double the size\n\nWe call amortized time of a push operation the average time taken by a push over the series of operations, i.e., T(n)/nT(n)/nT(n)/n\n Why is amortised analysis different from the average case analysis?\n\n“Amortised”: (long) real sequence of dependent operations\n“Average”: Set of (possibly independent) operations\n\nWe have different measures of the runtime cost:\n\nWorst-case “cost per operation of a sequence”\nnot just\n“Worst case of a single operation”\n\n Incremental Strategy Analysis\nAverage, per push operation, is O(n)O(n)O(n)\n\nWe replace the array $k = n/c $ times\nEach “replace” costs the current size\nThe total time T(n)T(n)T(n) of a series of nnn push operations is proportional to\nn+c+2c+3c+4c+…+kc=n+ c + 2c + 3c + 4c +… + kc =n+c+2c+3c+4c+…+kc=\nn+c(1+2+3+…+k)=n+ c(1 + 2 + 3 + … + k) =n+c(1+2+3+…+k)=\nn+ck(k+1)/2n+ ck(k + 1)/2n+ck(k+1)/2\nSince ccc is a constant, T(n)T(n)T(n) is O(n+k2)O(n+k^2)O(n+k2),i.e., O(n2)O(n^2)O(n2)\nThe amortized time of a push operation is O(n)O(n)O(n)\nThis is bad as the normal cost of a push is O(1)O(1)O(1).\n\n Doubling Strategy Analysis\nGives an average of O(1)O(1)O(1) per operation\n\nWe replace the array k=log2nk = log_2nk=log2​n times\nThe total time T(n)T(n)T(n) of a series of nnn push operations is proportional to\nn+1+2+4+8+…+2k−1=n+ 1 + 2 + 4 + 8 + …+ 2^{k-1}=n+1+2+4+8+…+2k−1=\nn+2k−1=2n−1n+2^k-1 = 2n -1n+2k−1=2n−1\nT(n)T(n)T(n) is O(n)O(n)O(n)\nAmortized time of a single push operation is O(1)O(1)O(1)\nThat is, no worse than if all the needed memory was pre-assigned!\n\n(Big-Oh hides the constant factor ‘2’ extra cost.)\n\n\n\n\n Priority Queues &amp; Heaps\n Priority Queue ADT\n\nA priority queue stores a collection of entries\nEach entry is a pair(key, value)\nMain methods of the Priority Queue ADT\n\ninsert(k, v) inserts an entry with key k and value v\nremoveMin() removes and returns the entry with smallest key\nNote: unlike Map (seen later), there is no requirement of a method find(k).\n\n\nAdditional methods\n\nmin() returns, but does not remove, an entry with smallest key\nsize(), isEmpty()\n\n\nApplications:\n\nStandby passengers\nAuctions\nStock market\nPrinter queues\n\n\nTotal Order Relations\n\nJust need a “compare” operation between the keys and that behaves like &lt;= or &lt; as needed\nTwo distinct entries in a priority queue can have the same key\n\n\n\n Heaps\nA heap is a binary tree storing key-value pairs at its nodes and satisfying the following properties:\n\n\nHeap-Order: for every internal node vvv other than the root,\nkey(v)≥key(parent(v))key(v)\\geq key(parent(v))key(v)≥key(parent(v))\n\n\nComplete Binary Tree: let hhh be the height of the heap\n\nfor i=0,…,h−1i= 0, … , h -1i=0,…,h−1, there are 2i2^i2i nodes of depth iii\nThe last node of a heap is the rightmost node of depth hhh\n\n\n\n Insertion into a Heap\n\nFind the insertion node zzz (the new last node)\nStore kkk at zzz\nRestore the heap-order property\n\nAfter the insertion of a new key kkk, the heap-order property may be violated\nAlgorithm upheap restores the heap-order property by swapping kkk along an upward path from the insertion node\nUpheap terminates when the key kkk reaches the root or a node whose parent has a key smaller than or equal to kkk\nSince a heap has height O(log n)O(log\\, n)O(logn), upheap runs in O(log n)O(log\\, n)O(logn) time\n\n\n\n Removal from a Heap\n\nReplace the root key with the key of the last node www\nRemove www\nRestore the heap-order property\n\nAlgorithm downheap restores the heap-order property by swapping key kkk along a particular downward path from the root\nDownheap terminates when key kkk reaches a leaf or a node whose children have keys greater than or equal to kkk\nSince a heap has height O(log n)O(log \\,n)O(logn), downheap runs in O(log n)O(log\\, n)O(logn) time\n\n\n\n Heap-Sort\nConsider a priority queue with nnn items implemented by means of a heap\n\nthe space used is O(n)O(n)O(n)\nmethods insert and removeMin take O(log n)O(log \\, n)O(logn) time\nmethods size, isEmpty, and min take time O(1)O(1)O(1) time\n\nUsing a heap-based priority queue, we can sort a sequence of nnn elements in O(n log n)O(n\\,log\\, n)O(nlogn) time\n\nInsert all elements into the heap one by one.\n\nThis takes nnn insertions with O(log n)O(log\\,n)O(logn) each for a total of O(n log n)O(n \\,log \\,n)O(nlogn)\n\n\nRemove all elements one by one, using removeMin()removeMin()removeMin(), hence obtaining them in sorted order.\n\nThis takes nnn removals with O(log n)O(log\\, n)O(logn) each for a total of O(n log n)O(n \\,log \\,n)O(nlogn).\n\n\n\n Map\n The Map ADT over pairs &lt;K,V&gt;\n\n\n\n\n\n\n\n\n\nA map models a collection of key-value entries that is searchable `by the key’\nThe main operations of a map are for searching, inserting, and deleting items.\nMultiple entries with the same key are not allowed.\nMap ADT methods:\n\n\nV get(K k):\nif the map M has an entry with key k, return its associated value;\nelse, return null\n\n\nV put(K k, V v): insert entry (k, v) into the map M;\nif key k is not already in M, then return null;\nelse, return old value associated with k\n\n\nV remove(K k):\nif the map M has an entry with key k, remove it from M and return its associated value;\nelse, return null\n\n\nint size(), boolean isEmpty()\n\n\n&#123;K&#125; keys(): return an iterablecollection of the keys in M\n\n\n&#123;V&#125; values(): return an iterablecollection of the values in M\n\n\n&#123;&lt;K,V&gt;&#125; entries(): return an iterablecollection of the entries in M\n\n\n A Simple List-Based Map\nIt is straightforward to implement a map using a list – either singly or doubly-linked\n\n\nThe get(k) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nThe put(k,v) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nThe remove(k) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nOverall: the operations are O(n)O(n)O(n) because of needing to traverse the list\n\n\nList version of a Map is “simple but inefficient”\n\n\nEasier to be correct\n\n\nEven though slow, they can be useful\n\nallow the rest of the project to proceed without waiting for the efficient version\ncan be used for a regression test of a “faster but trickier” later version\n\ne.g. use in a “shadow mode” where the “slow-obviously-correct” version is used together with the “fast-possibly-buggy” version and the results checked against each other.\n\n\n\n\n\n Hash Tables\n\n\n\n\n\n\n\n\n\nHash tables are a concrete data structure which is suitable for implementing maps.\nBasic idea: convert each key into an index into a (big) array.\nLook-up of keys and insertion and deletion in a hash table usually runs in O(1)O(1)O(1) time.\n\nNot guaranteed, and design of the table needs to be done carefully if want the access to be “reliably O(1)O(1)O(1)”\n\n Basic Hash Functions\n\nA hash function hhh maps keys of a given type to integers in a fixed interval [0,N−1][0, N - 1][0,N−1]\n\nExample: h(k)=k  mod  Nh(k) = k \\;mod\\; Nh(k)=kmodN is a hash function for integer keys\n\n\nThe integer h(k)h(k)h(k) is called the hash value of key kkk\n\n Collision Handling\nCollisions occur when different elements are mapped to the same cell\n Hash Functions\n\n\nA hash function is usually specified as the composition of two functions:\n\n\nHash code:\nh1h_1h1​: keys → integers\n\n\nCompression function:\nh2h_2h2​: integers → [0,N−1][0, N - 1][0,N−1]\n\n\nDivision:\n\n\nh2(y)=y  mod  Nh_2 (y) = y\\; mod \\;Nh2​(y)=ymodN\n\n\nThe size NNN of the hash table is usually chosen to be a prime\n(hash codes will tend to spread better)\n\n\n\n\nMultiply, Add and Divide (MAD):\n\nh2(y)=(ay+b)  mod  Nh_2 (y) = (ay + b)\\; mod\\; Nh2​(y)=(ay+b)modN\naaa and bbb are nonnegative integers\nsuch that a  mod  N≠0a\\; mod\\; N \\not= 0amodN=0\nOtherwise, every integer would map to the same value bbb\n\n\n\n\n\n\n\nThe hash code is applied first, and the compression function is applied next on the result,\ni.e., h(x)=h2(h1(x))h(x) = h_2(h_1(x))h(x)=h2​(h1​(x))\n\n\nThe goal of the hash function is to “disperse” the keys in an “apparently random” way\n\nWhy disperse?\n\nto reduce numbers of collisions\n\n\nWhy random?\n\nrandom means ‘no pattern’\nif there is an obvious pattern then the incoming data might have a matching pattern that leads to many collisions\n“sometimes ‘no pattern’ is the only safe pattern” (e.g. rock-paper-scissors game)\n\n\n\n\n\n Separate Chaining\nlet each cell in the table point to (e.g.) a linked list of entries that map there\n\nNote: In practice, should use a more efficient Map;\ne.g. a Binary Search Tree (BST)\n\n\n\n\n1234V get(k)&#123;    return A[h(k)].get(k); \t// Simply delegates the “get” to the list-based map at A[h(k)]&#125;\n\n\n123456789101112V put(k,v)&#123;   /*    * If there is an existing entry in our map with key equal to k,     * then we return its value (replacing it with v);     * otherwise, we return null    */    t = A[h(k)].put(k,v); // t is new value or null    // Simply delegates the put to the list-based map at A[h(k)]    if (t = null) n = n + 1; // if k is a new key    return t;&#125;\n\n\n123456V remove(K k)&#123;\tt = A[h(k)].remove(k);    // Simply delegates the remove to the list-based map at A[h(k)]    if (t != null) n = n - 1 // k was found    return t&#125;\n\n\nSeparate chaining is simple and fast, but requires additional memory outside the table.\n\n\nWhen memory is critical then we try harder to remain within the existing memory\n\n\n Open addressing\n\n\n\n\n\n\n\n\n\nthe colliding item is placed in a different cell of the table\n\n\nLinear probing handles collisions by placing the colliding item in the next (circularly) available table cell\n\n“Circular array” – once get to the right-hand end, then start again at the beginning of the array\n\n\n\nDisadvantage: Colliding items lump together, causing future collisions to cause a longer sequence of probes\n\n\nV get(k)\n\nWe start at cell h(k)\nWe probe consecutive locations until one of the following occurs\n\nAn item with key k is found, or\nAn empty cell is found, or\nN cells have been unsuccessfully probed\n\n\n\n\n\nsafely remove an element x\n\n\nFind x using `get’ and set the entry back to blank\n\n\n“Lazy deletion”: don’t mark the entry as a blank, but as a ‘deleted’ and fix the entries later\n\n\nMove such entries by removing them and then re-inserting them all\n\n\n\n\n Double Hashing\nDouble hashing uses a secondary hash function d(k)d(k)d(k) and handles collisions by placing an item in the first available cell of the series\n\n\n(h(k)+j⋅d(k))  mod  N(h(k) + j\\cdot d(k))\\; mod \\;N(h(k)+j⋅d(k))modN for j=0,1,…,N−1j = 0, 1, … , N - 1j=0,1,…,N−1\n\n\nThe secondary hash function d(k)d(k)d(k) cannot have zero values\n\nCommon choice for the secondary hash function:\nd(k)=q−(k  mod  q)d(k) = q - (k \\;mod\\; q)d(k)=q−(kmodq) where\n\nq&lt;Nq &lt; Nq&lt;N\nqqq is a prime\n\n\n\n\n\nLinear probing is just d(k)=1d(k)=1d(k)=1\n\n\nThe table size NNN must be a prime to allow probing of all the cell\n\nWith a prime NNN, then eventually all table positions will be probed\n\n\n\nExample\n\n\n\n Performance of Hashing\n\n\nIn the worst case, searches, insertions and removals on a hash table take O(n)O(n)O(n) time\n\n\nThe worst case occurs when all the keys inserted into the map collide\n\n\nThe load factor a=n/Na = n/Na=n/N affects the performance of a hash table\n\n\nIn Java, maximal load factor is 0.75 (75%) – after that, rehashed\n\n\n Summary\nThe expected running time of all the map ADT operations in a hash table is O(1)O(1)O(1)\n\nIn practice, hashing is very fast provided the load factor is not close to 100%\n\n Re-Hashing\nWhen the table gets too full then “re-hash”: Create a new larger table and new hash function.\n\n\nNeed to (eventually) transfer all the entries from the old table to the new one\n\n\nIf do so immediately, then\n\none can amortise the cost over many entries (as for Vector) and so get an average cost of O(1)O(1)O(1) again\nbut the worst case might be O(n)O(n)O(n) when the table is rehashed, and this might be bad for a real time system\nOption:\ndo not transfer all entries “in one go” but do “a few at a time”\nKeep both tables until the transfer is complete; but only do insertions into the new table\n\n\n\n Applications of Hashing\n\nDirect applications of hash tables:\n\nsmall databases\ncompilers\nbrowser caches\n\n\nHash tables as an auxiliary data structure in a program\n\n\n Binary Search Trees\n\n\n\n\n\n\n\n\n\nA binary search tree is a binary tree storing key-value entries at its internal nodes and satisfying the following “search tree” property:\nLet u,vu, vu,v, and www be any three nodes such that\nuuu is in the left subtree of vvv and\nwww is in the right subtree of $ v$.\n\nThen we must have\nkey(u)≤key(v)≤key(w)key(u) \\leq key(v) \\leq key(w)key(u)≤key(v)≤key(w)\nor, as we will assume there are no duplicate keys:\nkey(u)&lt;key(v)&lt;key(w)key(u) &lt; key(v) &lt; key(w)key(u)&lt;key(v)&lt;key(w)\n\n Search Algorithm and Basic operation\n Pseudocode\n123456789Node TreeSearch(Key k, Node n)    if n.isExternal () // or, “if n == null”    \treturn null    if k &lt; n.key()    \treturn TreeSearch(k, n.left())    else if k = n.key()    \treturn n    else // k &gt; n.key()    \treturn TreeSearch(k, n.right())\n Fundamental Property of Search Tree\n\n\nAn in-order traversal of a (binary) search trees visits the keys in increasing order\n\n\nNote that to access the minimum key, we just need to ‘always go left’\n\n\n Insertion\n\nWe search for key k (using Tree Search)\nIf k is already in the tree then just replace the value\nOtherwise, k is not already in the tree, and let w be the leaf reached by the search\n\nWe “insert k at node w and expand w into an internal node”\n\n\nAgain, only follows a path from the root and so is O(h)O(h)O(h)\n\n Deletion\nFour cases\n\n\nk is not present, nothing to do\n\n\nn has no children, straightforward remove\n\n\nn has one child\n\n\nn has two children\n\n\n Deletion – with one child\nExample: remove 4\n\nTo perform operation remove(4), we search for key 4.\nLet nnn be the node storing 4.\nNode nnn has a null left child, and a real child 5\nWe remove nnn from the tree and connect 5 back to the parent of nnn\n\n      \n Deletion – with two children\nExample: remove 3\nThe key node nnn has two internal children\n\n\nwe find the internal node www that follows nnn in an in-order traversal\n\n\nwe copy key(w)key(w)key(w) into node nnn\n\n\nwe remove node www by means of same procedure as before for “one child”\n\n\n      \n Balanced Trees\nBinary search trees: if all levels filled, then search, insertion and deletion are O(log n)O(log \\, n)O(logn).\n\nAs they are all O(height)O( height )O(height)\n\n Performance\nThe height hhh is O(n)O(n)O(n) in the worst case and O(log n)O(log \\, n)O(logn) in the best case\nCould make trees balanced using a “total rebuild”\n\nBut would require O(n)O(n)O(n), and so very inefficient compared to the desired O(log n)O( log\\, n)O(logn)\nRe-balancing needs to be O(log n)O(log \\, n)O(logn) or O(height)O( height )O(height)\n\n\n Dynamic Programming\nThere are various general methods (“paradigms”) for finding solutions to problems:\n\nBrute force – “generate and test”\nDivide-and-conquer\nHeuristics\nDynamic Programming\n\n Brute Force\n\n\n\n\n\n\n\n\n\nThis is roughly “generate and test”\n\nGenerate all potential solutions\nTest for which ones are actual solutions\n\n\n\nExample: we could do “sorting” by\n\nGenerate all possible permutations\nTest to see which one is correctly ordered\n\nExtremely inefficient, as is O(n!)O(n!)O(n!)\n\n\n\n\n\nCan be useful in some (small) cases\n\nE.g. Due to the simplicity\n\n\n\n Divide and Conquer\n\n\n\n\n\n\n\n\n\nRecursively, break the problem into smaller pieces, solve them, and put them back together\nMerge-sort and Quicksort were classic examples\n Heuristics\n\n\n\n\n\n\n\n\n\n“Heuristic” = “rule of thumb”\n\nGenerally, meant to mean something that gives better decisions, than the naive methods, but still not necessarily optimal\n\n Heuristics in exact methods\nThese are general methods that works in an algorithm that does give exact or optimal answers\n\nBut need the heuristics to decrease the (average/typical) runtime\n\nExamples:\n\n“Admissible heuristic” in A* search – decreases the search time compared to plain search\n“pick a random pivot” in quicksort\n\n Heuristics in inexact methods\nThese are general methods that (generally) are not be guaranteed to give the best possible answers, but that can give good answers quickly\nUsed on problems when the exact methods are too slow\n\nTimetabling and scheduling and many design problems\n\n Greedy algorithms\nA common “heuristic” is to be “greedy”\n\nTake the decision that looks best in the short term – without looking ahead\n\nSometimes greedy algorithms can still give optimal answers\n\nE.g. Prim’s algorithm for constructing a Minimal Spanning Tree is a greedy algorithm\n\nUsually greedy algorithms cannot guarantee to give optimal answers\n\n\nbut often still give (nearly) optimal answers in practice\n\n\nExample: “Change-giving”:\n\nProblem: given a collection of coins (a multi-set, that allows repeated elements), and a desired target for the change. Supply the change in as few coins as possible\nPick the largest coin which is still available and does not cause to exceed the target\n\n\n\n Dynamic Programming (DP)\nDP is a general method that can be suitable when the optimal solutions satisfy a “decomposition property”\nThe general idea is roughly: solve small sub-problems first, then build up towards the full solution.\n Subset-Sum\nGiven (multi-)set S of positive integers x[i] and a target K. Is there a subset of S that sums to exactly K?\n\n\nBoolean Array, Y, for [0,…,K]\n\nY[m] = true iff some subset has been found that sums to m\n\n\n\nMain idea: if some subset summed to m, then with the inclusion x[i], we can also find a subset that sums to m+x[i]\n\n\n Pseudocode:\n123456789101112Input: x[0],…,x[n-1] and KInitialise all Y[m] = false for m=1,…, KY[0] = true; // As can always provide no changefor (int i=0 ; i&lt;n ; i++) &#123; // consider effect of x[i]    for (int m=K-x[i] ; m&gt;=0 ; m--) &#123; // “scan down”        if (Y[m]==true) &#123;                     // m was achievable with x[0]… x[i-1]            if (m+x[i] == K ) return success; // hence now also m+x[i] is achievable            if (m+x[i] &lt; K ) Y[ m+x[i] ] = true;        &#125;    &#125;&#125;\n Complexity\n\nOuter loop has to consider all the coins, hence O(n)O(n)O(n)\nInner loop scans the entire array Y, hence O(K)O(K)O(K)\nOverall is O(nK)O( n K )O(nK)\n\nHowever, “K” has the “hidden exponential” if it is represented in binary:\nThe relevant input size is the number of bits BBB that are needed to represent, B=O(log(K))B=O(log(K))B=O(log(K))\n\n\nThe complexity in terms of the size of the binary input is O(n 2B)O(n\\,2^B )O(n2B), which is called “pseudo-polynomial”\n\n Min-Coins version\nPrevious just asked if it is possible to do the change. But want to minimise the coins.\n\n\nAlgorithm: Inspect the coins one at a time keeping track of the best answers obtained so far\n\n\nMain data structure:\n\nInteger Array, Y, for [0,…,K]\n\nY[m] = -1 if have not found any sum for m as yet\nY[m] = c &gt;= 0 means that have found that can achieve the sum m with c coins.\n\n\n\n\n\nAim: when the algorithm finishes then Y[K] will be the minimum number of coins\n\n“Side-effect”: All the values of Y[m] m &lt; K, will also be the minimum number for a value of m.\n\n\n\nMain idea: if some set summed to m, then with the inclusion x[i] we can also find a subset that sums to m+x[i]\n\n\n1234567891011121314151617Input: x[0],…,x[n-1] and KInitialise: Y[0] = 0,and Y[m] = -1 for m &gt; 0 // 0 coins can give a change of 0for (i=0 ; i&lt;n ; i++) &#123; // consider effect of x[i]    for (m=K-x[i] ; m&gt;=0 ; m--) &#123; // scan array        if (Y[m] &gt;= 0 ) &#123;            // value m was achievable with x[0]…x[i-1] using Y[m] coins,            // so, m+x[i] is now achievable with Y[m]+1 coins            // but might already have found a better answer            // stored as Y[m + x[i] ] so then take the best            if (Y[m + x[i] ] == -1 )            \tY[ m + x[i] ] = Y[m] + 1;            else                Y[ m + x[i] ] = min( Y[m + x[i] ] , Y[m] + 1 );        &#125;    &#125;&#125;\n Worked example\nInput: x[] = {5,2,2,2,1} and K=6\n\nk=0, Y[] = [0,-1,-1,-1,-1,-1,-1], Y[0]=0 for change {}\nk=1, Y[] = [0,-1,-1,-1,-1,1,-1], Y[5]=1 for change {5}\nk=2, Y[] = [0,-1,1,-1,-1,1,-1], Y[2]=1 for change {2}\nk=3, Y[] = [0,-1,1,-1,2,1,-1], Y[4]=2 for change {2,2}\nk=4, Y[] = [0,-1,1,-1,2,1,3], Y[6]=3 for change {2,2,2}\nk=5, Y[] = [0,-1,1,-1,2,1,2], Y[6]=min(3, 1+1)=2 for change {5,1}\nFinished: so optimal answer is 2 coins.\n\n Minimum Spanning Trees\n Spanning Tree\n\nInput: connected, undirected graph\nOutput: a tree which connects all vertices in the graph using only the edges present in the graph\n\n Minimum Spanning Tree\n\n\nInput: connected, undirected, weighted graph\n\n\nOutput: a spanning tree\n\n(connects all vertices in the graph using only the edges present in the graph)\nand is minimum in the sense that the sum of weights of the edges is the smallest possible for any spanning tree\n\n\n\nUsages: Gas and water pipelines, optic fibers networks…\n\n\n Why MST is a tree\n\nWe really want a minimum spanning sub-graph\n\na subset of the edges that is connected and that contains every node\n\n\nIf a graph is connected and acyclic then it is a tree\n\n Prim’s algorithm\nTo construct an MST:\n\n\nStart by picking any vertex M\n\n\nChoose the shortest edge from M to any other vertex N\n\n\nAdd edge (M, N) to the MST\n\n\nLoop:\n\n\nContinue to add at every step a shortest edge from a vertex in MST to a vertex outside,\nuntil all vertices are in MST\n\n\n(If there are multiple shortest edges, then can take any arbitrary one)\n\n\n\n\n Why is this optimal!?\nArgument by contradiction\n\n\nlet V1 and V2 be a partition of the vertices of G into two disjoint non-empty sets\n\n\nSuppose that some minimum spanning tree T that containing e is better than all other trees\n\n\nThen can add edge e to T and remove some other edge between V1 and V2 and obtain a better MST\n\n\nThe algorithm adds a minimum weight edge between V1 and V2,\nand so this edge must be part of some MST\n\n\nHence, the construction cannot make a “fatal mistake” - at no point can it add an edge not part of an MST\n\n\n Binary Search Trees: Balance and Rotations\n      \n\n\nBoth trees are valid BSTs with the same content\n\n\nBoth have in-order traversal: 1,2,3,4,5. But …\n\nThe left has height 4\nThe right has height 2\n\n\n\nThis matters because the vital BST algorithms are O(height)O( height )O(height)\n\n\nA tree is said to be “balanced” if the heights of left and right subtrees of any node are (close to) equal, and so the height is O(log n)O(log \\,n)O(logn)\n\n\n The BST Imbalance problem\n\nAIM: Do “small local rebuilds” during insert/delete operations, to maintain the balance, and so retain the O(log n)O( log \\,n )O(logn) cost\n\n Example of a “rotation”\n\n\n\nBoth trees have in-order traversal: In(T1), a, In(T2), b, In(T3)\n\n\nDepths:\n\n‘a’ and T1 sink down,\n‘b’ and T3 rise up\n\n\n\na single rotation only has O(1)O(1)O(1) in cost\n\n\n Simple Example\n\n\n“Raising b” was the good choice because it is the median value and so should be the root\n\n Specific Example: Double works\n\n\n\nRotation on the edge b-c\n\n\nThen can rotate on edge a-b\n\n\nThe median node is ‘b’, needs to drop from 2 to 0\n\nEach rotation can only move ‘b’ up by 1, hence need two rotations\n\n\n\n Summary\nThe goal is then to control the usage of rotations to reduce the overall height of the tree\nAdvanced:\n\nAVL trees\n2-4 trees &amp; red-black trees\n\nThey guarantee O(log n)O( log \\,n )O(logn) – unlike Hash maps\n Shortest Paths: Floyd-Warshall (FW)\n Floyd-Warshall: All-Pairs Shortest Paths\nSuppose that wanted to find the shortest path between all pairs of start and end nodes\n Basic method\n\nBuild the optimal answers using a subset of the nodes.\nThen add the effects of other nodes one at a time\n\n Data structure\n\n\nd(i,j,k)d(i,j,k)d(i,j,k) = shortest distance between nodes iii and jjj,\nbut using only the nodes 1,…,k{1,…,k}1,…,k as potential allowed intermediary points\n\n\nd(2,5,3)d(2,5,3)d(2,5,3) = shortest distance from n2n_2n2​ to n5n_5n5​ using only {n1,n2,n3}\\{n_1,n_2,n_3\\}{n1​,n2​,n3​} as potential intermediate points\n\n\nwe will assume that d(i,i,k)=0d( i, i, k ) = 0d(i,i,k)=0 for all iii\n\n\n Initialisation\n\n\nd(i,j,0)d(i,j,0)d(i,j,0) = best distance between nodes iii and jjj, but not using any intermediate nodes,\n\n\nd(i,j,0)=w(i,j)   if there is an edge i to jd(i,j,0) = w(i,j) \\;\\text{ if there is an edge i to j}d(i,j,0)=w(i,j) if there is an edge i to j\n=∞               otherwise= \\infin \\;\\;\\;\\;\\;\\;\\;\\text{ otherwise}=∞ otherwise\n\n\n FW equations\n\n\nNow suppose that we add the node ‘n1n_1n1​’ to the set of nodes that can be intermediates,\ni.e. consider k = 1\n\n\nBest path is now the best of “either direct, or via n1n_1n1​.”\n\n\nd(i,j,1)=min( d(i,j,0),  d(i,n1,0)+d(n1,j,0))d(i,j,1) = min (\\, d(i,j,0), \\;d(i,n_1,0) + d(n_1,j,0) )d(i,j,1)=min(d(i,j,0),d(i,n1​,0)+d(n1​,j,0))\n\n\n\n\n\nNow suppose that we add the new node “(k+1)” to the set of “via nodes” that can be intermediates, but have already considered k of them\n\n\nBest path is now either direct using only the k ‘via nodes’ already accounted for,\nor else also via node ‘k+1’ (and using the previous k via’s)\n\n\nd(i,j,k+1)=min( d(i,j,k),  d(i,k+1,k)+d(k+1,j,k))d(i,j,k+1) = min ( \\,d(i,j,k), \\;d(i,k+1,k) + d(k+1,j,k) )d(i,j,k+1)=min(d(i,j,k),d(i,k+1,k)+d(k+1,j,k))\n\n\n\n\n FW code &amp; complexity\nThe main loop after initialisation is:\n1234foreach k = 1,…,|V| // size of V\tforeach i ∈ V\t\tforeach j ∈ V\t\t\td(i,j,k+1) = min( d(i,j,k), d(i,k+1,k) + d(k+1,j,k) );\n\n\nHave 3 nested loops, of ranges |V|. Hence is O(∣V∣3)O( |V|^3 )O(∣V∣3)\n\n\nIf the graph is sparse, then ∣E∣&lt;&lt;∣V∣2|E| &lt;&lt; |V|^2∣E∣&lt;&lt;∣V∣2, so then “all-starts Dijkstra” may be better.\n\n“&lt;&lt;” means “much less than”\n\n\n\n FW on digraphs\nFW also works the same on directed graphs\n\nThe initial matrix d(i,j,0)d(i,j,0)d(i,j,0) need not be symmetric, but then the remaining calculations use exactly the same formulas\n\n FW with negative edges\nFW even works if some (directed) edge weights are negative\n\nBUT it is essential that there are no cycles of total negative weight\nOtherwise simply repeatedly following around the negative cycle may reduce lengths to be as negative as desired, so there is no shortest path\n\n Key Idea\nUses that shortest path does satisfy a nice decomposition of\n\n\n\n\n\n\n\n\n\nIf P(A,B)P(A,B)P(A,B) is a shortest path, and goes via MMM, then P(A,M)P(A,M)P(A,M) is optimal for AAA to MMM and P(M,B)P(M, B)P(M,B) is optimal for MMM to BBB\n\nHence uses a version of “dynamic programming”\n\n","slug":"ade","date":"2024-06-09T15:34:56.000Z","categories_index":"Notes","tags_index":"Algorithms,Data structure,Efficiency","author_index":"Huaji1hao"},{"id":"c69b9ac1223f91f4483d13b90656a509","title":"IELTS Writing Task 2","content":" Writing Task 2\n Understanding and Analysis\n The 5 types of questions:\n\nOpinion (Agree or Disagree)\nDiscussion (Discuss both view)\nAdvantages and Disadvantages\nProblem/Causes and Solution\nDouble Question\n\n Question Analysis\nWhen analyzing a question we have to think about 3 things:\n\nTopic\nKeywords\nInstruction words\n\n\n\n\n\n\n\n\n\n\nSome experts believe that it is better for children to begin a foreign language at primary school  rather than secondary school.\nDo the advantages of this outweigh the disadvantages?\n Think like an examiner!\n Task Response Dos\n\n\nAnswer the specific question being asked, Not the general topic\n\n\nMake sure your ideas are relevant\n\n\nFully address each part of the question\n\n\nState your opinion in introduction and use the supporting paragraphs to support this opinion\n\n\nReiterate your opinion in the conclusion\n\n\n Task Response Don’ts\n\nSpend lots of time on just one part of the question\nGive very general examples\nLeave opinion until the last sentence\nRepeat the same points over and over\nWrite under 250 words\n\n Coherence and Cohesion Dos\n\nUse four paragraph structure\nOutline your main ideas and opinion in the introduction\nHave clear topic sentences in your supporting paragraphs\nSkip a line between paragraphs\nUse cohesive devices appropriately and accurately\n\n Coherence and Cohesion Don’ts\n\nInclude background statement in the introduction\nHave lots of ideas in one paragraph\nUse cohesive devices at the start of every sentence\nUse cohesive devices inaccurately\n\n Vocabulary Dos\n\nBe careful with spelling ad grammar\nBe aware of collocations (match the words)\nUse ‘less-common’ words (specific description)\nUse topic specific words\nFollow the 100% rule\n\n Vocabulary Don’ts\n\nRepeat the same words again and again\nForce complex words into your essay without knowing them 100%\nUse synonyms that are wrong\nLearn lists of ‘academic’ words out of context\n\n Grammar Dos\n\nTry to write as many error-free sentences as possible\nUse ‘complex’ sentences (two classes rather than single)\nUse a variety of structures\nCheck work when writing and at the end\nFollow the 100% rule\n\n Grammar Don’ts\n\nTry to use as many different structures as possible\nTry to impress the examiner with complex grammar\nWrite sentences that stop meaning being conveyed\n\nCLEARITY IS KING\n Planning\n Generate ideas\n 6 Questions\n\nWho? What? Why? Where? How? When?\n\n Bonus Idea Generation\n\nIf you asked 100 people, what would be the common answer?\nIf you were trying to win an argument, what idea would you use?\n\n Structure Planning\n Introduction\n\n\nParaphrase Question\n\n\nOutline\n\n\n Supporting Paragraph 1\n\nMain point\nExplanation\nExample\n\n Supporting Paragraph 2\n\nMain point\nExplanation\nExample\n\n Conclusion\n\nSummarize main points\n\n Pre-think Vocabulary (Synonyms)\n\nStudents = graduates = undergraduates\nUniversity = College = third-level education = tertiary education\nConsume = use\nPeople = human\nTeenagers = adolescents\nGovernments = States\nAdvantages = benefits\n\n Timing\n\n40 minutes to complete task2\n10 minutes planning\nMost essays are around 12 sentences long\n2 minutes per sentence = 24 minutes\n6 minutes reviewing and checking our work\n\n Introduction\n Common Mistake\n\nWriting long general background statements or hooks\nNo opinion or outline of main ideas\n\n Combining opinion and outline\nThe continued rise in the world’s population is the greatest threat faced by humanity at the present time.\nDo you agree?\n\nrun out of resources\ndamage to the planet\n\n\n\n\n\n\n\n\n\n\nIncreasing overpopulation is the biggest thread human beings face today. This essay totally agrees with this statement because this will lead to a serious depletion of resources and pollution\n Examples\n Opinion\nIn some countries an increasing number of people are suffering from health problems as a result of\neating too much fast food. It is therefore necessary for governments to impose a higher tax on this kind of food.\nDo you agree?\n\n\n\n\n\n\n\n\n\nIt is argued that states should charge fast food companies more tax because of the growing amount of men and women with health conditions associated with this type of food. This essay totally agrees with that statement because these illnesses cost the health service too much money and increasing the price of junk food would reduce the demand for it.\n Discussion\nSome people work for the same organisation all their working life.Others think that it is better to work for different organisations.\nDiscuss both views and give your own opinion.\n\n\n\n\n\n\n\n\n\nSome say that it is more beneficial to be employed with the same company all their lives, while others would argue that it is better to work for a variety of companies. This essay will argue that although working for just one employer gives you more financial and other benefits,working at lots of different places provides an employee with more experience.\n Advantages and Disadvantages\nOne of the consequences of improved medical care is that people are living longer and life expectancy is increasing.\nDo you think the advantages of this development outweigh the disadvantages?\n\n\n\n\n\n\n\n\n\nOne of the results of modern medicine is that men and women are able to live longer. This essay will argue that despite the strain this might cause on the pension system,the alleviation of suffering means that the advantages far outweigh the drawbacks.\n Problem and Solution\nSmartphones are becoming a common sight in the primary school classroom.\nWhat problem does this cause and what is a viable solution?\n\n\n\n\n\n\n\n\n\niPhones and other devices are being used more often by primary school children. This essay will suggest that they are very distracting and that the best solution is to ban them completely in class.\n Double Questions\nCar ownership has increased so rapidly over the past thirty years that many cities in the world are now ‘one big traffic jam’.\nHow true do you think this statement is?\nWhat can governments do to discourage people from using their cars?\n\n\n\n\n\n\n\n\n\nLots of city centers are highly congested due to an increase in the ownership of cars in the last three decades. This essay will argue that this is true only during peak times and that improving public transport will reduce this.\n Body paragraphs\n Common Mistake\n\nToo many ideas\nUndeveloped ideas\nNo/Poor explanations or examples\nFirstly, secondly, thirdly, finally\nPoor grammar and vocabulary\n\n Checklist\n\nRelevant ideas\nFully address all parts of the task\nClear position throughout\nFully extended and well supported ideas\n\n Structure - 3 Key Elements\n\nTopic Sentence\nExplanation Sentences\nExample\n\n\n\n\n\n\n\n\n\n\nIt is argued that states should charge fast food companies more tax because of the growing amount of men and women with health conditions associated with this type of food. This essay totally agrees with that statement because these illnesses cost the health service too much money and increasing the price of junk food would reduce the demand for it.\nObesity related illnesses cost the taxpayer billions of dollars every year. People who eat too much Food are more likely to suffer from costly diseases associated with being overweight. As a result, they have to go to hospital more regularly for treatment and this puts a strain on the health service. For example,a large proportion of the United Kingdom’s National Health Service budget is spent on preventable, obesity-related diseases, such as heart disease, hypertension and diabetes.\n Topic Sentence\n\nShort clear statement about what the paragraph is about\nDirectly answers the question\nNot much detail.\nMakes your main ideas clear\nCan’t write one unless you have clear ideas already\n\n\n\n\n\n\n\n\n\n\nIntroduction:\nincreasing the price of junk food would reduce the demand for it.\n↓\nTopic Sentence:\nAnother reason why fast food restaurants should pay extra tax is to raise the cost in order to decrease demand.\n Explanation\n\n\nPretend that you are writing to someone with on knowledge of the subject\n\n\nClear explain\n\n\nwhat you topic sentence means\n\n\nhow it answers the question\n\n\nwhat the result is\n\n\n\n\nShould be around 2-4 sentences\n\n\nUseful language\n\nThat is to say… = In other words…\nThis is because… = The reason is…\nAs a result… = Therefore…\n\n\n\n\n\n\n\n\n\n\n\n\nAnother reason why fast food restaurants should pay extra tax is to raise the cost in order to decrease demand. Any extra tax would be added to the normal price of the food, and as prices go up fewer people will be able to afford to buy fatty food. Therefore, this would reduce the amount of junk food people eat and the result would be a healthier nation.\n Examples\n Better Examples\n\nReal examples\nConverting personal examples\n\n How to think of examples\n\n\nTake examples from your own life experience\n\n\nConvert them into more general examples\n\n\nThink about\n\nWhere are you form?\nWhere do you live\nWhat is  your job?\nWhat about your family and friends?\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal:\nFor example, I used to smoke 20 cigarettes everyday, but the government kept increasing the tax and now I can’t afford to smoke.\n↓\nGeneral:\nFor example, when the duty on cigarettes is raised each year in the UK, more people quit smoking and this has a knock on effect on the number of people dying from lung cancer and other smoking related diseases.\n\n\n\n\n\n\n\n\n\nYour own personal experience:\nMy sister’s bookshop closed down because it couldn’t compete with Amazon.\n↓\nGeneral:\nFor instance, many bookshops in the UK closed down because of competition from online.\n\n\n\n\n\n\n\n\n\nYour own personal experience:\nI can’t afford to buy a laptop for my child.\n↓\nGeneral:\nFor example, in Ireland laptops are very expensive which would prevent most parents from being able to purchase them for their children.\n\n\n\n\n\n\n\n\n\nState something and add:\nas reported by many media outlets.\n Conclusion\n Big Mistakes\n\n\nNew ideas\n\n\nTrying to be entertaining\n\n\nBeing too vague\n\n\nRepeating exactly the same thing as in the rest of your essay\n\n\nUsing the wrong cohesive devices\n\n\n Why are conclusions important?\n\nIt is the last thing the examiner reads\nIt shows the examiner that you can summarise\nOpinion is clear the whole way through\nMakes essay cohesive and coherent\n\n Appropriate Cohesive Devices\n\nIn conclusion,\nTo conclude,\n\n Structure\n\n\nTwo essential things are\n\nSummary of main points\nOpinion\n\n\n\nOne sentences\n\n\n Example\n\n\n\n\n\n\n\n\n\nQuestion:\nThe continued rise in the world’s population is the greatest threat faced by humanity at the present time. Do you agree?\nIntroduction:\nIncreasing overpopulation is the biggest threat human beings face today. This essay totally agrees with this statement because this will lead to a serious depletion of resources and pollution.\nConclusion:\nIn conclusion, there are too many people in the world and this is a huge threat to everyone because\nessential resources are running out and we are also polluting our planet.\n\n\n\n\n\n\n\n\n\nIntroduction:\niPhones and other devices are being used more often by primary school children. This essay will\nsuggest that they are very distracting and that the best solution is to ban them completely in class.\nConclusion:\nIn conclusion, mobiles are not a good idea for young children because they interfere with the learning process and this should be stopped by telling parents that they are not allowed in schools.\n Review\n\n\nGrammar\n\nPrepositions\nArticles\nVerb-subject agreement\nCountable/Uncountable nouns\nTenses\nCapital letters\n\n\n\nVocabulary\nFirst you should scan for any words repeated that you can easily change using synonyms.\nFocus on\n\nCollocations\nMeaning\nSpelling\nWord form\n\n\n\nChecklist\n\nDoes the essay answer all parts of the question?\nIs the opinion clear in the introduction?\nDoes each supporting paragraph have a clear topic sentence?\nAre ideas fully developed with explanations and examples?\nIs your opinion clear throughout the whole essay?\nIs there a suitable conclusion?\nOver 250 words?\nRange of complex and simple sentences.\nAnything confusing or unclear?\n\n\n\n 1. Opinion\n Question\n\nDo you agree or disagree?\nTo what extent do you agree or disagree\n\nalways state ‘strongly agree’ or ‘strongly disagree’\n\n\n\n Alternative wording of opinion questions\n\nIs this a positive or negative development?\nTo what extent is this a positive or negative development\n\n Common Mistakes\n\nNot giving your opinion\nLeaving opinion until the conclusion\nDiscussing someone else’s opinion\nDiscussing both sides of the argument\n\n Band 9 Checklist\n\nGive a clear opinion in the introduction\nThink of two main points supporting your opinion\nDevelop these main points with explanations and examples\nShort conclusion summarizing your main points and reiterating your opinion\n\n Deciding Opinion\n\nWastes time\nPersonal opinion does not matter\nPick one side only\nPick the side  you can easily write about\n\n Language\nGood in introduction:\n\nThis essay completely agrees that…\nThis essay totally disagrees that…\nIn my opinion,\nI totally agree/disagree\n\n Structure\nParagraph Introduction\n\nParaphrase Question\nGive opinion and outline main points\n\nSupporting Paragraph I\n\nTopic Sentence\nExplanations\nExample\n\nSupporting Paragraph II\n\nTopic Sentence\nExplanations\nExample\n\nConclusion\n\nSummary\n\n Planning- Idea Generation\n\n\n\n\n\n\n\n\n\nIn many countries it is now illegal to advertise alcohol. Do you agree or disagree?\nThoughts:\n\n\nWhy do I agree? (Coffee shop method)\n\n\nAlcohol is a problem both socially and for our health\n\n\nTaking advantage of alcoholics\n\n\nAdvertisers can influence people to drink alcohol\n\n\nYoung people are exposed to persuasive ads\n\n\nIntroduction\n\nParaphrase Question\nAgree and outline main points\n\nSupporting Paragraph 1\n\nTopic Sentence - health and social problems\nExplanations - addiction and bad behaviour\nExample - Russia\n\nSupporting Paragraph 2\n\nTopic Sentence - influence\nExplanations - glamorise and young people\nExample - Specific advert\n\nConclusion\n\nSummary\n\nVocabulary:\n\nIllegal-banned, ban, prohibited, not allowed.\nAdvertise-TV, radio, print, ads, commercials, promoting.\nAlcohol-drinking, alcoholic drinks/beverages,beer, wine and spirits.\nAddiction-alcoholic, addicted to alcohol, alcohol dependence, dependent on alcohol.\nBad behavior -anti-social behavior, violence,vandalism.\nInfluence-sway,persuade,convince.\n\n\n\n\n\n\n\n\n\n\nMore and more people are now working remotely from home. Is this a positive or negative development?\nIntroduction\n\nParaphrase Question\nPositive and outline main points\n\nSupporting Paragraph 1\n\nTopic Sentence - less stress\nExplanations - no commute in heavy traffic / timeable adaptable to personal needs\nExample - workers can make timetable to incorporate regular breaks in order to relax\n\nSupporting Paragraph 2\n\nTopic Sentence - less environmental impact\nExplanations - lower carbon footprint due to less travel and energy to power office buildings\nExample - Pandemic meant working from home, which meant lower levels of pollution\n\nConclusion\n\nSummary\n\n Review\n\nNow don’t forget to review your essay\nMake sure that:\n\nGive a clear opinion in the introduction\nThink of two main points supporting your opinion\nDevelop these main points with explanations and examples\nShort conclusion summarizing your main points and reiterating your opinion\n\n\n\n Template\n\nsentences\n\nIn the contemporary era, discussions regarding xxx have consistently garnered public interest. Some advocate for\nthe idea that xxx, a position I find myself inclined to support.\n\n\n\n\nAgree or disagree\nIt is important for everyone, including young people, to save money for their future. To what extent do you agree or disagree with this statement?\nIt is often said that saving money for the future is important for everyone, including young people. This essay fully agrees with this view because reducing unnecessary expenses enables people to purchase higher-quality items and helps them improve their quality of life after retirement.\nBy spending less money on unnecessary items, people can save more and afford higher-quality products. Young people, in particular, may often be tempted by attractive items in stores, such as decorative crafts or small models, which serve little purpose beyond taking up space. Instead of falling into these consumption traps, it is wiser to invest in durable, high-quality products that can have a positive impact on daily life. Many experts suggest that choosing quality over quantity can lead to long-term savings and greater satisfaction.\nAfter retirement, those who have saved enough money are more likely to enjoy a comfortable life. Although many elderly people receive government pensions, these payments are often insufficient to maintain their previous lifestyle. With extra savings, individuals can earn interest by keeping their money in the bank, which reduces financial pressure. This allows them to spend more on essential areas like healthcare and nutrition, helping them live longer and healthier lives. Additionally, they may also feel more confident about supporting their children financially.\nIn conclusion, saving money is essential for everyone, as it allows them to make wiser purchases by investing in higher-quality items. Moreover, it ensures a more comfortable life in retirement, where individuals can spend more on healthcare and other important aspects, reducing the financial burden as they age.\n\nThe working week should be shorter and workers should have a longer weekend. Do you agree or disagree?\nIt is often argued that workers should have shorter working weeks and longer weekends. This essay fully agrees with this view because it allows employees to better balance their work and personal lives, increases their efficiency, and helps companies save money while becoming more attractive to potential employees.\nHaving more time on weekends can help workers achieve a better work-life balance. With additional free time, they can take care of their children, spend quality time with family, and pursue hobbies such as sports or music. This not only benefits individuals but also makes companies more attractive to talented employees. For example, Google promotes a strong work-life balance, and as a result, many young professionals prefer working there over companies that offer higher salaries but demand more working hours.\nWith fewer working hours, employees are likely to work more efficiently and smartly. After having more rest, their brains are refreshed, allowing them to focus better and complete tasks in less time. Consequently, the company’s overall efficiency may improve by providing employees with more relaxation. Furthermore, companies can reduce operational costs, such as electricity and air conditioning, by operating fewer hours each week.\nIn conclusion, reducing the working week is beneficial not only for employees’ personal lives but also for companies. It can increase efficiency, attract talented workers, and reduce operational costs, making it a positive change for both individuals and businesses.\n\nThe most important aim of science should be to improve people’s lives.\nTo what extent do you agree or disagree with this statement? Give reasons for your answer and include any relevant examples from your own knowledge or experience.\nIt is often argued that the primary role of science is to enhance human lives. This essay fully agrees with this view, as scientific advancements have significantly improved healthcare and greatly enhanced the quality of life.\nThere is no doubt that medical and healthcare systems have advanced significantly over time, benefiting nearly everyone. In the past, many deadly diseases, spread by animals like mice and insects, led to the deaths of thousands. However, thanks to the diligent efforts of scientists, many of these bacterial and viral illnesses have been eradicated through vaccines developed from organic materials. These scientific breakthroughs have saved countless lives.\nScientific advancements have also played a vital role in improving the quality of life. The first and second industrial revolutions introduced humans to the power of fire and electricity, now essential domestic resources that have made life more convenient. For instance, while people once had to burn wood for heat, today we can simply turn a knob to enjoy hot water. Additionally, in the past, ice was a luxury only the wealthy could afford during summer, but now nearly every household owns a refrigerator, making ice readily available. Without scientific development, modern life would not be as comfortable as it is today.\nIn conclusion, the ultimate goal of science is to improve people’s lives, whether through advancements in healthcare or living conditions. This is a meaningful endeavor that benefits humanity as a whole.\n\nSome people believe that zoos where animals are kept in a man-made environment should no longer exist in the 21st century. To what extent do you agree or disagree?\nMany people argue that zoos where creatures are contained in artificial enclosures should be banned in this century. This essay totally agrees with this statement because it is cruel to keep an animal in a confined space, and nature programs allow us to view animals without the need of a zoo.\nThe first reason that animals should not be kept in artificial enclosures is that it is inherently cruel. That is to say that as human beings, we are intelligent enough to know that the animal is suffering and if we decide to do it anyway, we are causing unneeded pain and discomfort. For example, Sea World in California recently admitted that their whales were depressed and not in optimal health because they cannot swim in the ocean, but decided to keep them in the park because they were a big attraction.\nThe second main reason is that high quality documentaries remove our need to see animals in captivity. This is because TVs have such high quality screens and programmers are now able to offer us a very intimate glimpse of animals in their natural habitat. Thus, we can get a better insight into the lives of wild animals by staying at home. For instance, the BBC recently produced a series called Life in the Undergrowth which allowed us to understand the lives of insects in stunning high definition.\nIn conclusion, wild animals should all be allowed to live freely because it is evil to trap and force them to live in an animal park, and television shows about animals allow us to learn about them without going to see them in person.\n\nIn many countries it is now illegal to advertise alcohol. Do you agree or disagree?\nPlan:\nMain idea-drinking can lead to serious health problems\nWhy/How-drinking too much can cause liver damage. People often get violent and this can lead to assaults and vandalism.\nMain idea-influences people to drink more.\nWhy/How-advertisers main goal is to sell. Adverts glamorise drinking.\nExample-Hennessy adverts.\nOpinion-Agree that it should be made illegal.\nMain points-harmful to health and influences people unfairly.\nEssay:\nAdverts for beer, wine and spirits are now banned in many nations. This essay agrees with this idea because drinking can lead to serious health problems and advertisers can often influence people to drink more than they should.\nAdvertising alcoholic drinks should be banned because alcohol is harmful to a person’s health. Drinking too much causes several conditions including liver cancer,diabetes and brain damage. This puts a huge strain on the health service and not only causes suffering to the patients but also to their families. For example, Russia had a huge problem with alcoholism and many men were dying from alcohol-related diseases, but after they decided to stop companies promoting alcoholic beverages on TV and in print ads, it resulted in a huge reduction of such cases.\nAnother reason why the promotion of alcohol should be prohibited is the fact that it influences people unfairly to buy that product. The main goal of any advertising campaign is to sell more and this could be seen as irresponsible considering the harm drinking can do. Commercials often make certain drink brands look cool or sexy, in order to sway potential customers,and don’t actually talk about the dangers. For example, Hennessy adverts often portray attractive men and women in a luxurious setting, drinking cognac with a beautiful woman.\nIn conclusion, alcohol is a hazardous product and advertisers should not be allowed to try and convince people to drink it and I, therefore, believe that more countries should ban the advertising of liquor.\n\n\n\nAdvantages and disadvantages\nMany teenagers have their own smartphones.\nDoes this situation have more advantages or more disadvantages?\nMany adolescents own a smart device. This essay argues that despite the main advantage of this being that they have the ability to stay in contact with their parents, I believe that there are more disadvantages as this creates a lack of face-to-face communication, which has many detrimental effects.\nThe biggest benefit of teenagers having mobile phones is the fact that their parents can call them and even check on their location. This is because smartphones have GPS, and many apps allow parents to accurately determine where their child is or simply message them to make sure they are safe. For example, messaging apps like Viber have a location setting so that parents can ask where their children are in a message and then check on Google Maps when their loved ones respond. However, I think that the lack of physical contact caused by mobile phone usage\nis a more serious concern.\nSome believe that the drawback to teens having smartphones is that they reduce the amount of time they spend talking face to face. Most adolescents spend too much time on their phones. Face-to-face communication allows children to develop social skills and grow as human beings, and they may become anti-social adults in the future if they stare at a screen all day. For instance, it is very common to go into any coffee shop and see large groups of young people sitting around and not talking for several minutes because they are so engrossed in their smartphones. Therefore, I believe that smartphones, overall, have a detrimental effect on adolescents.\nIn conclusion, the primary value of teenagers having smartphones is the fact that their parents can easily stay in touch with them; however, I feel that the lack of in-person interaction is a more significant downside.\n\nIt is important for people to take risks, both in their professional lives and their personal lives.\nDo you think the advantages of taking risks outweigh the disadvantages?\nGive reasons for your answer and include any relevant examples from your own knowledge or experience.\n(这篇内容写错了，应该两边都要讨论，然后说哪个好)\nMany people are willing to take risks not only in their personal lives but also in their professional careers. This essay completely agrees with this view, as without risks, our lives would be relatively monotonous, and taking risks can lead to more business or career opportunities.\nPeople generally find a life without risks dull and unfulfilling. If nothing changes and we simply go through daily routines—such as going to school or work in the morning and returning home in the evening—there would be little room for growth or progress. However, by stepping out of our comfort zones and trying new experiences, like traveling or climbing a mountain to appreciate the view, life becomes more exciting and meaningful. For instance, the Japanese culture of lifelong employment in one company discourages people from changing their lifestyles, and this rigidity has contributed to a higher suicide rate compared to other countries.\nTaking risks in the business world often leads to more opportunities and greater rewards. In the highly competitive market, companies that refuse to innovate risk being left behind by their competitors. However, taking calculated risks, such as developing new products or changing marketing strategies, may require effort but can result in significant success. For instance, Elon Musk, a well-known entrepreneur, consistently takes risks by investing in seemingly unprofitable technology ventures, many of which have since become highly successful.\nIn conclusion, embracing risks is important because it brings excitement and growth to our personal lives, and in business, it often leads to more opportunities for success. For these reasons, the advantages of taking risks outweigh the disadvantages.\n\n\n\nOpinion\nSome university students want to learn about other subjects in addition to their main subjects. Others believe it is more important to give all their time and attention to studying for a qualification.\nDiscuss both these views and give your own opinion.\nIt is argued that university students are interested in learning subjects outside their main courses, while others believe that focusing entirely on obtaining a qualification is more important. This essay will argue that although learning additional subjects may be interesting, obtaining a qualification is ultimately more beneficial for students’ future careers.\nSome university students choose to learn additional subjects because they find them fun, interesting, and helpful for socializing. For example, universities often offer extracurricular activities such as basketball, music, or climbing, where students can bond with peers and enjoy their time. However, some students may devote too much time to these activities, which could negatively impact their academic performance, as reported by various media outlets.\nOn the other hand, some students focus on obtaining professional qualifications during their university years, which can make their career path smoother. They spend time studying for qualifications in fields such as law, medicine, or sports. After graduation, those who possess the necessary qualifications will have a significant advantage over those who do not. This is why many Asian families encourage their children to prioritize studying for professional exams over participating in social activities. Although this may put pressure on students and affect their mental health, it ultimately helps them integrate more easily into the workforce.\nIn conclusion, while participating in extracurricular activities may be more appealing to students, obtaining qualifications is crucial as it provides them with better job opportunities and a smoother transition into the workforce.\n\nSome people think that a sense of competition in children should be encouraged. Others believe that children who are taught to cooperate rather than compete become more useful adults.\nDiscuss both views and give your own opinion.\nSome people believe that competition in our careers, school, and daily life is beneficial, while others argue that cooperation is more important. This essay will argue that although competition can make individuals stronger in skills, teamwork creates greater value and can achieve more than what individuals can accomplish on their own.\nCompetition between individuals can undoubtedly make them stronger. When competition is encouraged, people are more motivated to succeed. Winning releases chemicals in the brain, providing a sense of reward and accomplishment. At the same time, those who lose are often driven to improve their skills to succeed in future competitions. For example, many high school students enjoy competing in one-on-one basketball games. Although some may lose initially, they often improve their basketball skills in subsequent games.\nHowever, cooperation has been a key theme throughout human history. A team is always stronger than an individual because each member contributes their unique skills, making the project more efficient and manageable. In a well-organized team, individuals focus on their specific tasks, allowing the workflow to move smoothly rather than placing all the responsibility on one person. For example, most software companies have clearly structured employee levels and delegate tasks to the most qualified individuals, which has led to the development of successful software like Google Search.\nIn conclusion, while competition can enhance individual skills, cooperation is more valuable as it allows people to work together efficiently and achieve greater outcomes.\nIt is argued that children should be motivated to be competitive, while others feel that teaching them to be cooperative will be of more value as they enter adulthood. I believe that while competition can help children be successful, cooperation is more important because it teaches them to work within a team, a crucial adult skill.\nSome argue that instilling a sense of competition in children helps them to achieve success in whatever they do. This is because being competitive creates a drive to win, which teaches them that hard work and discipline are the keys to success. For example, it is often the case that children who participate in competitive sports are less likely to quit when things are difficult and are, therefore, more likely to overcome obstacles in their jobs as adults. Despite this, I would argue that children require lessons on teamwork more than the will to win.\nIf children are taught to be cooperative, they learn the importance of working in a team, which is something adults are expected to do. Through working with others, children learn not only how to respect different opinions but also how to pool their strengths. For instance, preschools include cooperation as one of the first skills in their curriculum as they recognise that it is a vital social skill in all spheres of life. I therefore believe that teaching children to work with others is more important than giving them a sense of competition.\nIn conclusion, while instilling a sense of competition in children can help them succeed, I think that teaching children to be cooperative gives them the ability to work as part of a team, which is far more valuable later in life.\n\n\n\n","slug":"Writing_Task_2","date":"2024-06-09T13:53:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"79276e65a56422ae2094bb3bd778a530","title":"Configure Namesilo domain for Github page","content":" use Namesilo domain for Github page / 使用 Namesilo 域名配置 Github Pages\n 1. buy a domain in namesilo\n 2. manage the domain DNS\n1234567891011121314151617TYPE A    HOSTNAME: leave blank here    Address(add four times):     185.199.108.153    185.199.109.153    185.199.110.153    185.199.111.153TYPE CNAME    HOSTNAME: www    Address: xxx.github.ioTYPE AAAA(optional, for ipv6)    HOSTNAME: retain empty    Address:     2606:50c0:8000::153    2606:50c0:8001::153    2606:50c0:8002::153    2606:50c0:8003::153\n 3. verify domain in user site\n\n\nprofile-&gt;settings-&gt;Code, planning, and automation-&gt;Pages-&gt;Add a domain\n\n\nenter the domain name\n\n\nopen Namesilo DNS management and add\n 123TYPE TXT\tHOSTNAME: _github-pages-challenge-....(your TXT record)\tAddress: asdd324fsdf(your code)\n\n\nVerify until success\n\n\n 4. add custom domain in repository\n\nxxx.github.io-&gt;settings-&gt;Pages-&gt;Custom domain\nadd your domain and save\nclick enforce HTTPS\n\n 使用 Namesilo 域名配置 Github Pages\n 1. 在 Namesilo 购买域名\n 2. 管理域名的 DNS\n在 Namesilo 的域名管理界面，添加以下 DNS 记录：\n1234567891011121314151617TYPE A    HOSTNAME: 这里留空    Address（添加四次不同的地址）:     185.199.108.153    185.199.109.153    185.199.110.153    185.199.111.153TYPE CNAME    HOSTNAME: www    Address: xxx.github.io（将 xxx 替换为你的 GitHub 用户名）TYPE AAAA（可选，用于 IPv6）    HOSTNAME: 保持空白    Address:     2606:50c0:8000::153    2606:50c0:8001::153    2606:50c0:8002::153    2606:50c0:8003::153\n 3. 在 GitHub 用户站点验证域名\n\n\n进入 GitHub 个人资料页，依次点击：Settings（设置） -&gt; Code, planning, and automation（代码、计划和自动化） -&gt; Pages（页面） -&gt; Add a domain（添加域名）\n\n\n输入你的域名\n\n\n打开 Namesilo 的 DNS 管理界面，添加以下记录：\n123TYPE TXT\tHOSTNAME: _github-pages-challenge-....(你的 TXT 记录)\tAddress: asdd324fsdf(你的代码)\n\n\n等待并验证，直到成功\n\n\n 4. 在 GitHub 仓库中添加自定义域名\n\n进入你的 GitHub Pages 仓库（例如 xxx.github.io），依次点击：Settings（设置） -&gt; Pages（页面） -&gt; Custom domain（自定义域名）\n添加你的域名并保存\n勾选 Enforce HTTPS（强制 HTTPS）\n\n这样，你就成功地将 Namesilo 域名配置到你的 GitHub Pages 上了。\n","slug":"Namesilo_GithubPage","date":"2024-06-04T10:49:36.000Z","categories_index":"Article","tags_index":"Domain,Github","author_index":"Huaji1hao"},{"id":"156564d90d95b8548776617619625b2a","title":"Something Interesting","content":" QAQ\n\n你真的会百度吗？\n你谷模板\n剪贴板迷宫\n2048\n2048朝代版\n给神犇推荐一个构图网站\n 万能头文件（手动滑稽）：\n1include &lt;bugs/stdc--.h&gt;\n\n 快读\n123456inline int read()&#123;    register int x=0,f=1;char c=getchar();    while(c&lt;&#x27;0&#x27;||c&gt;&#x27;9&#x27;)&#123;if(c==&#x27;-&#x27;)f=-1;c=getchar();&#125;    while(c&gt;=&#x27;0&#x27;&amp;&amp;c&lt;=&#x27;9&#x27;) x=(x&lt;&lt;3)+(x&lt;&lt;1)+(c^48),c=getchar();    return x*f;&#125;\n\n 快排\n12345678910111213141516void qsort(int a[], int l, int r)&#123;    // sort a[] in the range of [l, r].\tif(l == r) return;\tint i = l, j = r;\tint pivot = a[l + (r - l) / 2];\tdo&#123;\t\twhile(a[i] &lt; pivot) i++;\t    while(a[j] &gt; pivot) j--;\t    if(i &lt;= j)&#123;\t    \tint tmp = a[i];a[i] = a[j];a[j] = tmp;\t        i++;j--;\t\t&#125;\t&#125;while(i &lt;= j);\tif(l &lt; j) qsort(a, l, j);\tif(i &lt; r) qsort(a, i, r);&#125;\n\n 快速幂\n12345678910int quickPower(int a,int b)&#123;    // calculate a ^ b.\tint base=1;\twhile(b)&#123;\t\tif(b&amp;1)base*=a;//base%=mod;\t\ta*=a;//a%=mod;\t\tb&gt;&gt;=1;\t&#125;\treturn base;&#125;\n\n 秦九韶\n123456789double QinJiushao(double x)&#123;    // f(x) = a_1 * x^n + ... + a_n-1 * x^2 + a_n * x + a_n+1    double sum = indexNum[1];    for(int i = 1; i &lt;= n; ++i)&#123;        sum *= x;        sum += indexNum[i + 1];    &#125;    return sum;&#125;\n\n本人精通CSS、JavaScript、PHP、ASP、C、C＋＋、C#、Java、Ruby、Perl、Lisp、python、Objective-C、ActionScript、Pascal等单词的拼写\n熟悉Windows、Linux、Mac、Android、IOS、WP8等系统的开关机\n\n\n","slug":"Luogu_profile","date":"2024-06-01T10:49:36.000Z","categories_index":"Memes","tags_index":"Fun","author_index":"Huaji1hao"}]