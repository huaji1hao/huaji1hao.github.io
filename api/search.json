[{"id":"effcbc8f13e44d6613f1219a6caa5340","title":"Computer Security","content":" What is Security?\n\nSecurity is about the protection of assets\n\nPrevention: Preventing access and damage to assets\nDetection: Steps to detect the access or damage to assets\nRecovery: Measures allowing us to recover from asset damage\n\n\nAssets could be physical, or simply information\n\n Historic Computer Security\n\nHistorically systems have been built to serve single users\nOften only a few highly trusted users were permitted to access a system\n\nMistakes still a concern\n\n\nCurrent multi-user systems have totally different security concerns\n\n Modern Computer Security\n\nPossibly thousands of users\nDistributed over wide networks\nNot all users are inherently trustworthy\nMore and more things are moving electronic\n\nRequire protocols to manage them\n\n\n\n Computer Security\nUsually defined as three key areas (CIA):\n\nConfidentiality\n\nprevention of unauthorised disclosure of information\n\n\nIntegrity\n\nprevention of unauthorised modification of information\n\n\nAvailability\n\nprevention of unauthorised witholding of information or resources\n\n\n\n Confidentiality\n\n\nThe prevention of unauthorised users reading private or secret information\n\n\nPrivacy – The protection of personal data\n\nIn our context, privacy ≈ confidentiality\n\n\n\nExamples:\n\nMedical records\nTransfer of credit card details\n\n\n\n Integrity\n\nThe prevention of unauthorised modification of data, and the assurance that data remains unmodified\nExamples:\n\nDistributed bank transactions\nDatabase records\n\n\n\n Integrity vs Authenticity\n\nJust because we have integrity, doesn’t mean we have authenticity\n\nCan we verify the sender?\nDoes it have freshness?\n\n\nAuthenticity = Integrity + Freshness\nFreshness may seem trivial, but it’s pretty important e.g. in bank transactions!\n\n Availability\n\nThe property of being accessible an useable upon demand by an authorised entity\nIn other words, we want to prevent denial of service (DoS)\nExamples:\n\nRedundant power supplies\nFirewall packet filtering\nBackups\n\n\n\n Accountability\n\nUsers should be held responsible for their action\nThe system should identify and authenticate users and ensure compliance\nAudit trails must be kept\n\nThe university has a detailed audit-trail policy!\n\n\n\n Non-repudiation\n\nProvides un-forgeable evidence that someone did something\nMostly a legal concept\nEvidence verifiable by a trusted third party\n\nE.g. Notaries, Digital Certificates\n\n\nApplies to physical security: e.g. keycards\n\n A Fundamental Dilemma\n\n\n\n\n\n\n\n\n\n“Security-unaware users have specific security requirements but no security expertise”\nThere is a trade off between security and ease of use\n\nIncreased resource demands\nInterferes with working patterns\n\nOften user experience is placed at the forefront of software engineering\nThis is usually not compatible with security\n\n\n\n Data vs Information\n\nSecurity can be seen as controlling access to information\nThis is hard, we usually control access to data instead\n\nData – A means to represent information\nInformation – An interpretation of that data\n\n\nFocusing on data can still leave information vulnerable\n\nInference:\n“Xavier’s criminal record not found in the database.”\n“You do not have permission to access Xavier’s criminal record.”\n\n\n\n The Tradeoffs of Computer Security\n\nSome objectives are conflicting:\n\nSecurity vs. usability (~ availability)\nAccountability vs. privacy (e.g. audit trails)\nAvailability vs. authentication (e.g. account lockout)\nOverhead (computation, communication, storage)\n\n\n\n Security Design\n\nGood security design focuses on these principles:\n\nFocus of control\nComplexity vs. Assurance\nCentralised or Decentralised Controls\nLayered Security\n\n\n\n Focus of Control\nIn a given application, should the focus of protection mechanisms be:\n\nData\nOperations\nUsers\n\n Complexity vs. Assurance\n\nFeature-rich security systems and high assurance do not match easily\n\nE.g. Linux vs Windows permissions\n\n\n\n Decentralised Controls\n\nCentral Entity – A possible bottleneck\nDistributed Solution – More efficient, but harder to manage\n\n Layered Security\n\nWe can visualise our security model in layers\nEach layer protects a boundary, and relies on the security of the layers below\n\n\n What is Cryptography?\n\n Symmetric Cryptography\n\n\n\n\n\n\n\n\n\nBoth the encryption and decryption algorithms use the same key\n\nEncryption gives us confidentiality\nSymmetric encryption≈&quot;padlock&quot;\nSame key to encrypt and to decrypt\nUsed for general communication\nIt is lightweight and fast\n\nstreaming, disk encryption, browsing, etc.\n\n\nKey management difficult\n\nIn a network of n users, n(n-1)/2 keys\n\n\n\n\n\nImplemented using block ciphers or stream ciphers\nNo “proof” of security!\n\nBut based on established principles\nNot broken after many years and tremendous scrutiny\n\n\n\n Stream Ciphers\n\nStream ciphers use an initial seed key to generate an infinite keystream of random looking bits\nThe message and keystream are usually combined using XOR - ⊕ - which is reversible if applied twice\n\n\n Advantages of Stream Ciphers\n\nEncrypting long continuous streams, possibly of unknown length\nExtremely fast with a low memory footprint, ideal for low-power devices\nIf designed well, can seek to any location in the stream\n\n Disadvantages of Stream Ciphers\n\nThe keystream must appear statistically random\nSteam ciphers do not protect the ciphertext\n\n Block Ciphers\n\nBlock ciphers use a key to encrypt a fixed-size block of plaintext into a fixed-size block of ciphertext\n\nChanging and permuting the bits of the block depending on the key\n\n\nDifferent lengths of messages can be handled by splitting the message up, and padding\n\nModes of operation (e.g. CTR, OFB, GCM)\n\n\n\n Key Mixing\n\nMixing in the key prevents attackers from reversing the process\n\n\n Symmetric Algorithms\n\nDES was used from 1970s to 2000\n3DES (using DES 3 times) is sometimes used in legacy systems\nAES and ChaCha20 are the only two ciphers used in TLS 1.3\n\n Key Sizes\n\nSymmetric encryption (if secure) is only susceptible to brute force\n128 bits key means 2^128 required to brute-force\n256 is not “twice as difficult”!\nToday, yearly &quot;global computational power&quot;≈ 2^80 – 2^90\n\nStill very far from practical threat\n\n\n\n Asymmetric\n\n\n\n\n\n\n\n\n\nUses a pair of keys, one public and one private\n Public-key Cryptography\n\nAsymmetric encryption≈&quot;mailbox&quot;\nTwo keys: a private and a public one\nUsed key exchange and signatures\nIt is much more expensive\n\nbut not too bad\n\n\nKey management easier\n\nIn a network of n users, n keys (key pairs)\n\n\n\n\n\n\n\n\n\n\n\n\nIt is computationally infeasible to calculate a private from a public key\n\nIn practice this is achieved through reduction to intractable (NP) mathematical problems\n\n Asymmetric Algorithms\n\nDiffie-Hellman\nRSA\nElgamal\nDSA\n\n Protocols\n\n\n\n\n\n\n\n\n\nThe application of cryptographic algorithms in secure systems\n Transport Layer Security\n\nTLS is a handshake and record protocol that provides end-to-end encryption. It is used in HTTPS\nProvides:\n\nConfidentiality, Integrity, Server Authenticity\n\n\n\n\n\n\nSymmetric\nAsymmetric\n\n\n\n\nOne Key\nTwo Keys\n\n\nKeys are usually 128 or 256-bits\nKeys can be much longer, 2048 or 4096-bits\n\n\nUsually extremely fast\nComputationally slower\n\n\nLonger term communication\nKey exchange, verification and authentication only\n\n\nBased on circuits of permutation and substitution\nBased entirely on mathematical principles\n\n\n\n Big Numbers\n\nModular arithmetic and integer factorisation drive public-key cryptography\nOur examples are small, in reality numbers are hundreds of digits long\nAs computer power increases, we can increase the size of these numbers to preserve the integrity of our algorithms\n\n Authentication\n\nTo allow some access to an asset we must ensure:\n\nThey are permitted to access that asset\nThey are who they say they are\n\n\nA process of verifying a user’s identity\nWe can attempt to verify identity using credentials\n\nSomething the user is\nSomething the user has\nSomething the user knows\n\n\n\n Usernames and Passwords\n\nIdentification – Who you are\nAuthentication – Verify that identity\nAuthentication should expire\n\n“Remember my credentials” turns this into something you have\n\n\nTime of check to time of use – TOCTTOU\n\nRepeated authentication\nAt the start and during a session\n\n\n\n Hash Functions\n\n\nAnother cryptographic primitive in our toolbox\n\n\nTakes a message of any length, and returns a pseudorandom hash of fixed length\nh(M):{0,1}n→{0,1}128h(M): \\{0, 1\\}^n \\rightarrow \\{0, 1\\}^{128}h(M):{0,1}n→{0,1}128\n\n\nHash functions are used everywhere. Message authentication, integrity, passwords etc.\n\n\nCommon hash functions – MD5, SHA-1, SHA-256\n\n\n Strong Hash Functions\n\n\nThe output must be indistinguishable from random noise\n\n\nBit changes must be diffused through the entire output\n\n\nFor a hash function to be useful, we need it to have some important properties:\n\n\nGiven a hash, we can’t reverse it\nx→h(x),h(x)↛xx \\rightarrow h(x), \\\\h(x) \\not\\rightarrow xx→h(x),h(x)→x\n\n\nIt is impractical to find messages that produce the same hash – a hash collision\nx→h(x),x′→h(x′)x≠x′,h(x)≠h(x′)x \\rightarrow h(x), x&#x27; \\rightarrow h(x&#x27;) \\\\ x \\not=x&#x27;, h(x) \\not= h(x&#x27;)x→h(x),x′→h(x′)x=x′,h(x)=h(x′)\n\n\n\n\n Storing Passwords\n\n\nStoring passwords in plain text is a terrible idea\n\nYou might be hacked\nAdministrators can read them\n\n\n\nStoring encrypted passwords is better, but not perfect\n\nWhere are the keys stored?\nAdministrators can still read them\n\n\n\nUsing a one-way hash function is a much better solution\n\n\n Password / Shadow Files\n\nOperating systems have taken steps to stop people reading hashes for offline attacks\n\nLinux stores hashes in a shadow file /etc/shadow\nWindows stores this in …\\system32\\config\\SAM\n\n\nThese files are now read protected\nAdministrators or people booting another OS will often find a way in\n\n Cracking Passwords\n\n\nCracking a password isn’t always illegal, though obviously it sometimes is!\n\n\nPassword cracking falls into two basic types:\n\nOffline: You have a copy of the password hash locally\nOnline: You do not have the hash, and are instead attempting to gain access to an actual login terminal\n\n\n\nOnline is usually attempted with phishing\n\n\nOffline password cracking quite simply a case of trying possible passwords, and seeing if we have a hash collision with a password list\n\n\nMight be a brute force approach\n\nDifficulty is calculated as {char_count}length\\{char\\_count\\}^{length}{char_count}length\nGPUs are fast, but not fast enough for long passwords\n\n\n\n Dictionary Attacks\n\nMost password cracking is now achieved using dictionary attacks rather than brute force\n\nUsing a dictionary of common words and passwords\nApply small variations to this list, trying them all\nCombine words from two different lists\n\n\nqwerty1234password1 is unbreakable using brute force, but won’t last against a dictionary attack\n\n Password Salting\n\n\nWe can improve security by prepending a random “salt” to a password before hashing\n\n\nThe salt is stored unencrypted with the hash:\n\n\nIf we use a different random salt for each user, we get the following security benefits:\n\nCracking multiple passwords is slower – a hit is for a single user, not all users with that password\nPrevents rainbow table attacks – we can’t pre-compute that many password combinations\n\n\n\nSalting has no effect on the speed of cracking a single password – so make your passwords good!\n\n\n Hashing Speed\n\nWhen password cracking, the most important factor is hashing speed\nNewer algorithms take longer\n\nPartly because they’re more complex\nBut some have been specifically designed to take a while\n\n\nIterate to increase complexity - PBKDF2\nbcrypt can’t be used on easily GPUs\n\n If Cracking Fails: Pretexting\n\nObtaining private details by offering some “pretext” as a reason for needing them\nWe continue to rely on email addresses, DOB and Mother’s maiden names as our “last line of defense” for security\nHow much information do we need to ring up a company as someone else?\n\n Inherence Factors (Biometrics)\n\nBiometrics are unique identifiable biological characteristics\nInherent to individuals\n\n Biometrics Authentication Process\n\n Biometrics Tolerance and Errors\n\n Biometrics Security Considerations\n\nNo risk of losing access!\n\nCannot forget, lose, misplace, steal, etc.\n\n\nVery convenient (when it works well)\nAvailability of sensors? (cost, compliance, etc.)\nErrors: false positives (FAR), false negatives (FRR)\nCan never be changed!\nNot foolproof, e.g. adversarial ML(but difficult to attack in practice)\nEthical/privacy concerns\n\n Hardware Tokens Security Considerations\n\n\nRelies on strong keys and cryptography\n\n\nIn principle very secure\n\n\nOften as part of Multi-Factor Authentication\n\n\nPhysical loss, damage, or theft\n\n\nCost\n\n\nUsability\n\n\n Knowledge Factors\n\nPasswords are prone to be guessed, cracked, stolen, misused, etc.\nOften the weakest link\nStill not always well managed server-side!\nMuch effort to replace them, but still around!\n\nDifficult to beat the balance of knowledge factor, availability, portability, convenience, low cost, etc. despite flaws\n\n\nTargeted attacks versus wide net\n\nShould passwords resist both?\n\n\n\n Password Managers: the Good…\n\nSingle master password VS many passwords\n\nEntropy sufficient for individual “service” passwords (in principle)\nEntropy better since a single password is needed (in principle)\n\n\nConvenience!\n\nCentralized\nAutofill\n\n\n\n … and the Bad\n\nNot all free\nMaster password = single point of failure\n\nAttractive target!\nStill requires good password hygiene\n\n\nVaults still at risk\n\nLeaks\nCan you trust a single third-party with all your auth data?\n\n\nTrading some types of attacks\n\nService leaks/mismanagements\nLower entropy\n\n\n… against others\n\nVault leaks\nMalware\nBugs\n\n\n\n Multi-Factor Authentication\n\nMFA is the combination of several authentication methods\n\nIdeally, factors from different categories\n\n\nDisproportionately more difficult to compromise\nUsability, (in)convenience\n\n Authentication II -Conclusion\n\nThe range of authentication options has grown\n\nStill no single universally applicable effective solution!\n\n\nPasswords are familiar but not used effectively\n\n…but show no sign of going away quite yet\n\n\nBiometrics likely to become more commonplace\n\nNeed further research and development\n\n\nEvolving field! Stay informed and cautious of “magic” solutions\n\n Reference Monitor\n\n\n\n\n\n\n\n\n\nAn access control concept that refers to an abstract machine that mediates all access to objects by subjects\n\nMust be tamper proof / resistant\nMust always be invoked when access to an object is required\nMust be small enough to be verifiable / subject to analysis to ensure correctness\n\n Placement\nCan be placed anywhere within a system\n\nHardware – Dedicated registers for defining privileges\nOperating system kernel – E.g. Virtual Machine Hypervisor\nOperating system – Windows security reference monitor\nServices Layer – JVM, .NET\nApplication Layer – Firewalls\n\nReference monitors could be placed in a variety of locations relative to the program being run\n Lower Is Better\n\nUsing a reference monitor or other security features at a Usually, means:\n\nWe can assure a higher degree of security\nUsually, simple structures to implement\nReduced performance overheads\nFewer layer below attack possibilities\n\n\nHowever:\n\nAccess control decisions are far removed from applications\n\n\n\n OS Integrity\n\n\nThe operating system:\n\nArbitrates access requests\nIs itself a resource that must be accessed\n\n\n\nThis is a conflict, we want to use the OS but not mess with it\n\n“Users must not be able to modify the operating system”\n\n\n\nModes of operation\n\nDefines which actions are permitted in which mode, e.g. system calls, machine instructions, I/O\n\n\n\nControlled Invocation\n\nAllows us to execute privileged instructions safely, before returning to user code\n\n\n\n Modes of Operation\n\nDistinguish between computations done on behalf of:\n\nThe OS\nThe user\n\n\nA status flag within the CPU allows the OS to operate in different modes\n\n Controlled Invocation\n\nMany functions are held at kernel level, but are quite reasonably called from within user level code\n\nNetwork and File IO\nMemory allocation\nPrivileged instructions (e.g. HLT)\n\n\nWe need a mechanism to transfer between kernel mode (ring 0) and user mode (ring 3)\n\n Controlled Invocation: Interrupts\n\n\nExceptions / Interrupts / Traps\n\nCalled various things, for now we’ll just use “Interrupt”\nIn many ways is the hardware equivalent to a software exception – not always bad\n\n\n\nHandled by an interrupt handler which resolves the issue and returns to the original code\n\n\nGiven an interrupt, the CPU will switch execution to the location given in an interrupt descriptor table\n\n\n Descriptors and Selectors\n\nDescriptors hold information on crucial system objects like kernel structure locations\nDescriptors are held in descriptor tables\n\nContain a Descriptor Privilege Level (DPL)\n\n\nDescriptors are indexed by selectors\n\nLoaded when required, e.g. on jump calls\n\n\nThe CPU protects the kernel by checking the Current Privilege Level (CPL) when a Selector is loaded\n\n Interrupt-Gates\n\nThe code segment (CS) register in x86 CPUs has 2 bits reserved for the Current Privilege Level (CPL)\nDescriptors that have a privilege level higher than where they point are called gates\nSince these descriptors are created by the kernel, they offer a secure means of entry into ring 0\n\n Processes and Threads\n\nA process is a program being executed\nImportant unit of control:\n\nExists in its own address space\nCommunicates with other processes via the OS\nSeparation for security\n\n\nA Thread is a strand of execution within a process\n\nShare a common address space\n\n\n\n Memory Protection\n\nSegmentation – divides data into logical units\n\nGood for security\nChallenging memory management\nNot used much in modern OSs\n\n\nPaging – divides memory into pages of equal size\n\nEfficient memory management\nLess good for access control\nExtremely common in modern OSs\n\n\n\n Page Tables\n\nAll processes see an individual linear address space\nPage tables map from a linear address space to the physical address space\n\n Meltdown\n\nIn most operating systems, the entire kernel is stored in the upper address space\nPages in this area are flagged as supervisor, and cannot be accessed outside of ring 0\nMeltdown is an exploit that allows us to read this privileged memory\n\nWe do this using a side-channel\n\n\n\n Side-Channels and Cache Timing\n\nIn Intel (and some other) CPUs, it’s common to speculatively evaluate code prior to reaching it\n\nE.g. conditionals\nSignificant speed up\nNo harm done! Changes are rolled back\nBut, the cache isn’t rolled back\n\n\n\n Meltdown\n\nMeltdown attempts to read a value from kernel memory\n\nRead from kernel\nMask out a single bit\nAccess user memory at that location\n\n\n\n Spectre\n\nSpectre is very similar to meltdown\n\nSpeculative evaluation to side-step application bounds checks\nMask out a single bit\nAccess user memory at that location\n\n\n\n Linux\n\nCompactly combine:\n\nIdentification\nAuthentication\nAccess control\nAuditing\n\n\nUser accounts to store permissions\nInstallation and configuration\n\n Once We’ve Authenticated\n\nAuthentication lets us verify who we are to a system\nAssuming we’ve authenticated:\n\nSome files are private, some are public\nSystem files should be protected\nWe need to be able to access some applications\n\n\nWe need a mechanism to enforce access control\n\n Authentication &amp; Authorisation\n\nSubject / Principal – an active entity\nObject – resource being accessed\nAccess operation\nReference monitor – grants or denies access\n\n\n Principals vs. Subjects\n\nPrincipal\n“An entity that can be granted access to objects or can make statements affecting access control decisions”\n\nE.g user identity in an OS\nUsed when discussing security policies\n\n\nSubject\n“An active entity within an IT system”\n\nE.g. process running under a user identity\nUsed when discussing operational systems enforcing policies\n\n\n\n Objects\n\nObject – Files or resources\n\nE.g. Memory, printers, directories\n\n\nTwo options for focusing control:\n\nWhat a subject is allowed to do\nWhat may be done to an object\n\n\n\n General Model\nWe’ll settle on some common access on files:\n\nRead – Simply viewing (Confidentiality)\nWrite – Includes changing, appending, deleting (Integrity)\nExecute – Can run a file without knowing its contents\n\n Ownership\n\nWho is in charge of setting security policies?\nDiscretionary: Owner can be defined for each resource\n\nOwner controls who gets access\n\n\nMandatory: There could be a system-wide policy\nMost OS’s support the concept of ownership\n\n UNIX\n\nUnix simplifies access control by considering only the owner, group and others\n\nUser is the current owner\nGroup is a named group entity\nEveryone else\n\n\nUnix offers Read, Write and Execute access controls\n\n Groups\n\nUsers with similar access rights can be collected into groups\nGroups are given permissions to access objects\n\n UID / GID\n\nUsernames in Unix / Linux are soft aliases, your UID is what determines permissions\n\nUser identities: UID\nGroup identities: GID\n\n\nYour IDs are stored in /etc/passwd\nRoot has a special UID: 0\n\n /etc/passwd\n\n/etc/passwd stores user accounts, not just passwords\n\n The Shadow File\n\nIn an attempt to improve password security, we can store password hashes in a shadow file\n\nReadable only by root users\n\n\n/etc/shadow stores the hashed passwords needed to authenticate users\n\n Root Management\n\nWrite protect /etc/passwd and /etc/group – Obviously!\nSeparate superuser duties (e.g. daemon, uucp)\nNever use root as a normal user\nAudit su and sudo usage\n\n Objects\n\nIn Unix, everything is a file\nFiles really represent resources\nOrganised in a tree structure, with alterations depending on the file system\nInodes store permission information\nEvery resource has an owner and a group\n\n Inodes\n\nInodes in Unix and Linux store the metadata for files\nEach file name links to an inode, which stores security information\n\n Permissions\n\n\nEvery resource has permission bits - held in the inode metadata\n\n\nPermissions for the user / group / others\n\n\nOctal representation (bit-wise, really)\n\nBit 3: read (0x4, octal/decimal)\nBit 2: write (0x2, octal/decimal)\nBit 1: execute (0x1, octal/decimal)\n\n\n\nPermissions are changed using chmod, usually by passing three octal values\n\n\nGiven a directory with octal permissions 754, can others:\n\nList its contents? Yes\nExecute known files? No – no execute\nDelete files? No – no write\n\n\n\n Directories\nDirectory permissions are slightly different to files:\n\nr – List files within the directory\nw – add or remove files\nx – traverse directory, open files in the directory\n\n SUID\n\nSet UID: set the effective user to be the file owner when executed\nNecessary to allow non-privileged access to privileged actions e.g. passwords\nDangerous!\n\n Linux Security Modules\n\nSince 2.6, Linux provides the ability to hook into security calls\nThis adds the ability to perform more complex Mandatory Access Control after standard Unix DAC\nDAC(Discretionary Access Control) check happens irrespective of whether SM is operational\n\n\n SELinux\n\n\nBegan as a collaboration between the NSA and Redhat – Now entirely Redhat\n\n\nAllows mandatory (centralised) access control, role-based and multi-level security\n\n\nObjects and processes have contexts that allow SELinux to make access control decisions\n\n\nSELinux defines its own users, which are mapped to the Linux users\n\n\nMany users and user processes map to unconfined_u\n\nThis means SELinux doesn’t apply to them by default\nBut it can, you just need a committed administrator!\n\n\n\n AppArmor\n\nInstalled as standard on openSUSE and Ubuntu\n\nMuch easier to set up than SELinux\n\n\nAs with SELinux, is usually used to enforce policies on services and daemons\n\n Windows\n Security Subsystem\n\nRuns in user mode\nLogon processes (winlogon, LogonUI)\nLocal security authority (LSA)\n\nChecks User Accounts\nProvides access token\nResponsible for auditing\n\n\nSecurity Account manager (SAM)\n\nMaintains user account database used by LSA\nEncrypts / hashes passwords\n\n\n\n Windows\n\nWindows predominantly uses Access Control Lists, and has done since Windows NT\nExtends the usual read, write and execute with:\n\nTake ownership\nChange permissions\nDelete\n\n\n32-bit access masks (cf. Unix 9-bit)\nA higher degree of control, with the associated complexity increase!\n\n Access Control Matrix\n\n\nAccess rights are defined individually for each combination of subject and object\n\nQuite an abstract concept, but would allow for very fine grained control\nNot practical, think of the memory required in scaling it up!\n\n\n\nA list of capabilities defined per user, equivalent to a row in the ACM\n\n\n Capabilities\n\nA list of capabilities defined per user, equivalent to a row in the ACM\n\n Access Control List\n\nStored with an object itself, corresponding to a column of an ACM\n\n Access Control\n\nAccess control in windows treats more than just files, also:\n\nRegistry keys\nActive directory objects\nGroups\n\n\nInheritance is implemented\n\nFile can inherit ACLs from parent directories\n\n\n\n Principals\n\nPrincipals more broadly defined as well:\n\nLocal users\nDomain users\nGroups\nMachines\n\n\nEach principal has a human-readable name and security ID (SID)\n\n Local / Domain Principals\n\nLSA creates local principals\n\nprincipal = MACHINE\\principal\n\n\nDomain principals administered on DC by domain admins\n\nprincipal@domain = DOMAIN\\principal\n\n›net user /domain\n›net group /domain\n›net localgroup /domain\n\n\n\n\n\n Groups\n\nA group is a collection of SIDs\nGroup can itself be an SID\nGroups can thus be nested\nGroups are not nest-able on local machines\nManaged by a domain controller within Active Directory\n\n Objects\n\nObjects are passive entities in access operations\nIn Windows:\n\nExecutive objects (processes, threads, etc.)\nPrivate objects (files, directories)\n\n\nSecurable objects have a security descriptor\n\nBuilt-in securable objects managed by the OS\nPrivate objects managed by application software\n\n\n\n Access Tokens\n\nSecurity credentials for a login session stored in access token\nIdentifies the user, the user’s groups, and the user’s privileges\n\n Subjects\n\nWindows subjects: Processes and threads\nNew processes get a copy of the parent access token, possibly modified\nIndividual access tokens are immutable, and can live beyond policy changes (TOCTTOU issue)\n\n User Account Control\n\nAfter Vista, administrator users do not use an administrative access token by default\nUsers have two tokens, one heavily restricted and used by default\nA prompt allows a user to spawn a process with the other token, or switch a process’ token\n\n Domains\n\nSingle sign-on for network resources\nCentralised security administration\nDomain Controller (DC)\n\nHandles user accounts and access control\nTrusted 3rd party for authentication\n\n\nMultiple DCs allow for decentralisation by design\n\n Interactive Logon\n\n\nThe windows interactive logon allows a user to authenticate\n\n\nWindows logon begins with the Secure Attention Sequence – Ctrl + Alt + Delete\n\nCan prevent spoofing – is tied directly to winlogon\n\n\n\nThe logon process differs slightly for local and domain authentication\n\n\nThe logon process contains:\n\nWinlogon – the process responsible for authenticating users\nGraphical Identification and Authentication (GINA)\nThe Local Security Authority (LSA)\nAn authentication package (NTLM and Kerberos)\nSecurity Account Manager (SAM)\nSince Vista, additional Credential Providers are allowed\n\n\n\n Credential Providers\n\nSince Vista, winlogon uses a LogonUI to query Credential Providers\nThese don’t actually log you in, they simply serialize your credentials and pass them to the LSA\n\n Malware – Malicious Software\nA very general term, malware is usually categorised based on:\n\nHow it proliferates\nWhat it does\n\n Vectors\n\nVectors are the mechanism through which malware infects a machine\nUsually the vector will be a software vulnerability\nOr, someone clicked something they shouldn’t have!\n\n Payloads\n\nPayloads are the actual malware deposited on the machine, or the harmful results\nThey range in severity:\n\nEssentially do nothing\nMessages and adverts\nRecruited into a botnet or mail spam\nStealing private information\nSystem destruction\n\n\n\n Viruses\n\nA virus is a piece of self-replicating code\nPropagates by attaching itself to a disk, file or document\nWhen the file is run, the virus runs, and attempts to proliferate\nInstalls without the user’s knowledge or consent\n\n Worms\n\nViruses traditionally require a human to spread\nWorms are self-replicating and stand-alone programs\n\nDo not require human intervention\n\n\nScanning worms or email worms\nExploit known software vulnerabilities in order to spread\n\n MS Blaster\n\nSpread between machines on an internal network easily – no port filtering\nUsed a buffer overflow in a windows Remote Procedure Call (RPC) service – spreads without user clicking\nCompromised machines performed DDOS on windowsupdate.com\n\n Sasser\n\nSpread 17 days after a patch to the vulnerability was released by Microsoft\nBuffer overflow in the Local Security Authority Subsystem Service (LSASS)\nScans IP addresses and infects via port 445\n\n Exploit Life Cycle\n\nMany exploits are reverse engineered from patches, or developed simultaneously to patches\n\n\n Zero-Day Exploits\n\nAn exploit that is previously unknown – by far the most dangerous\n\n\n Stuxnet\n\nBelieved to be an American-Israeli cyber weapon:\n\nUses four zero-day flaws to infect Windows\nSeeks out any instance of Siemens Step7\nFinds programmable logic controllers (PLC)\nDetects attached centrifuges and spins them to destruction\nReports that the centrifuges are fine\n\n\n\n Trojans\n\nA malicious program pretending to be a legitimate application\nOften obtained in email attachments or at malicious websites\nDon’t replicate themselves – user error\nRansomware is the most common form of Trojan now\n\n Ransomware\n\nWill usually encrypt or block access to files and demand a ransom\nIt is a clever solution, because if an AV system removes it, it is often too late\nUsually distributed from malicious websites, or from already infected machines\nThe file decryption keys are protected by encrypting using the public key of a C&amp;C server\n\n Ransomware Variants\n\nMost of the challenge in successfully using ransomware is tricking a user into running it, and bypassing AV and browser protections\n\nFake emails\nMalicious web pages\nObfuscated Javascript attachments\nDeployed using exploit kits\n\n\n\n Exploits\n\nThe easiest way of getting access to a machine is having the user install something for you –not always possible!\nA software or hardware bug that allows an attacker to circumvent an Operating System’s security perimeter\n\n Memory Management\n\nIn C and C++, the programmer performs memory management\nFlexible, powerful, fast, but dangerous\n\nBuffer Overruns\nStack Overruns\nHeap Overruns\n\n\nMemory-managed languages avoid this, but of course may have their own vulnerabilities\n\n Buffer Overflows\n\nWhen a program is executed, contiguous blocks of memory can be allocated to store arrays (buffers)\nIf data is written into a buffer that exceeds its size, an overflow occurs\nThe data will overwrite the memory beyond the buffer –Bad!\n\n Program Memory\n\nMemory is stored in a virtual address space from 0x00000000 to 0xFFFFFFFF (32-bit)\nParts of the program are held in different regions by convention\nDifferent restrictions are placed on these regions\n\n The Stack\nThe stack holds information on local variables and functions calls (stack frames)\n\nA function call will push a new frame onto the stack\nA return will pop it off, and go to ret\n\n Stack Smashing\n\n\nIn C and C++, low level functions like strcpyperform no bounds checking at all\n\n\nIf str is long, we can write into other memory\n\n\nBy crafting the string str, we can overwrite the buffer and the return address with custom exploit code\n\n\n Protection: Canaries\n\nStack canaries modify the prologue and epilogue of all functions to check a value in front of the return address is unchanged:\nIf you can work out the canary value, there is no issue. You could also corrupt the Structured Exception Handler (SEH)\n\n Data Execution Prevention (NX)\n\nModern operating systems (where possible) will mark the stack as non-executable.\n\nNX(No eXecute) on AMD, XD(eXecute Disable) on Intel, XN(eXecute Never) on arm\n\n\nAn NX stack means that adding in our exploit code won’t work\nWe can circumvent this using a return-to-libcattack\n\n Further Protection\n\nTo defeat ret2libc various 0x0 null bytes are inserted into standard library addresses\nDevelopers also restrict access to obvious system calls\nAddress Space Layout Randomisation (ASLR) moves the address of library and programs around\nThey don’t have to move too much before your hand-crafted ret addresses will break\n\n Return-Oriented Programming\n\nLet’s forget about injecting code, how about just using existing code in the actual exploitable program?\nNo individual section of this program will do what we want\nFind short sections, “gadgets” and link them together\n\n Race Conditions\n\nWith concurrent threads or processes, timing can lead to security vulnerabilities:\n\n\n Heartbleed\n\n\nHeartbleed is a bug in OpenSSL\n\nOpen source SSL library\nStarted in OpenBSD\nUsed almost everywhere\n\n\n\nSpecifically targets the heartbeat extension –buffer over-read\n\n\nExtension to regular SSL\n\n\nUsed for keepalive purposes, to stop quiet connections being closed\n\nClient sends a message to the server to say its alive\nServer responds (also alive)\n\n\n\nNot always used, optional in SSL\n\n\n Database Security\n Concept\n\nDB confidentiality — protection of sensitive data within a DB\nDB integrity — data in a DB is accurate, complete, consistent, and valid\n\nInternal consistency — database entries obey some predefined rules.\nExternal consistency — database entries are correct\n\n\nDB availability — data in a DB is readily available for use when needed\n\n SQL\n Three Entities\n\nUsers\nActions\nObjects\n\n View-based Security\n Advantages\n\nViews are a flexible way of creating policies closer to application requirements\nViews can simplify complex queries and enhance data security by restricting direct access to tables\nData can be easily reclassified\n\n Disadvantages\n\nINSERT / UPDATE actions depend on the CHECK options, else might be blind inserts\nCompleteness and consistency are not achieved automatically\nCan quickly become very inefficient\n\n Further Defenses\n\nData swapping – Swap records but keep stats the same\nNoise addition – Alter aggregate output (a little)\nTable splitting – Separate data completely\nUser tracking – Log queries\n\n SQL Injection Attacks\n\nAn application or website is vulnerable to an injection if it doesn’t filter SQL control characters:\n\n’ represents the beginning or end of a string\n; represents the end of a command\n/*…*/ represent comments\n– represents a comment for the rest of the line\n\n\n\n Blind SQL Injection\n\nMost servers won’t directly output SQL errors to the screen\nA blind SQL injection performs database analysis without any actual output\n\n Anti-virus\n\n\n\n\n\n\n\n\n\nComputer programs used to prevent, detect, and remove malware\n\nPrevention, detection, and recovery!\n\n\n\nSignature-based Detection:\n\nStore some small code signature for each virus\nScan files either in bulk or at runtime, compare with the signatures on file\nGeneric signatures\n\n\n\nHeuristics:\n\nDetermine what actions and rules a virus program will normally adopt\nStart the program in a VM and see what it doe\nTheoretically could detect a virus that doesn’t strictly match some signature\n\n\n\nNext-gen\n\nThis is often used to describe newer antivirus technologies. Mostly a buzz word.\nRecent improvements are a wider scope of attributes on which to judge malware, and these are captured over time\n\n\n\n Intrusion Detection / Prevention\n\nIntrusion Detection Systems (IDS)\n\nDetects possible intrusion attempts\nGenerates alerts and logs for administrators\n\n\nIntrusion Prevention Systems (IPS)\n\nIdentical to IDS except also stops the attack\n\n\n\n IDS Deployment\n\nHost-based (HIDS):\n\nMonitors a single host to find suspicious activity including resource / app usage\nIn many ways modern Anti-virus does this\nAdditional layer of security software running on a host within a protected LAN or VPN\nCreates a profile of usage for specific users\nCan monitor CPU, memory use, application use and the network stack\n\n\nNetwork-based (NIDS):\n\nMonitors network traffic and analyses packets from different protocols to identify suspicious activity\nPlaced at a viewpoint on a network to examine and analyse traffic\n\nInstalled on a firewall or in a DMZ(Demilitarized Zone)\nInstalled behind a screened subnet\n\n\nMay perform deeper analysis than many firewalls, e.g. stateful protocol analysis and deep packet inspection\n\n\n\n Components of IDS\n\nSensors / Agents: collect and collate data from multiple viewpoints on a network\nAnalysers: ascertain if an intrusion has taken place\nReporting: notify the administrators via alerts on a console or graphical interface\n\n\n\n\n\n\n\n\n\n\nMultiple sensors allow us to distribute capture, but centralise computing overhead\n Detection Modes\n Signature-based\n\n\nFingerprinting sequences of operations or packets\n\n\nWhat are the signs that a host on the network is performing port scans?\n\nLarge amounts of ICMP traffic\nMany TCP connection (SYN) packets\nThese connections going to a variety of other hosts\n\n\n\nSnort\n\nSnort is a powerful and well established IDS\nUses rules to analyse network packets, and then can provide alerts or logging\n\n\n\nNmap timings\n\nYou can avoid detection when using nmap by reducing the speed of the scan\n\n\n\n Anomaly-based\n\nBuild a model of “normal” and find deviations\n\nBuild up a picture of normal usage, and detect when usage moves beyond what is normal\nAlways a trade off between false positives and false negatives\n\n\nWhat is Normal?\n\nRun a host within a quarantined environment and collect training data\nConstructed by monitoring audit logs\nSometimes rely on analysis of sequences of system calls through normal behavior\n\n\nNeural Networks for ID\n\nA network can be pre-trained\nSensor measurements are then passed through the network\nActivations in the specific output neuron signal an alert\n\n\n\n\n\nDrawbacks\n\nScaling:\n\nSearch space can increase exponentially\nReal-time data\n\n\nFalse negatives\n\nLimits in the representation\nWhat is normal can change – do we re-train and risk learning intruder behavior?\n\n\n\n\n\nStateful Protocol Analysis\n\n\nMore complex version of a stateful packet filter\n\n\nHold detailed session information on protocols being used, examine for attacks:\n\nWhy is this user logging in as root?\nWhy is this command being sent a 1000 byte buffer as a parameter?\n\n\n\nComputationally costly, and requires the IDS have all possible versions of these protocols described in its database\n\n\n TCP/IP Model\n\n\n\n\n\n\n\n\n\n\nA framework for communication over the computer network\nEach protocol carries the protocol in the layer above by appending headers to it\n\n\n IP Security\n\nIP is connectionless and stateless\n\nBest effort service\nNo delivery guarantee (Provided by TCP)\nNo order guarantee (Provided by TCP)\n\n\nIPv4 No guaranteed security support\nIPv6 security support is guaranteed - IPSec\n\n IPSec\n\nOptional in IPv4, mandatory support in IPv6\non Network layer (IP Layer)\nTwo major security mechanisms\n\nIP Authentication Header (AH) – authentication only (less used)\nIP Encapsulation Security Payload (ESP) – authentication + encryption\n\nIncludes an additional header within the IP packet that describes what encryption and authentication is in use\n\n\n\n\nDoes not contain any mechanisms to prevent traffic analysis\n\nPayload encrypted\nBut traffic (packets, source, destination, etc.) still visible\n(to obfuscate traffic, onion routing)\n\n\n\n Security Parameter Index\n\nStores security parameters e.g. crypto protocol and keys\nEstablished by Internet Security Association and Key Management Protocol (ISAKMP) during the Internet Key Exchange (IKE) handshake\nUses Diffie-Hellman for key exchange\nThe SPI references the entry in a table that corresponds to this session’s parameters\n\n Transport vs Tunnel Modes\n Transport mode\n\nTransport mode simply encrypts packets, providing host-to-host encryption but using the original IP header\nPrevents contents being read, but doesn’t stop traffic analysis or manipulation of the header\n\n Tunnel mode\n\nTunnel mode (usually gateway-to-gateway) protects some segment of a channel with encryption\nProvides some resistance to traffic analysis, and completely protects manipulation of the payload\nVPNs are commonly implemented this way\n\n Virtual Private Networks (VPNs)\n\nSecure connection over unsecure network (e.g. internet)\nDifferent technologies at different layers\n\nIPSec ESP Tunnel Mode (network layer)\nTLS (≈ application layer)\nSSH (≈ transport layer)\nWireGuard (network layer)\netc.\n\n\n\n Uses of VPNs\n\nUsed to provide C/I/Auth for remote access to resources\n\nEffectively “extending the private network”\n\n\nInstrumental for remote working\n\nAlso: branch offices, resources management, healthcare, etc.\n\n\nCan be a security bottleneck, so important to be holistic\n\nStrong authentication (e.g. MFA)\nUpdates\nMonitoring (IDS, etc.)\nEtc.\n\n\n\n Address Resolution Protocol (ARP)\n\nARP is a protocol used (in IPv4) to obtain physical MAC addresses for given IPs\n\nIt is used prior to constructing IP and TCP packets for communication\nNetwork layer\n\n\n\n ARP Cache Poisoning\n\n\nWe can simply send an unrequested ARP reply, and overwrite the MAC address in a hosts ARP cache with our own\n\n\nThis is a man-in-the-middle attack\n\n\n ARP Cache Poisoning Protection\n\nSome OSs ignore unsolicited ARP replies, or can be configured to use ARP differently\nSome software, such as intrusion detection packages, will include ARP spoofing detection\n\nMaintain a log of current MAC:IP assignments and ARP requests / replies\nAllows us to spot suspicious messages such as unsolicited ARP replies\n\n\n\n Domain Name System (DNS)\n\nDNS translates domain names into IP addresses\n\nE.g. nottingham.ac.uk → 128.243.80.167\n\n\nDNS packets are UDP\n\nStateless, on the transport layer\n\n\nDNS resolvers will cache the IPs for a while\n\n DNS Spoofing\n\n\nIf we can poison the cache of a nameserver people are using, we can replace a website lookup with our IP\n\n\nCan be achieved through prior ARP cache poisoning, a reply flood or a Kaminsky attack\n\n\n DNS Protection\n\nRandom query numbers help protect against spoof replies\nSince the Kaminsky attack, most resolvers now randomise the source port too\nDNSSEC aims to tackle DNS exploits by authenticating the name server and providing integrity for the messages\n\nWould integrate existing PKI/CAs into DNS\nNot new, and yet not very widespread yet\nOverhead, “concentration of powers”, lack of will by big players\n\n\n\n TCP Sequence Prediction Attack\n\nAlso “TCP session hijacking” (different from cookie session hijacking)\nIn TCP, sequence numbers used to keep track of order of packets\n\n TCP Sequence Prediction Attack\n\nDefenses\n\nRandomized SNs\nLower layer information (e.g. source mismatch)\n\n\nOther mitigation include router/firewall stateful config to disallow external sources, and IDS/IPS\n… or encryption\nNot very common these days, but still motivates safeguards in place\n\n Firewalls\n\n\n\n\n\n\n\n\n\n\nA hardware and/or software system\nPrevents unauthorised access of packets from one network to another\nAll data leaving any subnet must pass through it\n\n\nImplements ‘single point’ security measures\nSecurity event monitoring through packet analysis and logging\nNetwork-based access control through implementation of a rule set\n\n Location\n\nNetwork Firewalls\n\nPlaced between a subnet and the internet\n\n\nHost-based Firewalls\n\nPlaced on individual machines\n\n\n\n DMZ\nA demilitarized zone is a small subnet that separates externally facing services from the internal network\n Firewall Basic Function\n\nDefends a network against parties accessing internal services\nCan also restrict access from inside to outside services (e.g. IRC, P2P)\nNetwork Address Translation\n\nHides the internal machines with private addresses\n\n\n\n Firewalls are not enough\n\nCannot protect against attacks that bypass the firewall\n\nE.g. Tunnelling\n\n\nCannot protect against internal threats or insiders\n\nMight help a bit by egress filtering\n\n\nNetwork firewalls cannot always protect against the transfer of virus-infected programs or files\n\n Packet Filters\n\nSpecify which packets are allowed or dropped\nRules based on:\n\nSource / Destination IP\nTCP / UDP port numbers\n\n\nPossible for both inbound and outbound traffic\nCan be implemented in a router by only examining packet headers (IP / TCP)\n\n Packet Filter Rules\n\nRule execution depends on implementation\n\nIPTABLES: First rule to match is applied\nPF: All rules are examined, the last match is applied\n\n\nRules are organised in chains, which are logical subgroups of rules\nDepending on the packet, different chains are activated\n\n IPTABLES\n\nAn application that provides access to the Linux firewall rule tables\n\nNot actually a firewall, but configures the firewall\nThe firewall is mostly implemented as netfilter modules\n\n\n\n Tables and Chains\n\n\nIPTABLES uses tables to store chains\n\nDefault is the filtering table\n\n\n\nChains are ordered lists of rules\n\nRules match, or they don’t\n\n\n\nMatches result in a jump, else we check the next rule\n\n\nThere can be multiple chains per table\n\nE.g. a TCP handling chain\n\n\n\nJumps can go to ACCEPT, DROP, LOG or another chain\n\n\nComplex behaviour can be built up\n\n\n Defaults\n\nThere are four built-in tables in IPTABLES:\n\nFilter\nNAT\nMangle – Packet alteration\nRaw – Skips connection tracking\n\n\nThe default table is the filtering table, including Input, Output and Forward chains\n\n Policies\n\nPermissive – allow everything except dangerous services\n\nEasy to make a mistake or forget something\n\n\nRestrictive – block everything except designated useful services\n\nMore secure by default\nFairly easy to DoS yourself!\n\n\n\n Packet Filter Issues\n\nPacket filters are simple, low level and have high assurance\nBut, they cannot:\n\nPrevent attacks that employ application-specific vulnerabilities\nDo not support higher-level authentication schemes\nEasy to accidentally allow or deny packets incorrectly\n\n\n\n Stateful Packet Filters\n\nUnderstand requests and replies (e.g. ACK/SYN)\nDynamically generate rules\nE.g. FTP client, connect to 21, receive from 20\nCan support policies for a wider range or protocols\n\n Application-level Gateways\n\nPacker filters have limited criteria that allow data in and out\nAn application gateway considers the application-layer protocol that is in use\nSome protocols, like HTTP and SSH will be allowed, others, like BitTorrent, may be blocked\nCan perform much more complex port control than fixed rules\n\n Proxy Servers\n\n\nProxy servers initiate a connection on our behalf\n\n\nThey can block certain access, and scan for malicious files or web pages\n\n\nIssues\n\nLarge overhead per connection\nMore expensive than packet filtering\nConfiguration is complex\nA separate server is required for each service\n\n\n\n Network Address Translation\n\n\nThe shortage of IP addresses mean that most routers now perform NAT automatically\n\n\n\n\n\nThe implicit advantage in NAT is that your machine is almost totally hidden from the internet\n\n\nOnly established connections are forwarded to your internal machine\n\nOr, specific port forwarding rules\n\n\n\nThis prevents any unsolicited attacks on random ports, but no other types of attack\n\n\n Network Segmentation\n\nDividing computer networks into smaller parts\nGranular policies\n\nSometimes easier to manage/configure and sometimes more efficient\n\n\nDamage control, containment\nSeparation can be physical\n\nDistinct routers/switches\n\n\nOr logical\n\nVirtual Local Area Networks (VLANs)\nSoftware-Defined Networks (SDNs)\nPhysical resources are shared but distinct logical networks\n\n\n\n Zero Trust\n\nIn the past, internal network considered “trustworthy”\n\nFew restrictions\n\n\nThreats can very much originate from internal network\n\nOr “lateral” movement if perimeter breached\n\n\nIn a Zero Trust Architecture, no implicit trust\n\n“Never trust, always verify”\nPenetrating the perimeter is no longer enough\nIt is the current mindset for networks (e.g. NCSC 2019)\n\n\nExplicit and mutual authentication, context-aware, attribute/role-based access control, least privilege, segmentation, etc.\n\n WiFi Security Protocols\n\nWired Equivalent Privacy (WEP)\n\nWeak, password cracked in minutes\n40-bit keys!\nDeprecated since 2003\n\n\nWiFi Protected Access (WPA)\n\nCurrent standard is WPA2\n128-bit keys\nPersonal (Pre-Shared Key or PSK) and Enterprise (unique credentials)\nWPA3 in deployment (more secure but not all devices compatible yet)\n\n\n\n Common Threats\n\nPacket sniffing/eavesdropping\nRogue Access Point / Evil Twin / KARMA\n\nCopying SSID of existing network, relaying traffic\nCaptive Portal\n\n\nPassword Cracking (WEP, or weak PSK in WPA)\nKRACK (2017 WPA2 attack)\n\nExploiting weakness in handshake\nPatched in most systems (…ish) for WPA2\nWPA3 is safe\n\n\n\n Some Good Practice\n\nStrong Authentication\nUpdates\nTurn off WiFi Protected Setup (WPS)\n\n2011 attack on PIN brute force\n\n\nMAC filtering\nSegment IoT/legacy devices into separate VLANs (e.g. WPA2)\nWIPS/WIDS\n\n Denial of Service Attacks\n\nA denial of service attack is an attempt to make a machine or network resource unavailable to its authorised / intended users\nThis will usually involve flooding a machine with enough requests that it can’t serve its legitimate purpose\n\nE.g. Ping flood\n\n\nA distributed denial of service (DDoS) occurs where there is more than one attacking machine\n\n Amplification Attacks\n\nRegular attacks are your bandwidth vs your targets\nAmplification attacks utilise some aspect of a network protocol to increase the bandwidth of an attack\n\n Smurf and Fraggle Attacks\n\nSmurf attacks broadcast an ICMP Ping request to a router, but with a spoofed IP belonging to the victim\nA Fraggle attack is identical in principle, using UDP echo packets\n\n DNS Amplification\n\n\nRecursive resolvers respond to DNS queries then return a response\n\n\nThis response can be many times larger than the query\n\n\nIn an ideal world, all DNS resolvers would:\n\nUse an authorised list of requesters – e.g. and ISP allowing requests from only their customers\nEgress filtering – Why is this external IP using my resolver?\n\n\n\nMany DNS servers are set up incorrectly, and will happily amplify your traffic – Open Resolvers\n\nSome open resolvers (e.g. Google’s 8.8.8.8) use mitigation like rate limiting or short answers\n\n\n\nBotnets maintain lists of misconfigured open resolvers, and there are projects attempting to shut them down\n\n\n NTP Amplification\n\nNTP is a protocol for synchronising time between machines\nExtremely similar to DNS amplification\nMON_GETLIST request returns the list of the last 600 contacts -&gt; 200x amplification\n\n Low and Slow\n\nSlowloris\n\nOpen numerous connections to a server\nBegin an HTTP request, but never actually finish it\n\n\nR-U-Dead-Yet?\n\nSimilar to slowloris\nBegin an extremely long HTTP POST, send tiny amounts at a time\n\n\n\n Internet Threat Models\n\nDifferent from other threat models:\n\nThe attacker isn’t in control of the network\nThe attacker hasn’t got access to the target’s OS\n\n\n\n Cookies\n\n\nHTTP is a stateless protocol\n\n\nMost of what we do online is stateful\n\n\nCookies are small text files used to provide persistence\n\n\nServers can provide cookies during HTTP responses, using Set-Cookie\n\n\nBrowsers will return any cookies for a given domain in GET and POST requests\n\n\n Types of Cookie\n\nSession – Deleted when the browser exits, contain no expiration date\nPersistent – Expire at a given time\nSecure – Can only be used over HTTPS\nHTTP Only – Inaccessible from JS\n\n Third Party Cookies\n\nCookies are associated with the domains that produced them\n\nAmazon.com cookies don’t go to google.co.uk\n\n\nSome websites include request to other domains, such as 3rd party advertisers\n\nThese serve cookies – a lot\n\n\n\n Cookie Vulnerabilities\n\nHow a website uses a cookie is up to the server\nCookie stealing/ hijacking –\n\ngaining access to the information stored in a cookie, allowing them to impersonate the user and potentially gain unauthorized access to accounts or data\n\n\nCookie poisoning –\n\nmodifying or tampering with a cookie, often to inject malicious code or data, which can lead to a variety of security vulnerabilities\n\n\n\n Cross-site Scripting (XSS)\n\nA type of injection attack, similar in many ways to an SQL Injection\nHTML is read by a browser, and is a combination of content (text) and structure (html tags)\nIf we can inject html structures into the content of a website, the browser will simply execute these – e.g.  tags\n\n Persistent XSS\n\nEven worse, no need to trick people into clicking links\nAny website that doesn’t properly sanitise html tags from user input is vulnerable\nBlog posts with comment sections are obvious targets, but there are many more\n\nForums, web comments, shopping reviews, etc.\n\n\n\n Preventing XSS\n\nWebsites must aggressively escape HTML characters from any user input / output\n\nLocate all positions in which a website handles untrusted data\nEscape appropriately depending on type of input\n\n\nWhen you consider all of the things people input on interactive websites, this can be a real problem\n\n Cross-site Request Forgery (XSRF)\n\nWhen a user puts in an HTTP request, they will also send any relevant session cookies\n\nE.g. an SID from having logged in\n\n\nIf the user has already authenticated, a malicious URL can then perform some action on their account\n\n XSRF In POST\n\nMost websites use POST, this is little defence\nThe phishing email just points to a convincing website with a malicious form on it\n\n Preventing XSRF\n\nXSS vulnerabilities make XSRF a lot easier! Fix these!\nUse synchroniser tokens\n\nEach website form has a one-time token that the server validates when the form is submitted\n\n\n\n SSL/TLS\n\nTLS is a protocol that provides authenticated and encrypted sessions\nSecure Socket Layer (SSL) came first, then after v3.0 it became Transport Layer Security (TLS), currently v1.2 / v1.3\n\n TLS\nTransport Layer Security has two layers:\n\nThe Record Layer\n\nUsing established symmetric keys and other session info, will encrypt application packets, much like IPSec\n\n\nThe Handshake Layer\n\nUsed to establish session keys, as well as authenticate either party – usually the server using a public-key certificate\n\n\n\n TLS Handshakes\n\nClient and server say hello\nPair agree on cipher suite\nPublic key verification\nKey Exchange\nFinal checks\nSend Application Data\n\n Hello\nEstablish which cipher suite will be used\n\nWhich TLS version?\nWhat ciphers and algorithms?\nClient and server random numbers for this session\n\n Cipher Suites\nTLS defines cipher suites that describe the cryptographic primitives and other security parameters that will be used in that session:\n\n Key Exchange\nBoth parts of a Diffie-Hellman key exchange\n\nModern TLS always uses an elliptic curve version of the scheme\nEphemeral means we do this fresh for every handshake\nEstablishes secret data for later use\nWe call this data the pre-master secret\n\n Public Key\n\nPublic key is used to confirm the identity of the server\n\nThe certificate includes a public key for the server\nThe signature is computed with the private key\nThis means that the client can verify the signature using the certificate\n\n Symmetric Keys\n\n\nWe need to confirm everything has worked safely, and prepare for the session\n\n\nThe hash function is useful for creating a symmetric key\n\n\nPre-master secret, Hello random numbers -&gt; Hash Function -&gt; Master Secret -&gt; Hash Function -&gt; Symmetric Key\n\n\n\n Finishing up\nThe finished messages contain a digest of all the messages in the handshake\n\nVerifies whether both parties have seen the same version of events\nEncrypted using the symmetric key\n\n Application Data\nWe switch to regular TLS records for an encrypted session\n\nThe 1.3 handshake embeds much of the key exchange into the hello messages\nThe client guesses a key sharing algorithm within client hello\nIf the key share isn’t supported, server sends a hello retry request\nEncryption begins after the second message\n\n 0-RTT Resumption (Zero Round Trip Time)\n\nDuring application data, the server can send a pre shared key for use next time\nNote this doesn’t always supply forward secrecy (not secure)\n1-RTT session resumption is closer to original 1.2 session keys\n\n TLS Vulnerabilities\n\nThe man-in-the-middle vulnerabilities are usually countered using public-key authentication\nThe majority of TLS problems are implementation\n\nHeartbleed\n\n\nProtocol downgrade attacks can be a concern – many servers still allow weak cipher suites\n\nFREAK and Logjam force the use of 512-bit keys\n\n\n\n Public Key Infrastructure\nWhy do we need PKI?\nThe key problem:\n\nWe have no real reason to trust this certificate\nSo we have no real reason to trust the signature\n\n Digital Certificates\n\nIf we want to use public key cryptography, we need trust\nWe can use a trusted third party in order to verify the ownership of a public key\nPrimarily managed through Public Key Infrastructure (PKI)\nCertificates usually held in X509 format\n\n Certificate Issuance\n\n\nA server has a public key that they want people to trust\n\n\nUsing some subject details, the server creates a Certificate Signing Request (CSR)\n\n\nA Certification Authority (CA) uses this to create and sign a certificate\n\n\n Certificate Use\n\nThe server can supply digital signatures using the public key, backed by the certificate when requested, e.g. during a TLS handshake\n\n Chains of Trust\n\n\nTo verify the trust in the Server.com certificate, we need to examine the signing certificate\n\n\nIn many cases, the chain involves multiple certificates\n\n\nChains always end in a root certificate, located on your machine\n\n\n Limitations of PKI\n\nAdministrators use it badly:\n\nUse of wildcard certificates\nPoor server setup – low quality ciphers, TLS versions etc.\n\n\nOver-reliance on a few key libraries\nUsers don’t understand it\n\nIf asked to bypass a bad certificate, users will!\nCheck out badssl.com\n\n\n\nThe main problem with PKI is that once you’ve trusted one or more CAs, you trust every certificate signed by them\n\nIf a private key is stolen – the certificate must be revoked\nCertificate Revocation Lists\n\nA list of revoked certificates, signed by the CA\nPublicly available via a URL\nMust be checked during certificate verification\n\n\nOpen Certificate Status Protocol (OCSP)\n\nA challenge-response in which a CA is polled for live revocation status\nCan be pre-obtained and reused for a period\n\n\n\nMany clients do not properly check either CRLs or OCSP\n Alternatives to PKI CAs\n\nPublic CA\n\nA must for public websites\nChose a CA that supports what you need\n\n\nSelf-signed\n\nHarder to deploy\nMay encourage bad practice\n\n\nPrivate CA\n\nMore upfront work\nEasier to deploy\nAs secure as PKI\n\n\n\n","slug":"Computer_Security","date":"2025-08-28T07:58:00.000Z","categories_index":"Notes","tags_index":"Security,Crypto","author_index":"Huaji1hao"},{"id":"31190dc1cbfcc752001985338f19d05d","title":"Mobile Development Exam","content":" Handling UI State Preservation in Android\nIn Android, activities may be destroyed and recreated due to system constraints, such as low memory, which can lead to the loss of UI state and data. Describe how Android handles such cases by default [2 marks] and the various strategies developers can use to preserve UI state across activity destruction and recreation [3 marks]. Specifically, explain how onSaveInstanceState() and ViewModel help in preserving UI data [2 marks]. Discuss the advantages and limitations of each approach [2 marks]. Provide specific examples of scenarios where each method would be most appropriate, such as using onSaveInstanceState() to preserve small UI elements like text input, and ViewModel for larger, more complex data sets that need to persist through configuration changes [1 mark].\n Answer:\n 1. Default Handling by Android (2 marks)\nBy default, when an activity is destroyed and recreated (e.g., screen rotation), Android calls onSaveInstanceState() to store UI-related data in a Bundle, which is restored in onCreate() or onRestoreInstanceState(). However, only primitive data types (e.g., strings, numbers) are saved, while complex data like lists or objects require additional handling.\n\n 2. Strategies for Preserving UI State (3 marks)\nDevelopers can use different strategies to preserve UI state:\n\nonSaveInstanceState(Bundle)\n\nSaves lightweight UI-related data such as form input, scroll positions, and selection states.\n\n\nViewModel\n\nKeeps temporary data like a list of books, user progress, or search results even when the screen rotates, ensuring a smooth user experience.\n\n\nPersistent Storage (SharedPreferences, Room Database, or Files)\n\nSaves long-term data like user preferences, book lists, or notes that should be available even after the app is closed.\n\n\n\n\n 3. How onSaveInstanceState() and ViewModel Help in Preserving UI Data (2 marks)\n\nonSaveInstanceState(Bundle)\n\nSaves small UI data (e.g., text input) in a Bundle  before an activity is destroyed.\nRestores data in onCreate() when the activity is recreated (e.g., after screen rotation).\n\n\nViewModel\n\nKeeps temporary app data (e.g., shopping cart items) even when the configuration changes.\nUnlike onSaveInstanceState(), it doesn’t reset after process death.\n\n\n\n\n 4. Advantages and Limitations (2 marks)\n\n\n\nApproach\nAdvantages\nLimitations\n\n\n\n\nonSaveInstanceState()\nSimple, fast, and automatically restored\nLimited to small data (e.g., String, Int), not suitable for large datasets\n\n\nViewModel\nKeeps data when the screen rotates without slowing down the app.\nData is lost if the app is fully closed or killed.\n\n\n\n\n 5. Best Use Cases (1 mark)\n\nUse onSaveInstanceState() when preserving small UI-related data, such as:\n\nTyped text in a form\nScroll position in a RecyclerView\nSelected tab in a ViewPager\n\n\nUse ViewModel for larger and reusable data, such as:\n\nA list of books retrieved from an online library\nUser’s reading progress and preferences\nAny data that should stay the same when rotating the screen or switching between pages\n\n\n\n\n Android Application Development\nYou are tasked with developing an Android application that serves as a personal library for tracking books and reading progress. The app should allow users to add and store book entries, including the title, author, genre, and a brief description. Users should also be able to mark their current reading status (e.g., “Reading”, “Completed”, or “Wishlist”) and log notes about each book.\nDescribe the Android application components you would implement [3 marks], the architecture for the application [4 marks], and the relationship between the components [3 marks].\n Answer:\n **1. **Android Application Components (3 marks)\nThe app will use the following core Android components:\n\nActivity (Main Screens of the App)\n\nMainActivity: Shows the list of books.\nBookDetailActivity: Displays details of a selected book and allows users to update status or add notes.\nAddBookActivity: Lets users add new books to the library.\n\n\nService (For Background Tasks, Optional)\n\nIf the app needs to sync books with cloud storage, a background service (WorkManager or Foreground Service) can handle it.\n\n\nContent Provider (For Data Sharing, Optional)\n\nIf the app needs to share book data with other apps, a ContentProvider can provide access.\n\n\n\n\n 2. Application Architecture (4 marks)\nThe app follows the MVVM (Model-View-ViewModel) architecture:\n\nModel (Data Layer)\n\nRoom Database: Stores books persistently.\nBookDao: Defines SQL queries (e.g., insert, fetch books).\nRepository: Manages data flow between the database and UI.\n\n\nViewModel (Logic Layer)\n\nBookViewModel: Handles book operations and updates UI via LiveData.\n\n\nView (UI Layer)\n\nActivities: Manage user interactions.\nRecyclerView + Adapter: Displays the book list efficiently.\n\n\n\n\n Relationship Between the Components (3 marks)\n\nUser Interaction\n\nUsers browse books in MainActivity.\nClicking a book opens BookDetailActivity to update status or notes.\nAddBookActivity allows adding new books.\n\n\nData Processing\n\nBookViewModel manages book data through BookRepository.\nLiveData updates the UI automatically when data changes.\n\n\nData Storage\n\nBookRepository saves book data in Room Database.\nBookDao handles database queries.\nIf cloud sync is needed, a Service manages it.\n\n\n\nExample Flow:\n\nUser adds a book → AddBookActivity calls BookViewModel.addBook()\nViewModel updates Repository, which stores data in Room\nLiveData notifies MainActivity, updating the book list automatically.\n\n\n\n\nStarting from Android 8.0 (Oreo), implicit broadcasts (those without a specific target app) can no longer be registered in the manifest, but they can still be dynamically registered at runtime.\n\n[x] True\n[ ] False\n\n(2 marks)\n\n\n\n\n\nAn Intent in Android can only be used to start activities; it cannot be used to start services or broadcast messages.\n\n[ ] True\n[x] False\n\n(2 marks)\n\n\n\nIntent can be used for activities, services, and broadcasts\n\n\n\n\nAll dangerous permissions must be declared in the app’s AndroidManifest.xml file, but they can be automatically granted without user intervention if declared correctly.\n\n[ ] True\n[x] False\n\n(2 marks)\n\n\n\nThey are not automatically granted, even if declared in AndroidManifest.xml\n\n\n\n\nIn a Recycler View, if you update the data set, the changes will automatically reflect in the UI without calling any additional methods.\n\n\n[ ] True\n\n\n[x] False\n\n\n(2 marks)\n\n\n\nNeed notifyDataSetChanged()\n\n\n\n\nIn Android, a Service must always run in a separate process from the main application.\n\n[ ] True\n[x] False\n\n(2 marks)\n\n\n\nA Service runs in the main application process unless explicitly set otherwise\n\n\n\n\nIn Android, the onCreate() method is the first callback method called during the lifecycle of an activity.\n\n[x] True\n[ ] False\n\n(2 marks)\n\n\n\n\n\nThe &quot;R.java&quot; file in Android is a dynamically generated file that contains resource IDs for all the resources in your app.\n\n[x] True\n[ ] False\n\n(2 marks)\n\n\n\n\n\nIn Android, the ViewGroup class is a subclass of the View class and can contain other views.\n\n[x] True\n[ ] False\n\n(2 marks)\n\n\n\n\n\nThe Room persistence library allows direct database access on the main thread by default.\n\n[ ] True\n[x] False\n\n(2 marks)\n\n\n\nPerforming database operations on the main thread can block the UI, so Room prevents it.\n\n\n\nWhat are components of Android Jetpack?\n\n\n[x] Core utilities like AppCompat\n[x] Lifecycle-aware components like ViewModel, LiveData, and Room\n[x] Utilities for background tasks such as WorkManager\n[ ] A modern toolkit for building native Android user interfaces declaratively with XML\n\nit does not rely on XML\n\n\n[ ] Components for linking XML directly to the manifest\n\nJetpack does not provide such a feature\n\n\n\n(2 marks)\n\n\nWhat are some features of Jetpack Compose that enhance UI development in Android?\n\n\n[x] Use of composable functions for building UI components\n\nuses Kotlin functions annotated with @Composable to build UI\n\n\n[ ] Replaces Java with Swift for UI design\n\nSwift is used for iOS development\n\n\n[x] Real-time UI updates with hot reloading\n\nsupports live previews and hot reloading\n\n\n[x] Reduces the need for XML layouts\n\ndefine UI entirely in Kotlin\n\n\n[ ] Requires constant internet connectivity to function\n\nIt is a UI framework that works offline within the app\n\n\n\n(2 marks)\n\n\nWhich of the following are good practices to prevent an Application Not Responding (ANR) error in Android apps?\n\n\n[ ] Running all tasks on the main thread to avoid thread switching\n\nThe UI thread should handle only UI-related tasks\n\n\n[x] Offloading time-consuming operations to background threads using AsyncTask, Thread, or WorkManager\n[x] Avoiding blocking operations like network requests or database queries on the main thread\n\nNetwork calls, database queries, or file I/O should never run on the UI thread\n\n\n[ ] Using Thread.sleep() on the UI thread to free up memory\n\nIt is a bad practice\n\n\n[ ] Updating the UI only from background threads\n\nUI updates must always be done on the main thread\n\n\n\n(2 marks)\n\n\nWhich of the following are recommended ways to handle long-running tasks in Android without blocking the main thread?\n\n\n[ ] Using Thread.sleep() on the main thread to simulate delays.\n\nFreezes the UI and can cause ANRs\n\n\n[x] Using Handler to post updates from a background thread to the main thread.\n[x] Using Thread and posting results to the main thread with Handler.post().\n\nA separate Thread runs the long task, and Handler.post() updates the UI\n\n\n[ ] Running background operations directly in the onCreate() method of an activity.\n\nBlocks UI thread; should use background threads instead.\n\n\n[ ] Using WorkManager’s update UI function.\n\nWorkManager is designed for background tasks but does not have a built-in UI update function\n\n\n\n(2 marks)\n\n\nWhy does the app crash when updating the UI from a background thread?\n\n\n[ ] The Looper object is not initialized in the background thread.\n\nLooper is only needed if a background thread needs to process messages\n\n\n[x] UI updates must only happen on the main thread, and performing UI changes from a background thread can cause a crash.\n[ ] The app is missing the necessary permission to update UI components from any thread.\n\nNo special permissions are required to update UI elements\n\n\n[ ] The background thread does not have access to the UI components due to permission restrictions.\n\nIt’s just not allowed due to thread safety\n\n\n[ ] The background thread is using Handler incorrectly to post messages to the UI thread.\n\nThe issue arises only when updates are made directly from a background thread.\n\n\n\n(2 marks)\n\n\nWhy does the service not restart after it is killed by the system?\n\n\n\n[x] The service is returning START_NOT_STICKY in onStartCommand().\n\nSTART_NOT_STICKY means the system will not restart the service if it is killed due to low memory\n\n\n\n[x] The service is bound to an activity, and when the activity is destroyed, the service stops.\n\nIf no component is bound, the service does not restart\n\n\n\n[ ] The service does not declare the required permission in the manifest.\n\n\nPermissions affect whether the service can be started,\nbut they do not control whether it restarts after being killed\n\n\n\n\n[x] The service uses IntentService, which automatically stops once it completes its task.\n\nIntentService handles tasks in the background and stops itself once it completes execution\n\n\n\n[ ] The service is running as a foreground service and should not be restarted by the system.\n\nForeground services have higher priority and they can restart if set to START_STICKY\n\n\n\n(2 marks)\n\n\nWhy might an activity’s onCreate() method inadvertently start a second instance of the service when a configuration change occurs?\n\n\n[ ] The service is automatically restarted by the system during configuration changes.\n\nThe system does not automatically restart services on configuration changes\n\n\n[x] The onCreate() method is called again after the configuration change, and it calls startService() without checking if the service is already running.\n[ ] The service is duplicated because onResume() is called twice during configuration changes.\n\nonResume() is not responsible for starting a service\n\n\n[ ] The service was bound to the activity and gets recreated when the activity restarts.\n\nThis will not create a second instance\n\n\n[x] The service does not have proper lifecycle management implemented.\n\n(2 marks)\n\n\nWhy should developers use ProGuard in their Android projects?\n\n\n[x] To optimize the APK size by removing unused code and resources.\n[ ] To optimize permission requests to only those which are necessary.\n\nProGuard does not modify permissions\n\n\n[ ] ProGuard improves the runtime performance of an app by converting Java code to native code.\n\nProGuard does not convert Java to native code\n\n\n[ ] To prevent the app from using deprecated APIs in production builds.\n\nProGuard does not check for deprecated APIs\n\n\n[ ] It is required by Google Play for all apps to reduce security vulnerabilities.\n\nGoogle Play does not require ProGuard\n\n\n\n(2 marks)\n\n\nWhy would a periodic task scheduled with WorkManager not execute periodically?\n\n\n[x] The device is in Doze mode, which prevents periodic tasks from running on time.\n[ ] The periodic task does not work if the app does not have the INTERNET permission.\n\nWorkManager does not require INTERNET permission\n\n\n[ ] WorkManager was not declared in the manifest.\n\nWorkManager does not require manual manifest registration\n\n\n[ ] WorkManager tasks are not designed for periodic execution and should be used only for one-time tasks.\n\nWorkManager supports both one-time and periodic tasks\n\n\n[x] The task constraints (e.g., network or battery conditions) are not satisfied, so the task is deferred.\n\n(2 marks)\n\n\nYou are working on an Android app that uses a remote service for background data processing. The service should communicate back with the activity using callbacks, but you are unsure how to implement this with inter-process communication.\n\n\n[x] Use Messenger to send messages between the activity and the service, and return responses with a Handler.\n[x] Use AIDL (Android Interface Definition Language) to define an interface for communication and handle callbacks in the service.\n[ ] Use IntentService, which supports communication between processes and automatic queuing of requests.\n\nIt does not support two-way communication.\n\n\n[ ] Use JobScheduler to schedule tasks, as it automatically handles communication between components.\n\nJobScheduler is used for delayed or periodic background tasks\n\n\n[ ] Use a thread pool to handle incoming requests concurrently.\n\nA thread pool can process tasks, but it does not provide IPC between an activity and a remote service\n\n\n\n(2 marks)\n\n\nYour app requests permissions to access contacts and the user’s location, but during runtime, you notice that some users are encountering issues where permissions are denied, even after they granted them during installation.\n\n\n[x] Dangerous permissions must be requested at runtime starting from Android 6.0 (Marshmallow).\n[ ] The app should ask for permissions in the manifest and trust the system to grant them at installation.\n\nSince Android 6.0, dangerous permissions must be requested at runtime\n\n\n[x] Use requestPermissions() to request dangerous permissions during runtime.\n[ ] Use setHighPermission alongside the permissions.\n\nThere is no such API as setHighPermission\n\n\n[ ] Use the APK release version for the permissions to be granted when the app is in use.\n\nPermissions work the same way in debug and release builds\n\n\n\n(2 marks)\n\n\nWhy might a task scheduled with WorkManager not execute immediately?\n\n\n[x] WorkManager is designed for tasks that are deferrable and should not execute immediately.\n[x] The task constraints, such as network availability or charging state, are not met, preventing the task from executing.\n[x] The system is prioritizing higher-priority background tasks over your WorkManager task.\n[ ] WorkManager tasks always run with the highest priority and should execute immediately unless there is an error in the code.\n\nWorkManager tasks are low-priority by design, as they are meant for background execution\n\n\n[ ] The WorkManager task was scheduled without using OneTimeWorkRequest or PeriodicWorkRequest.\n\nNot using them does not cause execution delays\n\n\n\n(2 marks)\n\n\nYou are developing an app that shares files via content URIs with other apps. You want to grant temporary permissions to another app to read or write a specific file. Which of the following approaches should be used?\n\n\n[x] Use Intent.FLAG_GRANT_READ_URI_PERMISSION to grant temporary read access to the file.\n[x] Use Intent.FLAG_GRANT_WRITE_URI_PERMISSION to grant temporary write access to the file.\n[ ] Store the file in external storage and make it publicly accessible to other apps.\n\nStarting from Android 10, apps cannot directly access external storage files unless they own the file\n\n\n[ ] Use a ContentProvider without any flags, as temporary permissions are automatically granted.\n\nThe intent flags must be used\n\n\n[ ] Explicitly request the user’s permission to grant access using runtime permission dialogs.\n\nRuntime permissions are needed to access app-owned storage but do not grant access to other apps\n\n\n\n(2 marks)\n\n\nAfter a recent code update, users report that the app’s settings are reset to default every time the app is restarted. What could be causing this issue?\n\n\n[x] Failing to store settings in a persistent storage like SharedPreferences or a database.\n\nTemporary variables will be lost when the app restarts\n\n\n[ ] Misusing the content provider.\n\nContent Providers are not typically used for app settings—they are for sharing data between apps\n\n\n[ ] Incorrectly handling onTemporaryPause(), leading to settings not being saved properly.\n\nThere is no lifecycle method named onTemporaryPause() in Android\n\n\n[x] Overwriting settings with default values in onCreate() without checking existing preferences.\n[ ] Not declaring the shared preferences in the manifest.\n\nIt does not need to be declared in the manifest\n\n\n\n(2 marks)\n\n\nHow does ProGuard function in reference to Android APK optimization?\n\n\n[x] To use ProGuard, it must be enabled in the build.gradle.\n\nIt must be explicitly configured in build.gradle\n\n\n[x] It minimizes APK size by removing unused code.\n[ ] ProGuard helps in identifying and fixing security vulnerabilities in the code.\n\nIt does not actively detect or fix security issues\n\n\n[ ] ProGuard automates the process of translating Java code to Kotlin.\n\nJava-Kotlin interoperability is handled by the Kotlin compiler\n\n\n[ ] It can be run to help debug your code.\n\nProGuard obfuscates the code, making debugging harder\n\n\n\n(2 marks)\n\n\nIn a news app, how should you handle the storage of recently viewed articles so that they persist across sessions but are also quickly accessible during runtime?\n\n\n[x] Employing the Room database to persist detailed article data.\n[ ] Utilizing content providers to speed up app execution.\n\nIt is meant for sharing data between apps, not local caching\n\n\n[ ] Relying on SharedPreferences for structured storage of articles and content.\n\nIt is meant for storing small key-value data\n\n\n[ ] Storing article details in ViewModel for immediate access.\n\nViewModel does not persist across app restarts\n\n\n[x] Using SharedPreferences to save a list of recently viewed article IDs.\n\n(2 marks)\n\n\nWhat are best practices when requesting permissions in an Android app?\n\n\n[ ] Request all necessary permissions at once during the app’s first launch for efficiency.\n\nRequesting too many permissions at launch confuses users and increases rejection rates\n\n\n[x] Permissions should be requested as close as possible to the point of use.\n\nRequesting permissions only when they are needed makes the request more understandable\n\n\n[ ] Use the requestPermissions() method from a background thread for better performance.\n\nPermissions must be requested from the UI thread\n\n\n[x] Explain to the user why the permissions are needed, possibly using a pre-permission dialog (if possible).\n\nIf a permission is critical, providing a rationale before requesting it increases user acceptance\n\n\n[x] Request only the minimum set of permissions necessary for the app’s functionality.\n\nRequesting unnecessary permissions can make users suspicious and increase permission denial rates\n\n\n\n(2 marks)\n\n\nWhat are important considerations when implementing remote services using Binder in Android?\n\n\n[x] Using Messenger instead of AIDL can simplify the implementation for communication.\n[ ] Remote services can share the same Linux user ID to increase data-sharing efficiency.\n\nApps can share the same UID using sharedUserId, but this is deprecated for security reasons.\n\n\n[x] AIDL (Android Interface Definition Language) is used to define the interface for remote services.\n[ ] Remote services run in the same process as the client by default.\n\nBy default, remote services run in a separate process\n\n\n[x] Remote service methods should be designed to be quick and non-blocking.\n\nMethods should be quick and non-blocking to avoid freezing the client\n\n\n\n(2 marks)\n\n\nWhat are the best practices for an Android application to handle memory constraints?\n\n\n[ ] Use static variables to retain large datasets across activity re-creations.\n\nStatic variables are not automatically cleared and can cause memory leaks if not handled properly\n\n\n[ ] Store all critical data in onSaveInstanceState() to ensure it’s preserved.\n\nIt is designed for small UI-related data\n\n\n[ ] Regularly call System.gc() to manually trigger garbage collection.\n\nIt does not guarantee immediate garbage collection and may even slow down the app\n\n\n[x] Override onTrimMemory() to release non-critical resources when notified.\n\nThis method is called when the system detects low memory conditions\n\n\n[ ] Store all critical data in onPause() to ensure it’s preserved.\n\nThe system may kill the app before onPause() is completed\n\n\n\n(2 marks)\n\n\nWhat are the implications of running long-lived Services on Android app performance, especially in memory-constrained environments?\n\n\n[x] They can lead to increased memory usage, potentially affecting overall system performance.\n\nLong-running services stay in memory, consuming RAM and CPU\n\n\n[ ] Long-lived Services enhance app performance by keeping the app active in memory.\n\nIt increases system load and reduces available memory for other apps\n\n\n[x] Running multiple long-lived Services can lead to the Android system frequently invoking garbage collection.\n[ ] The system prioritizes long-lived Services, reducing the likelihood of them being killed.\n\nIf memory is low, background services are among the first to be killed\n\n\n[ ] Such Services must implement onLowMemory() to mitigate performance impacts.\n\n(2 marks)\n\n\nWhat techniques are recommended for efficient background processing in Android apps?\n\n\n[ ] Running intensive background tasks in the onPause() method of the Activity.\n\nRunning heavy tasks in onPause() blocks UI responsiveness\n\n\n[x] Using WorkManager for tasks that need to be executed even if the app or device restarts.\n\nWorkManager is designed for deferrable, guaranteed background tasks that persist across app restarts\n\n\n[ ] Implementing a ViewModel to ensure data is passed between activities efficiently.\n\nViewModel is not for background tasks\n\n\n[ ] Background tasks should always run on the main thread to keep the process active.\n\nIt causes UI freezes and ANR (Application Not Responding) errors\n\n\n[x] Leveraging JobScheduler for periodic tasks and taking advantage of batching and timing constraints.\n\n(2 marks)\n\n\nIn a file download manager, you need to download large files in the background while the user performs other tasks. If the service gets interrupted (e.g., due to low memory), it should restart and resume the download when resources are available. Which settings and methods should you use for this type of service?\n\n\n[x] Use a Foreground Service with a persistent notification to keep the user informed.\n\nForeground services run with higher priority, reducing the likelihood of being killed\n\n\n[x] Use startService() and set the service as START_STICKY.\n\nIf the service is killed, START_STICKY automatically restarts it\n\n\n[ ] Use startService() and set the service as START_NOT_STICKY.\n\nSTART_NOT_STICKY means the service will not restart if killed\n\n\n[x] Set the service to START_REDELIVER_INTENT to retry the download if interrupted.\n\nThis ensures that if the service is killed, it restarts with the same intent, resuming the download\n\n\n[ ] Use an IntentService to manage multiple concurrent downloads.\n\nIntentService stops itself after completing a task and cannot handle concurrent downloads\n\n\n\n(2 marks)\n\n\nIn a news app, you want to ensure the app functions correctly across various devices and Android versions. To simulate real-world usage, which testing approach and tools would be most effective?\n\n\n[ ] Run unit tests on a local JVM without Android dependencies.\n\nUnit tests do not simulate real user interactions or device-specific behavior\n\n\n[x] Use Monkey Stress Test to generate random user events on the app.\n\nThe Monkey tool generates random UI interactions to simulate real-world unpredictable usage\n\n\n[ ] Perform manual testing only on a single device.\n\nAn app must be tested on multiple devices and Android versions for reliability\n\n\n[x] Use Espresso for automated UI testing across emulators and devices.\n\nEspresso allows automated UI testing\n\n\n[ ] Use logcat to monitor app behavior under stress conditions.\n\nlogcat helps debug issues, but it does not automate testing or simulate real-world usage\n\n\n\n(2 marks)\n\n\nIn a smart home app, you want to communicate with a remote service managing multiple connected devices, each needing real-time commands and status updates. Which method allows you to efficiently send commands and receive responses to this remote service?\n\n\n[ ] Start a new Activity for each device’s commands.\n\nActivities are for UI, not for managing background communication with devices\n\n\n[ ] Use a local Bound Service to manage each device’s status updates.\n\nA local Bound Service is only accessible within the same app process\n\n\n[x] Implement AIDL to handle commands and callbacks for multiple devices.\n\nAIDL allows multiple clients\n\n\n[ ] Use IntentService to periodically check the device statuses.\n\nIntentService is not designed for real-time updates because it stops after execution\n\n\n[ ] Use a Messenger to send data and commands in a single-threaded fashion.\n\nInefficient for real-time multi-device communication\n\n\n\n(2 marks)\n\n\nIn a weather app, you want to periodically check for new weather updates even if the user has exited the app. Which Android component and technique would allow you to accomplish this without requiring the user’s continuous interaction?\n\n\n[x] Use WorkManager to handle periodic tasks in a background-friendly way.\n[ ] Use startService() to run the service indefinitely.\n\nStarting from Android 8.0 (Oreo), background services started with startService() will be stopped by the system within minutes unless running in the foreground\n\n\n[ ] Use an IntentService that checks periodically.\n\nIntentService processes one request at a time and stops when finished\n\n\n[ ] Use a Bound Service that interacts with the main activity.\n\nA bound service cannot run in the background when the user exits the app\n\n\n[ ] Implement a Background Service to handle periodic checks.\n\nA background service consumes too many resources and may be killed by the system\n\n\n\n(2 marks)\n\n\nIn an employee management app, you want to add a feature where users can view an employee’s profile in a new screen. From this profile screen, they should be able to edit details and save changes before returning to the main employee list with updated information. Which methods will enable this functionality?\n\n\n[x] Use setResult() in the profile screen and check the result on the employee list screen.\n[x] Use finish() on the edit screen to return to the profile view.\n[ ] Use startActivity() to open the profile screen, and save changes within it.\n[x] Use startActivityForResult() to open the profile screen and return any updates.\n[x] Pass updated data through Intent extras to the main employee list screen.\n\n(2 marks)\n\n\nYou are building a recipe app that displays a list of recipes. When a recipe is selected, the app should open a new screen with the recipe details. If users press the back button, they should return to the recipe list. What is the best way to handle this scenario?\n\n\n[x] Use startActivity() to open the recipe details screen.\n[ ] Use startActivityForResult() so the recipe list knows when a recipe has been viewed.\n\nNo data needs to be returned\n\n\n[ ] Implement the recipe list and details on separate XML layouts within a single activity.\n\nUsing separate activities is preferred\n\n\n[ ] Pass data back to the main list screen by setting a result in the details screen.\n\nNo need to pass data back\n\n\n[x] Use finish() to close the details screen and return to the recipe list.\n\n(2 marks)\n\n\nYou are developing an app that allows users to upload images. Uploads may take a few seconds, and you want to ensure the UI remains responsive. How should you handle the image upload to avoid blocking the main thread?\n\n\n[ ] Start a new activity for the upload process.\n\nAn upload operation should be handled in the background\n\n\n[ ] Run the upload on the main (UI) thread since it’s a short operation.\n\nNetwork operations should never run on the main thread, as it can freezing UI and cause ANR errors\n\n\n[x] Use AsyncTask to manage the upload and update the UI on completion.\n\nAsyncTask runs tasks in the background and updates the UI once completed\n\n\n[ ] Use Activity.runOnUiThread() to handle the upload directly from the UI thread.\n\nThis will still block the UI thread since the upload is a network operation\n\n\n[x] Use a Handler and Looper to post the upload operation to a background thread.\n\nA Handler allows posting background tasks to a worker thread while keeping the UI responsive\n\n\n\n(2 marks)\n\n\nYou have implemented a service in your Android app, but it doesn’t start when you expect it to. What could be the reason?\n\n\n[x] The service is not declared in the AndroidManifest.xml file.\n\nAll services must be explicitly declared in AndroidManifest.xml before they can be started\n\n\n[ ] The service is declared but not exported, so it cannot start.\n\nExporting is only needed if other apps need to start the service. If it’s for internal use, this is not required\n\n\n[x] The service is using startForeground(), but no notification is provided.\n\nAndroid requires foreground services to show a persistent notification\n\n\n[ ] The service is trying to bind before being started with startService().\n\nA Bound Service does not require startService()\n\n\n[x] The service is running in the background on Jetpack 8.0+ without proper handling of background execution limits.\n\nStarting Android 8.0 (Oreo, API 26), background services cannot run freely unless:\n\nThe app is in the foreground.\nThe service is promoted to a foreground service\n\n\n\n\n\n(2 marks)\n\n\nYour app needs to communicate with Android’s Power Manager to set power-saving parameters. Which approach will let you access and control this system service for power management?\n\n\n[ ] Set up a local service with a Messenger for managing power-related settings.\n\nA local service does not provide access to system-level power settings\n\n\n[x] Use the PowerManager system service via the ServiceManager.\n[ ] Create an IntentService that periodically updates power settings.\n\nIt cannot control system power settings directly\n\n\n[ ] Utilize Binder directly to call PowerManager functions.\n\nPowerManager is already exposed via a system service, so there’s no need to use Binder\n\n\n[ ] Implement AIDL for handling all communication with the PowerManager.\n\nAIDL is used for defining remote services, but PowerManager is a system service that is already available in the Android framework\n\n\n\n(2 marks)\n\n\nYou’re building a news app where users can read articles. You want the app to remember the scroll position in an article if the device is rotated, so users can resume reading from where they left off. Which method should you use to achieve this?\n\n\n[x] Override onSaveInstanceState() to store the scroll position in a Bundle.\n\nIt stores UI-related data in a Bundle, which is restored when the activity is recreated\n\n\n[x] Use the ViewModel to store the scroll position across activity re-creations.\n\nViewModel survives configuration changes (like rotation)\n\n\n[ ] Use a singleton class to hold the scroll position.\n\nSingletons persist longer than needed, potentially holding old data when the user reads a different article\n\n\n[ ] Store the scroll position in SharedPreferences and reload it on activity restart.\n\nIt is for persistent storage, but the scroll position is temporary UI state, not long-term data\n\n\n[ ] Save the scroll position in a file and reload it on configuration change.\n\nSaving UI state in a file is overkill and unnecessary\n\n\n\n(2 marks)\n","slug":"Android Exams","date":"2025-08-28T07:57:00.000Z","categories_index":"Notes","tags_index":"Mobile,Android","author_index":"Huaji1hao"},{"id":"bcc935a25dfebbd1b850f330cfc413db","title":"Mobile Development (Android)","content":" Android Kernel Modifications\n\nWakelock: Prevents CPU/screen sleep for power management.\nBinder IPC: Efficient inter-process communication.\nLow Memory Killer: Manages process termination in low-memory scenarios.\nAshmem: Anonymous shared memory for inter-process data sharing.\nAlarm Driver: Wakes device for scheduled tasks.\nOOM Handling: Adjusts process priority dynamically.\n\n System on a Chip (SoC)\n\nCPU + Other Components: Uses only part of the transistors for CPU, rest for peripherals.\nIntegration: Combines CPU, GPU, NPU, DSP, Modem, etc.\nAdvantages:\n\nReduces communication overhead &amp; heat.\nOptimized computation and storage.\n\n\nDisadvantages:\n\nFabrication cost\nIncreases complexity\n\n\n\n Package on Package (PoP)\n\nConcept: Stacks RAM directly on SoC.\nBenefits:\n\nSaves PCB space.\nImproves speed by reducing memory access time.\nAllows independent production of CPU and RAM.\n\n\n\n ARM and Thumb\n\nWhy Thumb?\n\nARM uses 32-bit instructions, but not all need full 32 bits.\nThumb Mode: 16-bit compact instructions, improving memory usage and execution efficiency\nExecution Optimization: A single 32-bit fetch can load two 16-bit Thumb instructions.\n\n\n\n Big.LITTLE Architecture\n\nConcept: ARM’s solution to combine high performance with energy efficiency by integrating both powerful (big) and efficient (LITTLE) cores\nBig Core:\n\nHigh power, high performance.\n\n\nLITTLE Core:\n\nLower power, optimized for energy efficiency.\n\n\nDynamic Task Management:\n\nSystem switches cores based on workload to optimize efficiency.\n\n\n\n The Activity Lifecycle\n\n\n\nMethod\nDescription\n\n\n\n\nonCreate()\nRequired. The “constructor” for the component. Set up UI, initialize resources.\n\n\nonStart()\nActivity becomes visible to the user.\n\n\nonResume()\nActivity is in the foreground and interactive. Stays here until interrupted.\n\n\nonPause()\nUser is leaving the Activity. Stop non-essential processes (e.g., camera preview).\n\n\nonStop()\nActivity is no longer visible. Release resources, save data to storage.\n\n\nonDestroy()\nActivity is being destroyed, either due to user exit or configuration change.\n\n\n\n Intents\n\n\nIntent: Describes an operation (Action + Data URI) for Activity communication.\n\n\nExplicit Intent: Starts a specific Activity.\n12Intent intent = new Intent(this, SecondActivity.class);startActivity(intent);\n\n\nImplicit Intent: System resolves the best Activity.\n12Intent intent = new Intent(Intent.ACTION_VIEW, Uri.parse(&quot;http://example.com&quot;));startActivity(intent);\n\n\nintent-filter in AndroidManifest.xml determines what an Activity can handle.\n\n\n Types of Services\n Bound Service\n\nTightly coupled with components like Activity.\nLifespan: Exists as long as a component is bound to it.\nUse case: Media player where UI needs to control playback.\n\n Unbound (Started) Service\n\nLoosely coupled, runs independently in the background.\nLifespan: Not tied to any component; runs until explicitly stopped.\nUse case: A file download service that continues even after Activity exits.\n\n Foreground Service\n\nVisible to the user, requires a persistent notification.\nLifespan: Runs as long as the operation is active.\nUse case: Navigation apps, music players, fitness tracking.\n\n Background Service\n\nInvisible to the user, performs background tasks.\nLifespan: Can be terminated by the system under memory pressure.\nUse case: Syncing data, periodic background updates.\n\n IntentService (Deprecated)\n\nExecutes tasks asynchronously on a worker thread.\nLifespan: Stops itself after completing all tasks.\nUse case: Email sending, file uploads, one-time background tasks.\n\n\n\n\nType\nUse Case\nLifespan\nInteraction\n\n\n\n\nForeground Service\nOngoing tasks like music, fitness tracking\nUntil stopped\nCan be bound\n\n\nBackground Service\nSyncing, downloading\nCan be killed by system\nCan be bound\n\n\nBound Service\nUI interaction (e.g., media player)\nExists while bound\nDirect calls via Binder\n\n\nIntentService (Deprecated)\nOne-time background tasks\nStops after task\nRarely bound\n\n\nWorkManager\nBackground scheduling (sync, upload)\nManaged by system\nNo direct interaction\n\n\n\n Service Lifecycle\n\n\n\nMethod\nPurpose\n\n\n\n\nonCreate()\nInitializes the service.\n\n\nonStartCommand()\nCalled when using startService().\n\n\nonBind()\nCalled when using bindService().\n\n\nonDestroy()\nCalled when the service is stopped.\n\n\n\n 2. Binder Mechanism\n Binder Overview\n\nLightweight RPC system implemented via Linux kernel driver.\nMarshalling &amp; Unmarshalling: Converts complex objects into primitives for transmission.\nThread pool management: Handles concurrent IPC requests.\n\n 3. Messenger Protocol\n Concept\n\nMessenger acts as a postman delivering messages between processes.\nHandler &amp; Looper manage incoming messages in the destination process.\n\n How It Works\n\nProcess A (Client) sends a Message to Process B (Service).\nProcess B receives the Message via Handler.\nLooper processes messages sequentially in the main thread.\n\n Messenger Features\n\nUses a single-threaded message queue.\nData is passed via Message objects, not method calls.\nBi-directional communication possible using replyTo.\n\n\n 4. Messenger vs. AIDL\n\n\n\nFeature\nMessenger\nAIDL\n\n\n\n\nBest for\nSimple IPC\nComplex IPC\n\n\nThreading\nSingle-threaded\nMulti-threaded\n\n\nData Types\nPrimitive, Bundle\nSupports Parcelable\n\n\nBi-directional\nNeeds extra Messenger\nNative callback support\n\n\n\n\nMessenger: Simple message-based IPC, best for one-way interactions.\nAIDL: Supports complex APIs, high performance, and custom objects.\n\n\n 5. AIDL (Android Interface Definition Language)\n Why AIDL?\n\nDefines remote interfaces for complex IPC.\nAutomatically generates Proxy &amp; Stub classes to simplify communication.\nSupports multi-threading.\n\n AIDL Workflow\n\nDefine AIDL interface (.aidl file).\nAIDL compiler generates Java code (Proxy &amp; Stub).\nServer implements Stub and registers service.\nClient retrieves service via bindService().\n\n AIDL Features\n\nSupports method calls across processes.\nAllows callbacks using in, out, inout parameters.\nUses Parcelable for complex objects.\n\n 6. Parcelable (Efficient IPC Object Serialization)\n Why Parcelable?\n\nFaster than Serializable, optimized for Android.\nExplicit serialization logic avoids reflection overhead.\nIdeal for IPC where performance matters.\n\n Parcelable vs. Serializable\n\n\n\nFeature\nParcelable\nSerializable\n\n\n\n\nSpeed\nFaster\nSlower (uses reflection)\n\n\nFlexibility\nRequires manual implementation\nAutomatic\n\n\nUse Case\nAndroid IPC (Intent, AIDL)\nGeneral Java serialization\n\n\n\n 📂Storage in Android\n 🔹 Storage Principles\n\nApp specific requirements\n\nStore files that are meant for our use only\nStore files that we intend to share with other apps\nStore private, primitive data\nStore private, structured data\n\n\nMobile storage principles\n\nEfficiency and sharing vs privacy and security\n\nLimited storage capability\nHighly personal information\n\nContacts, messages, activity\nProbably do not want other apps to “just” be able to access it\n\n\n\n\nMedia\n\nImages, videos, music\n\n\n\n\n\n 🔹 Logical Storage Types\n\n\nInternal Storage:\n\nPrivate to the app.\nKernel-enforced security.\nDeleted upon app uninstallation.\nIncludes SharedPreferences, SQLite, and cache files.\n\n\n\nExternal Storage:\n\nLarger space, possibly removable (e.g., SD card).\nPublic access; other apps can read/write (requires permission).\nScoped storage (Android 10+) limits app access to media collections.\n\n\n\n 🔹File-based abstractions\n\n\nShared Preferences:\n\nKey-value storage for primitive data (strings, integers).\nNot suitable for persistent large data.\n\n\n\nDatabases in Android\n\nSQLite: A self-contained SQL database for structured data.\nSQLiteOpenHelper:\n\nManages database creation and upgrades.\nProvides methods for readable and writable database access.\n\n\nCursors:\n\nQuery results are returned as a Cursor object.\nmoveToFirst(), moveToNext() allow navigation.\ngetString(columnIndex), getInt(columnIndex) for data retrieval.\n\n\n\n\n\nFile-based storage\n\n\n\n 🎭 MVVM Architecture\n 🔹 Key Components\n\nModel: Database (Room).\nViewModel: Holds UI-related data.\nView: Activity/Fragment displaying data.\n\n 🔹 Benefits\n\nSeparation of concerns.\nLifecycle-aware architecture.\nUses LiveData for reactive UI updates.\n\n 🔄 Content Providers\n 🔹 Purpose\n\nAllows sharing structured data between apps.\nProvides a database-like interface using URIs.\nEnables Inter-Process Communication (IPC).\n\n 🔹 System Content Providers\n\nBrowser (history, bookmarks).\nCall log (phone records).\nContacts (address book).\nMediaStore (images, videos, music).\n\n What is a Broadcast?\n\nA messaging system used by Android to notify applications about system-wide or app-specific events.\nSent as Intents and can be:\n\nImplicit (delivered to any app that has registered for it).\nExplicit (targeted at a specific component).\n\n\n\n 1. Purpose of Permissions\n\nProtect user privacy and system security.\nRestrict access to sensitive data (e.g., contacts, location) and actions (e.g., sending SMS, recording audio).\nApps have no access by default unless granted by the user or system.\n\n 6. Component-Level Permissions\n (1) Restricting Component Access\n\nPermissions can be applied to Activity, Service, ContentProvider, and BroadcastReceiver.\nExample restrictions:\n\nActivity: Limits which apps can start it.\nService: Controls which apps can bind/start/stop it.\nContentProvider: Controls read/write access.\nBroadcastReceiver: Prevents unauthorized apps from listening.\n\n\n\n (2) Custom Permissions\n\nApps can define their own permissions (&lt;permission&gt;).\nNot recommended unless absolutely necessary (e.g., middleware services).\nUse signature protection level for internal API access.\n\n 2. Battery Consumption Analysis\n Dynamic State Tracking\n\nWi-Fi Status: Tracks active/inactive states.\nScreen Brightness: Monitors brightness levels.\nBatteryStats Service: Records and analyzes power usage at state transitions.\n\n CPU Utilization Tracking\n\nMonitors app CPU runtime to measure power consumption accurately.\n\n 3. Android Power Management States\n\n\n\nState\nDescription\n\n\n\n\nG0 (Active)\nDevice fully operational.\n\n\nG1 (Sleep Modes)\nMultiple sleep levels for power conservation.\n\n\nS1\nCPU idle but powered.\n\n\nS2\nCPU off, cache cleared.\n\n\nS3 (Standby)\nRAM remains powered for fast wake-up.\n\n\nS4 (Hibernate)\nSystem state saved to disk, RAM powered off.\n\n\nG2 (Soft Off)\nMinimal power consumption.\n\n\nG3 (Mechanical Off)\nFull shutdown, no power.\n\n\n\n Wake Locks\n\nPrevents CPU from sleeping when necessary.\nDifferent types control screen and CPU wake-up behavior.\nEfficient use is crucial to prevent unnecessary power drain.\n\n 5. Android Battery Optimization Features\n App Standby\n\nTriggers when an app is inactive (no foreground tasks, no launch history).\nRestrictions:\n\nNo background network access (except a daily maintenance window).\nSuspended background jobs.\nWake locks &amp; alarms allowed for brief tasks.\n\n\nExit conditions: User opens the app or connects to power.\n\n Doze Mode\n\nTriggers when the device is idle (screen off, stationary, battery-powered).\nRestrictions:\n\nNo network access.\nCPU tasks halted.\nWake locks ignored.\nOnly high-priority notifications allowed.\n\n\nExit conditions: User interaction, motion detection, alarm events.\n\n 6. Firebase Cloud Messaging (FCM) &amp; Battery Optimization\n\nEfficient Messaging: Optimized server-device communication.\nHigh-Priority Messages: Bypass Doze &amp; App Standby.\nSmart Delivery: Reduces battery drain via network batching.\nBattery-Friendly: Limits device wake-ups.\n\n 7. Sustained Performance Mode (SPM)\n\nPrevents thermal throttling in high-performance applications (games, VR).\nEnsures stable performance over long periods.\nDeactivates automatically when the app moves to the background to conserve power.\n\n Revision\n Difference Between a Started Service and a Bound Service\n\nStarted Service: Launched with startService(), runs in the background until explicitly stopped (e.g., file downloads).\nBound Service: Initiated with bindService(), allows two-way communication with components like activities via a client-server interface.\n\n Lifecycles\n\nStarted Service:\n\nLifecycle: onCreate() → onStartCommand() → Runs independently → Ends with stopSelf() or stopService() → onDestroy().\n\n\nBound Service:\n\nLifecycle: onCreate() → onBind() → Active while clients are bound → onUnbind() (when last client disconnects) → onDestroy().\n\n\n\n Communication Mechanisms with Activities\n\nStarted Service: Uses Intents, Handlers, or Broadcast Manager for communication.\nBound Service: Provides a direct interface using an IBinder object, allowing activities to call its methods directly.\n\n Appropriate Scenarios\n\nStarted Service: Suitable for independent tasks like data processing or file downloads.\nBound Service: Ideal for tasks requiring interaction with the UI, like music players or GPS tracking.\n\n Ensuring Long-Running Tasks Persist\n\nUse a Foreground Service by calling startForeground() and associating a notification to keep the service active.\n\n Two Golden Rules of the Single Thread Model in Android UI Framework\n\nDo not block the UI thread.\n\nThe UI thread should always remain responsive; long-running operations (e.g., network calls, database queries) should be performed on background threads.\nExample: Performing a network request using AsyncTask or Kotlin coroutines (Dispatchers.IO) instead of executing it directly on the UI thread.\n\n\nDo not access the UI from a background thread.\n\nUI components must be updated only from the UI thread to avoid crashes (CalledFromWrongThreadException).\nUse mechanisms like Handler, runOnUiThread(), or LiveData to safely update UI from background threads.\n\n\n\n Does Android Benefit from Linux’s Virtual Memory Capabilities?\n Answer: Yes, Android still benefits from Linux’s virtual memory capabilities.\n Justification:\n\n\nMemory Protection and Isolation\n\nEach Android application runs in its own process with a unique user ID, ensuring process isolation and preventing unauthorized access to another app’s memory.\n\n\n\nPaging Mechanism\n\nEven though Android does not typically use swapping, it still benefits from Linux’s demand paging, allowing apps to load only the necessary memory pages into RAM, reducing initial memory footprint.\n\n\n\nKernel-Level Memory Management\n\nAndroid leverages memory overcommitment and page sharing, where read-only system libraries are shared among processes using shared memory pages, optimizing RAM usage.\n\n\n\nOOM (Out of Memory) Killer\n\nInstead of swapping, Android uses the Out of Memory (OOM) killer, a Linux kernel feature that selectively terminates background or less important processes to free up RAM.\n\n\n\nZRAM Compression\n\nMany Android devices use ZRAM (compressed RAM swap), where a portion of RAM is used as a compressed swap space, effectively extending usable memory while avoiding the high I/O cost of disk-based swapping.\n\n\n\n Conclusion:\nEven without traditional disk-based swapping, Android benefits from Linux’s virtual memory features such as paging, memory protection, shared libraries, and ZRAM compression, ensuring efficient memory management and system stability.\n Battery Life Enhancement in Android\n\nApp Standby:\n\ndefer background activity for apps with no recent user interaction.\n\n\nDoze:\n\ndeep sleep if user has not actively used the device for extended periods of time\n\n\nExemptions:\n\nsystem apps and cloud messaging services preloaded on phone are exempted from App Standby &amp; Doze.\n\n\n\n 4 Main Components\n\nActivities\n\nUI components\n\n\nServices\n\nMechanism for doing something long-running in the background(c.f. front-end/ back-end)\n\n\nBroadcast Receivers\n\nRespond to broadcast messages from the OS/other apps\n\n\nContent Providers\n\nMake use of data from other apps\n\n\n\n Android Operation System - specific modifications\n\nwake locks\nBinder IPC\nLow memory killer\n\n","slug":"Mobile Key Note","date":"2025-08-28T07:56:00.000Z","categories_index":"Notes","tags_index":"Mobile,Android","author_index":"Huaji1hao"},{"id":"d7d61e6e12cae8825c6398bc1542d379","title":"Machine Learning","content":" Introduction\n What is Machine Learning?\n\n\n\n\n\n\n\n\n\nField of study that gives computers the ability to learn without being explicitly programmed.\nArthur Samuel 1959\n Key elements:\n\nA collection of examples\nAlgorithm to “learn”\n\n Aim of Machine Learning\n\n\n\n\n\n\n\n\n\nTrain machines to explore useful features and learn data patterns to predict/infer a target event.\n\n\n\nSupervised Learning\nDevelop predictive model based on both features &amp; labels\n\nClassification - output a label of class\nRegression - output a value in a continuous range\n\n\n\nUnsupervised Learning (Clustering)\n\nGroup &amp; interpret data based only on features\n\n\n\nReinforcement Learning\n\nLearn a policy to execute actions in an environment\n\n\n\n General Pipeline of Machine Learning\n\n\n\n\n\n\n\n\n\nData representation + Modelling + Evaluation + Optimisation\nPedro Domingos, 2012\n\n Some basic questions\n\n\nWhat could be the source of data?\n\nData can come from various sources like sensors, databases, web scraping, logs, APIs, surveys, or simulations.\n\n\n\nHow do we represent the data?\n\nData can be represented as structured formats like tabular data (rows and columns) or unstructured formats like text, images, and videos.\nFeatures are categorized as categorical, binary, or continuous, often requiring encoding or normalization.\n\n\n\nWhat is good data in the context of machine learning?\n\nGood data is accurate, relevant, diverse, complete (minimal missing values), consistent, and representative of the target use case to ensure reliable model training and testing.\n\n\n\nWhat makes a good feature?\n\nA good feature is relevant, informative, discriminative, and repeatable, helping the model accurately predict outcomes while reducing noise and redundancy.\n\n\n\n Linear Algebra\nAll data/features are represented as vectors, matrices and tensors.\n\n\nWe denote a (column) vector with NNN elements as x=[x1,x2,…,xN]T∈RN\\mathbf{x} = [x_1, x_2,…, x_N]^T∈ R^Nx=[x1​,x2​,…,xN​]T∈RN\n\n\nMatrix in ML is frequently used to represent features (N) of observations (M), as in the form of\nX=[x1,1⋯x1,N⋮⋱⋮xM,1⋯xM,N]\\mathbf{X} =\n\\begin{bmatrix}\nx_{1,1} &amp; \\cdots &amp; x_{1,N} \\\\\n\\vdots &amp; \\ddots &amp; \\vdots \\\\\nx_{M,1} &amp; \\cdots &amp; x_{M,N}\n\\end{bmatrix}\nX=⎣⎢⎢⎡​x1,1​⋮xM,1​​⋯⋱⋯​x1,N​⋮xM,N​​⎦⎥⎥⎤​\n\n\nSuppose that we have MMM measurements with M&gt;NM &gt; NM&gt;N. Then, XXX has dimension of M×NM×NM×N. In this case, XXX cannot be inverted as it’s not a square matrix. To solve www, we need a small trick as follows:\n\nXw=yXw=yXw=y\nMultiply XTX^TXT on both sides, we have\nXTXw=XTyX^TXw = X^TyXTXw=XTy\nThen XTXX^TXXTX is a square matrix, which is invertible.\nw=(XTX)−1XTyw = (X^TX)^{-1}X^Tyw=(XTX)−1XTy\n\n\n\nThis is equivalent to minimise mean squared error: ∣∣Xw−y∣∣2||Xw-y||^2∣∣Xw−y∣∣2 , called least squared\n\n\n Differentiation\n\n\nA derivative f’(x)f’(x)f’(x) of a function f(x)f(x)f(x) is a function or a value that describes how fast f(x)f(x)f(x) changes (decrease or increase). The process of finding the derivative is differentiation.\n\n\nA vector of partial derivatives is also called gradient\n\n\nf(x,y)=3x2+5y3+10f(x, y)= 3x^2+5y^3+10f(x,y)=3x2+5y3+10\n\n\n∂f∂x=6x,∂f∂y=15y2\\frac{\\partial f}{\\partial x} = 6x,  \\frac{\\partial f}{\\partial y} = 15y^2∂x∂f​=6x,∂y∂f​=15y2\n\n\nThe gradient of function fff is denoted as ∇f∇f∇f, given by the vector [∂f/∂x,∂f/∂y][∂f/∂x, ∂f/∂y ][∂f/∂x,∂f/∂y]\n\n\nThe chain rule can also be used for partial derivatives.\n\n\n\n\n Least Square Estimation (closed form solution)\n\\begin{align}\nf(w) &amp;= \\|Y - Xw\\|^2 \\\\\n&amp;= (Y - Xw)^T(Y - Xw) \\\\\n&amp;= Y^TY - Y^TXw - w^TX^TY + w^TX^TXw\n\\end{align}\n\n\nCalculate the first order derivative,\n∂f(w)∂w=−2XTY+2XTXw\\frac{\\partial f(w)}{\\partial w} = -2X^TY + 2X^TXw \n∂w∂f(w)​=−2XTY+2XTXw\n\n\nSet ∂f(w)∂w=0\\frac{\\partial f(w)}{\\partial w} = 0∂w∂f(w)​=0, we have $X^TY = X^TXw $\n\n\nWe can obtain:  w=(XTX)−1XTYw = (X^TX)^{-1}X^TYw=(XTX)−1XTY\n\n\n Gradient Descent Optimisation\n\n\n\n\n\n\n\n\n\nGradient descent is an iterative optimisation algorithm used to find a local minimum or maximum of a given function.\nAim: to find parameter w∗=[a∗,b∗]w^*=[a^*,b^*]w∗=[a∗,b∗] for a line that best fit these points (x,y)(x, y)(x,y)\n\n\na,ba, ba,b : parameter to be estimated\n\n\nkkk : iteration\n\n\nNNN : number of data samples\n\n\nααα: learning rate\n\n\nloss function: f(a,b)=∑n=1N(yn−(axn+b))2f(a,b)=\\sum\\limits_{n=1}^N(y_n-(ax_n+b))^2f(a,b)=n=1∑N​(yn​−(axn​+b))2\n\n\nCalculate partial derivatives:\n\\begin{align*} \\frac{\\partial f}{\\partial a^k} &amp;= -2 \\sum_{n=1}^N (y_n - a^k x_n - b^k) \\cdot x_n \\\\ \\frac{\\partial f}{\\partial b^k} &amp;= -2 \\sum_{n=1}^N (y_n - a^k x_n - b^k) \\end{align*}\n\n\n Demo\n\n\nInitialise a, b, 𝛼 when k=1k=1k=1\n\n\nIteratively update 𝑎𝑎a and 𝑏𝑏b: \\begin{align*} a^{k+1} = a^k - \\alpha \\frac{\\partial f}{\\partial a^k},  b^{k+1} = b^k - \\alpha \\frac{\\partial f}{\\partial b^k} \\end{align*}\n\n\nIterate until f(ak,bk)f(a^k,b^k)f(ak,bk) is small enough or k&gt;maxk&gt;maxk&gt;max iteration\n\n\n Generalised to multiple variables\n\n Requirements and Limitations\n\n\nThe function to be optimised is differentiable at each point of x (no discontinuity).\n\n\nThe function is convex: the line segment connecting two function’s points lay above the curve (does not cross it)\n\n\n Probability- Random Variable\n\n\n\n\n\n\n\n\n\nIt is a variable X whose possible values are numerical outcomes of a random process.\nIt can be discrete or continues.\n      \n Discrete\nThe probability distribution of a discrete random variable is a list of probabilities associated with each of its possible values (called probability mass function)\n\n\nThe sum of all probabilities of all possible values is 1\n\n\nLet XXX have kkk possible values {xi}i=1k\\{x_i\\}_{i=1}^k{xi​}i=1k​\n\n\nThe expectation E[X]\\mathbb{E}[X]E[X] is : E[X]≔∑i=1kxiPr⁡(X=xi)=x1Pr⁡(X=x1)+x2Pr⁡(X=x2)+⋯+xkPr⁡(X=xk)\\mathbb{E}[X] \\coloneqq \\sum\\limits_{i=1}^k x_i \\Pr(X = x_i) = x_1 \\Pr(X = x_1) + x_2 \\Pr(X = x_2) + \\cdots + x_k \\Pr(X = x_k)E[X]:=i=1∑k​xi​Pr(X=xi​)=x1​Pr(X=x1​)+x2​Pr(X=x2​)+⋯+xk​Pr(X=xk​)\n\n\n Continuous\nThe probability distribution of a continuous random variable is described by a probability density function\n\n\nThe area under the curve is 1\n\n\nThe expectation of a continuous random variable XXX is given by:\nE[X]≔∫RxfX(x) dx\\mathbb{E}[X] \\coloneqq \\int_{\\mathbb{R}} x f_X(x) \\, dxE[X]:=∫R​xfX​(x)dx\n\n\nWe don’t know fXf_XfX​ in most cases in ML. We estimate it by collecting samples.\n\n\n Bayes’ Rule\nIn many practical problems, the probability of one event occurs is often conditioned on another event.\nTaking account of priors!\n\n\nThe conditional probability P(X=x∣Y=y)P(X=x|Y=y)P(X=x∣Y=y) is the probability of the random variable XXX to have a specific value xxx given that another random variable YYY has a specific value of yyy.\n\n\nThe Bayes’ Rule is: P(A∣B)=P(B∣A)⋅P(A)P(B)P(A|B)=\\frac{P(B|A)\\cdot P(A)}{P(B)}P(A∣B)=P(B)P(B∣A)⋅P(A)​\n\n\nNote that the joint probability can be factorisedin two ways:\n\nP(A,B)=P(A∣B)⋅P(B)=P(B∣A)⋅P(A)P(A,B)=P(A|B)\\cdot P(B)=P(B|A)\\cdot P(A)P(A,B)=P(A∣B)⋅P(B)=P(B∣A)⋅P(A)\n\nIf AAA and BBB are independent (do not affect each other), P(A,B)=P(A)⋅P(B)P(A,B)=P(A)\\cdot P(B)P(A,B)=P(A)⋅P(B)\n Frequentist vs Bayesian\nFrequentist:\n\nLong-term frequency of an event occurring.\nData driven.\n\nBayesian:\n\nAssumes a prior model (knowledge) and update the model using data.\nModel driven.\n\n Data Pre-processing &amp; Feature Analysis\n\nUnderstand the data\n\nTruly understand the underlying problem as much as possible.\nVisualise the data (outliers, value range, etc.)\n\n\nFeature representation\n\nReliability, repeatability\nCategorical, binary, continuous.\nFeature value normalisation\n\n\nData pre-processing\n\nMissing data, errors, etc.\nData imputation\n\n\n\n Data Normalisation\nMethods:\n\n\nZ-Normalisation (Zero-mean normalisation)\n\nXnorm=(X−μ)/σX_{norm}=(X-\\mu)/\\sigmaXnorm​=(X−μ)/σ\nXXX is the feature vector of original values, μμμ and σσσ are the mean and standard deviation of vector XXX\n\n\n\nMin-Max Normalisation\n\nuse 5%/95% percentile as the min and max more robust to outliers\nXnorm=X−min(X)max(X)−min(X)X_{norm}=\\frac{X-min(X)}{max(X)-min(X)}Xnorm​=max(X)−min(X)X−min(X)​\nMin(X)Min(X)Min(X) and Max(X)Max(X)Max(X) are the minimum and maximum values of vector XXX respectively\n\n\n\nVector Normalisation\n\nXnorm=X∣∣x∣∣X_{norm}=\\frac{X}{||x||}Xnorm​=∣∣x∣∣X​\n∣∣X∣∣||X||∣∣X∣∣ is the vector length of XXX\n\n\n\nBenefit of data normalisation\n\nThey are all linear scaling methods, so won’t affect the original data distribution\nImproves the numerical stability of the machine learning model\n\ne.g. using gradient descend to optimise neural network, if different features are in different value range a fixed learning rate will likely overshoot or undershoot the optima for certain features\n\n\nReduce the negative impact to distance-based algorithms\n\ne.g. KNN, SVM\n\n\n\n Data Imputation (deal with missing data)\nMethods:\n\nUse mean or median values\nUse most frequent values\nUse k-nearest neighbour based on feature similarity\nUse multivariate imputation by chained equations\n\ni.e. try filling the missing data multiple times with different values and pool the final results\n\n\nEstimate the missing value using machine learning models based on other features.\n\n Curse of Dimensionality\nWhat is curse of dimensionality in machine learning?\n\n\n\n\n\n\n\n\n\nData samples are too sparse in the feature space.\ni.e. the number of instances are not large enough to densely distributed in the feature space\nWhat shall we do to avoid curse of dimensionality?\n\nIncrease the number of data samples.\n\nThe required sample numbers will need to increase exponentially with a linear increase of feature dimensions.\n\n\nReduce the number of features (more feasible)\n\nuse feature selection and dimensionality reduction methods\n\n\n\n Feature Selection &amp; Dimensionality Reduction\n Aim of Feature Selection and Dimensionality Reduction\n\nReduce the impact caused by Curse of Dimensionality\nRemove redundant features to improve performance\nIncrease computational efficiency\nReduce cost in new data acquisition\n\n Feature Selection vs Dimensionality Reduction\n\nFS retain a subset of the original features\nDR generate a new set of features that is compact but does NOT retain the original meaning of features\n\n Things to consider when using FS and DR\n\nThe target dimension\nInterpretability (Yes: FS; No: Dr or FS)\nFeature correlations/dependency\nFeature reliability and repeatability\nMethods (different methods likely to result in different features)\n\n Popular FS Methods\n\nWrapper methods\n\nSearch for optimal feature subset that maximise the decision-making performance\nMethods: recursive feature elimination; sequential feature selection.\n\n\nEmbedded methods\n\nIntegrate the FS process to the model learning process.\nMethods: ridge (ElasticNet); lasso; random forest (feature ranking).\n\n\nFilter-based methods\n\nSelection is based on feature relationships and statistics rather than performance.\nMethods: univariate (ANOVA); Chi Square; correlation/variance\n\n\n\n Methods\n Forward Feature Selection (Wrapper Method)\n\nXXX: the final selected feature set\nBBB: is stored best evaluation metric value\nYYY: the selected feature set at each iteration\nMMM: The evaluation metric, e.g. Entropy; Classification rate; Regression error\nRecursive feature elimination method is similar: starts with the full set and eliminate one at a time.\n\n\n LASSO (Embedded Method)\n\n\nLASSO (least absolute shrinkage and selection operator)\n\n\nAdd a L1 regularisation term to reduce the number of effective features\n\n\nThe loss function is not differentiable. Sub-gradient methods or least-angle\nregression can be used to optimise the loss\n\n\n\nA higher 𝜆 will make some of the weights of x becomes 0, hence reduce the dimensionalities.\n\n\n Filter Method\n\nUnivariant feature selection (assuming features are independent to each other)\n\n\n\n\nMethod\nFeature Type\nTarget Type\nPrinciple\n\n\n\n\nChi-square\nCategorical features\nCategorical targets\nTests the independence between the feature and the target using observed and expected frequencies.\n\n\nT-test\nContinuous features\nBinary classification\nCompares the statistical difference between the means of two groups.\n\n\nANOVA\nCategorical features\nContinuous targets\nUses variance analysis to evaluate the relationship between the feature and the target.\n\n\nCorrelation\nContinuous features\nContinuous targets\nMeasures the linear relationship between the feature and the target using correlation coefficients.\n\n\n\n Popular Dimensionality Reduction Methods\n Principal Component Analysis\nPCA: Unsupervised\nTwo methods resulted the same PCA calculation.\n\nMaximum variance\nMinimise average projection error\n\nPCA requires calculation of:\n\nMean of observed variables\nCovariance of observed variables\nEigenvalue/eigenvector computation of covariance matrix\n\n Linear Discriminant Analysis\nLDA: Supervised\n\n\nLDA is a predictive modelling algorithm for multi-class classification\n\n\nDimensionality reduction by providing a projection of a training dataset that best separates the examples by their assigned class\n\n\n Manifold Learning\n\n\nManifold learning aims to learn the latent representation of the original data in lower dimensions.\n\n\nPCA is a linear manifold learning method, which doesn’t work in some cases\n\n\n Supervised Linear Models\n Supervised vs Unsupervised\n\nBoth require a training dataset.\nSupervised learning requires labels y for each instance.\nUnsupervised learning does not use labels\n\n Linear Model vs Nonlinear Model\nLinear algorithms assume that the sample features x and the label output y are linearly related\n\nLinear regression\nLogistic regression (classification)\nNaïve Bayes\nSupport Vector Machines (SVM)\n\nNonlinear algorithms assume a nonlinear relationship between x and y.\n\nK nearest neighbour\nKernel SVM\nDecision Trees\nNeural Networks\n\n Linearly Separable Definition\n\nDatasets whose classes can be separated by linear decision surfaces\n\n100% classification accuracy in the training set\n\n\nImplies no class-overlap\nClasses can be divided by lines for 2D data or planes in 3D data\n\n Parametric vs Non-Parametric Algorithms\n\nParametric algorithm (model-driven): assumes the data follows a specific distribution in its feature space or a pre-defined relationship between feature and outcome.\n\nLinear regression\nGaussian naïve Bayes\nMaximum likelihood classifier\n\n\nNon-parametric algorithm (data-driven): approaches are not constrained to prior assumptions on the data distribution.\n\nDecision trees\nNeural networks\n\n\n\n Classification vs Regression\n\nRegression(predicting numerical values): estimate values for a given data.\nClassification(predicting categorical values): assign class labels to data.\n\n Overfitting vs Underfitting\n\nOverfitting:\n\na model learns to map the training data too well, which negatively impacts the performance of the model on new, unknown, data.\nPoor generalisability\n\n\nUnderfitting:\n\nif it neither can model the training nor the test data correctly. Underfitting is easy to detect in the training phase using evaluation metrics.\nPoor performance\n\n\n\n Variance and Bias\n\nBias indicates the error between the approximated model to the ideal model.\nVariance is the amount that the estimate of the target function will change given different training data.\n\n      \n Intrinsic Parameters vs Hyperparameters\n\nIntrinsic parameters\n\nCan be efficiently learned on the training set\nLarge in number\ne.g. weights in linear regression or Artificial Neural Network (ANN)\n\n\nHyper-parameters\n\nMust be learned by establishing generalisation error\nNo efficient search possible\nSmaller in number\ne.g. the number of nodes in an ANN or the weights of two terms in a loss function\n\n\n\n Linear Regression\n Univariate Linear Regression\n\n\ndetermines relationship between one independent (feature: x) variable and one dependent variable (outcome: y)\n\n\nTrain process:\n\ngiven a model hhh with solution space SSS and a training set {X,Y}\\{X,Y\\}{X,Y} that contains nnn samples,\na learning algorithm finds the solution S^\\hat SS^ that minimises the cost function J(S)J(S)J(S)\n\n\n\n Multivariate Linear Regression\nMultivariate linear regression is the linear sum of the multiplications of each of the features with their corresponding weight terms\n\nfirst-order polynomial: y^=h(x,w)=w0+w1x1+⋯+wjxj+⋯+wdxd\\hat{y} = h(\\mathbf{x}, \\mathbf{w}) = w_0 + w_1x_1 + \\cdots + w_jx_j + \\cdots + w_dx_dy^​=h(x,w)=w0​+w1​x1​+⋯+wj​xj​+⋯+wd​xd​\nhigher-order polynomial: h(x,w)=w0+w1x1+w2x12+⋯+wkx1k+⋯+wk∗dxdkh(\\mathbf{x}, \\mathbf{w}) = w_0 + w_1x_1 + w_2x_1^2 + \\cdots + w_kx_1^k + \\cdots + w_{k \\ast d}x_d^kh(x,w)=w0​+w1​x1​+w2​x12​+⋯+wk​x1k​+⋯+wk∗d​xdk​\n\n Linear Classification (Logistic Regression)\n\nLogistic regression provides probabilities and classifies new samples using continuous and discrete measurements.\nOutput is always between 0 and 1, indicating the likelihood of being one class.\nUse Maximum Likelihood or minimise the Cross Entropy\n\n Logistic function:   P(x)=11+e−(w0+w1x)P(x) = \\frac{1}{1 + e^{-(w_0 + w_1x)}}P(x)=1+e−(w0​+w1​x)1​\n Loss function: L=∑k=1K(ykln⁡(pk)+(1−yk)ln⁡(1−pk))L = \\sum\\limits_{k=1}^K \\left( y_k \\ln(p_k) + (1 - y_k) \\ln(1 - p_k) \\right)L=k=1∑K​(yk​ln(pk​)+(1−yk​)ln(1−pk​))\nFind the best w0w_0w0​, w1w_1w1​ that maximise LLL or minimise −L-L−L\n Naïve Bayes\nBased on Bayes’ theorem: P(A∣B)=P(B∣A)⋅P(A)P(B)P(A|B)=\\frac{P(B|A)\\cdot P(A)}{P(B)}P(A∣B)=P(B)P(B∣A)⋅P(A)​\n\nFind the probability of A happening, given that B has occurred.\nAssumption is that features are independent with the same importance (Naïve).\nNaïve Bayes leads to a linear decision boundary\nMultinomial Naïve Bayes\n\nfeatures are counts: number of occurrence\n\n\nGaussian Naïve Bayes\n\nfeatures are continuous values\n\n\nBernoulli Naïve Bayes\n\nfeatures are only binary values: yes or no\n\n\n\n Supervised Nonlinear Models (1)\n K Nearest Neighbour (KNN)\n\nInput:\n\nthe training set TTT, category label set CCC and test object xxx.\nEach object has features x1x_1x1​ and x2x_2x2​.\nThe number of neighbours KKK.\n\n\nOutput: the category cxc_xcx​ of the test object xxx (cxc_xcx​ belongs to CCC)\n\n1234567begin    for each t belongs to T do    \tcalculate the distance D(t,x)    end for    select K training examples from T that have the smallest distance D.    calculate c_x as the majority class in the K examples.end\n Questions: KNN\n Q2: How to determine K?\n\nCross-validation: Test different values of K and choose the one that minimizes the validation error.\nOdd numbers are preferred in binary classification to avoid ties.\nSmaller K: More sensitive to noise (high variance).\nLarger K: Smoother decision boundaries (low variance, possible high bias).\n\n Q3: What happens if K is an even number and there is a tie?\n\nIn case of a tie (equal votes for multiple classes), the algorithm can:\n\nChoose randomly between tied classes.\nUse secondary criteria, such as the class of the closest neighbor.\n\n\n\n Q5: What are the advantages and disadvantages of KNN?\nAdvantages:\n\nSimple and intuitive.\nNo training phase.\nWorks well for small datasets with well-separated classes.\n\nDisadvantages:\n\nComputationally expensive during prediction (due to distance calculations).\nSensitive to irrelevant or unscaled features.\nPerformance heavily depends on the choice of K and distance metric.\n\n Hard Margin Support Vector Machines\nHard margin SVM is a linear model: line in 2D, hyperplane in higher D\nSVM seeks a decision boundary that maximises the margin between the two classes\n\nPoints on the decision boundary satisfy: 𝑤𝑇𝑥+𝑏=0𝑤^𝑇𝑥 + 𝑏 = 0wTx+b=0\nDistance of a point x to the decision boundary is: wTx+b∣∣w∣∣\\frac{w^Tx+b}{||w||}∣∣w∣∣wTx+b​\n\nTo find the DB, it need to\nmin⁡w,b12∥w∥2s.t. tn(wTxn+b)≥1\\min\\limits_{w, b}   \\frac{1}{2} \\|w\\|^2 \\quad\n\\text{s.t.  }  t_n (w^T x_n + b) \\geq 1\nw,bmin​21​∥w∥2s.t. tn​(wTxn​+b)≥1\n Soft Support Vector Machines (Linear Model)\nSoft SVM allows some points to be on the wrong side of DB.\n\n\nAllow some violations,\n\n𝜉_𝑛 : slack that measures the violation for 𝑥𝑛𝑥_𝑛xn​\n\n\n\nThen we have the soft SVM,\n\n\n\\min\\limits_{w, b, 𝜉_𝑛}   \\frac{1}{2} \\|w\\|^2+C \\sum\\limits_{n=1}^N𝜉_𝑛 \\quad\n\\text{s.t.  }  t_n (w^T x_n + b) +𝜉_𝑛 \\geq 1\n\n\n\n\n\nCCC is a weighting term: higher CCC results in smaller DDD\n\n\n Kernel SVM\nKernel SVM maps the original feature into a higher dimensional feature space\n\n\nUse linear SVM to classify in the higher dimensional space\n\n\nUse kernel trick to reduce the computational load\n\n\n SVM Questions\n Q1: What are the hyperparameters for soft SVM?\n\nCCC: Controls the trade-off between maximizing the margin and minimizing the classification error.\n\nHigher CCC: Less tolerance for classification errors (smaller margin).\nLower CCC: Allows more errors (larger margin).\n\n\n\n Q2: What are the hyperparameters for kernel SVM?\n\nCCC: Same as soft SVM, controlling the margin vs. error trade-off.\nKernel-specific parameters:\n\nFor RBF kernel:\n\nγγγ: Controls the influence of individual data points.\n\nHigher γγγ: Focuses on closer data points (complex decision boundary).\nLower γγγ: Considers farther points (simpler decision boundary).\n\n\n\n\nFor polynomial kernel:\n\nDegree (ddd): The degree of the polynomial.\nCoefficients (rrr): Offset in the polynomial equation.\n\n\n\n\n\n Q3: How to use SVM to solve multi-class classification?\n\n\nOne-vs-One (OvO):\n\nPairwise classification between every two classes.\nFor NNN classes, train N(N−1)2\\frac{N(N-1)}{2}2N(N−1)​ classifiers.\nFinal prediction is based on majority voting.\n\n\n\nOne-vs-Rest (OvR):\n\nTrain NNN classifiers, each separating one class from all others.\nFinal prediction is the class with the highest confidence score.\n\n\n\n Q4: Soft SVM vs Linear Regression: Pros and Cons\n\n\n\nAspect\nSoft SVM\nLinear Regression\n\n\n\n\nPurpose\nClassification (separates classes)\nRegression (predicts continuous values)\n\n\nRobustness\nHandles outliers better with proper C.\nSensitive to outliers.\n\n\nGeneralization\nStronger generalization with kernel support.\nMay struggle with non-linear relationships.\n\n\nComputational Cost\nHigher (especially with kernels).\nLower (simple closed-form solution).\n\n\n\n Supervised Nonlinear Models (2)\n Decision Trees\nLearn a “tree-like” representation of a training dataset to achieve classification/ regression of unseen data.\n\nDecision node contains conditions to split the data\nLeave node contains the group of data and the associated label that share similar characteristics\n\n Binary Decision Trees for Classification\n\nBegin the tree with the root node ®, which contains the complete dataset.\nFind the best feature (attribute) using Attribute Selection Measure (ASM) to form a decision node\nDivide R into subsets using the best feature and best condition (e.g. threshold).\nFor each subset find the best feature using ASM to split the data again\nRepeat steps 2-4 until the stopping condition is satisfied\n\nall data in the branch belong to the same class\nvalue difference in each leaf node is small enough\n\n\n\n Best Feature and Condition at Each Decision Node\n\n\nGreedy Search at each decision node\n\ne.g. Age feature from 10 to 100, calculate ASM using (10.5,11.5, …, 99.5)\n\n\n\nAttribute Selection Measure:\n\n\nEntropy: measures uncertainty/purity of data SSS that contains CCC classes.\nH(s)=−∑i=1CPilog2(Pi)H(s)=-\\sum\\limits_{i=1}^{C}P_ilog_2(P_i)H(s)=−i=1∑C​Pi​log2​(Pi​)\n\n\nInformation Gain: measures the difference in entropy before and after splitting data SSS using feature AAA.\nIG(S,A)=H(S)−∑t∈TP(t)H(t),S=⋃t∈TtIG(S,A)=H(S)-\\sum\\limits_{t\\in T}P(t)H(t), S=\\bigcup\\limits_{t\\in T}tIG(S,A)=H(S)−t∈T∑​P(t)H(t),S=t∈T⋃​t\n\n\n\n\n Decision Tree for Regression\n\nSimilar to classification trees, decision tree can be used for regression task.\nUse variance as the Attribute Selection Measure.\n\nwhen spilt to NNN and MMM subset\nASM=∑i=1N(yi−yˉn)2+∑i=1M(yi−yˉm)2ASM=\\sum\\limits_{i=1}^{N}(y_i-\\bar y_n)^2+\\sum\\limits_{i=1}^{M}(y_i-\\bar y_m)^2ASM=i=1∑N​(yi​−yˉ​n​)2+i=1∑M​(yi​−yˉ​m​)2\n Pros and Cons of Decision Trees\nPros:\n\n\nRobust to features with different scales (don’t need feature normalisation)\n\n\nEmbedded feature selection (redundant features won’t be selected at decision node)\n\n\nScalable to handle a large training dataset\n\n\nCould handle missing feature value by ignoring it\n\n\nHigh interpretability\n\n\nCons:\n\nGreedy search at decision node which is computationally expensive\nOverfitting as the tree goes deeper (biggest problem)\n\n Tree Pruning to Avoid Over-fitting\nUse a validation set to calculate error (classification/regression) for different versions of pruned trees\n Random Forest\nDecision is made based on the aggregation of different variety of trees\n\n\nN samples (e.g. 1000), F features (e.g.100), B trees (e.g. 500 trees):\n\nFinal decision is based on majority voting from B decisions.\nRegression is based on average of B trees.\nUncertainty can be obtained based on ratios of positive/negative votes from B trees.\n\n\n\nBagging: the ensemble learning method that is commonly used to reduce variance within a noisy dataset.\n\nFor each of B trees, use randomly sampled 80% data for training and 20% for validation\n\n\n\nRandom Subspaces: randomly sampling the features (only use a random subset of features for each tree)\n\nDon’t always rely on the same set of features.\nTo avoid certain features overly emphasised.\nClassification uses F\\sqrt FF​ features , Regression uses F/3F/3F/3 features.\n\n\n\n Feature Importance in Random Forests\nPermutation Importance or Mean Decrease in Accuracy (MDA):\n\nRandomly permute feature values of the validation samples, and the accuracy is computed each time.\nThe decrease in accuracy as a result of this permuting is averaged over all trees.\nThe larger the decrease in accuracy the more important one feature is.\n\n Random Forest vs Decision Trees\n Pros:\n\nBetter Generalization (Reduced Overfitting)\nHigher Accuracy\nHandles Variance and Bias Better\nWorks Well with High-Dimensional Data\nFeature Importance\n\nCons:\n\nIncreased Computational Cost\nLoss of Interpretability\nHigher Memory Usage\n\n Artificial Neural Networks\n Neural Unit\n\n Neural Network\n\nInput layer connects to the input data (normally features)\nHidden layer performs feature interactions (weighted sum)\nOutput layer is in charge of the decision making\n\n Activation Functions\nEach neurons in the hidden layer and output layer contains an activation function.\n\n\nThe hidden layer and output layer can have different activation functions.\nOutput layer’s activation function is task dependent\n\n ANN Training\n\nRandomly initialise the weights and bias: W and b\nInput a data (normally a batch of data one by one) and perform forward passing, then generate a prediction.\nCalculate the error between the predicted output and the true label.\nBackpropagate the error from the output layer back to the input layer by calculating the averaged gradients of the batch for each W and b.\nUpdate all W and b using the gradient (Gradient Descent).\nRepeat 2-5, until the predicted output is good enough or W and b can’t be further updated.\n\n ANN\n\n\nRegression (Forward Passing)\n\nLoss function MSE: L=12N∑n=1N(Yn−Y^n)2L=\\frac{1}{2N}\\sum\\limits_{n=1}^N(Y_n-\\hat Y_n)^2L=2N1​n=1∑N​(Yn​−Y^n​)2\n\n\n\nMulti-class Classification\n\nCross entropy loss function: L=−∑n=1N∑k=1C[yknlog(y^kn)]L=-\\sum\\limits_{n=1}^N\\sum\\limits_{k=1}^C[y_{kn}log(\\hat y_{kn})]L=−n=1∑N​k=1∑C​[ykn​log(y^​kn​)]\nOutput activation function are softmax functions\n\n\n\nOptimisation\n\n\nHow to update wab(n)w_{ab}^{(n)}wab(n)​?\nwab(n)′=wab(n)−α∂L∂wab(n)w_{ab}^{(n)&#x27;}=w_{ab}^{(n)}-\\alpha\\frac{\\partial L}{\\partial w_{ab}^{(n)}}wab(n)′​=wab(n)​−α∂wab(n)​∂L​\n\n\n\n\n Compare Decision Trees and ANN: Pros and Cons\n\n\n\nAspect\nDecision Trees\nANN (Artificial Neural Networks)\n\n\n\n\nEase of Use\nSimple to interpret and visualize.\nComplex and difficult to interpret (“black box”).\n\n\nTraining Time\nFast for small datasets.\nSlow, especially for deep networks with large datasets.\n\n\nOverfitting\nProne to overfitting (can be reduced with pruning).\nLess prone to overfitting (with regularization and dropout).\n\n\nData Requirements\nPerforms well on small datasets.\nRequires large datasets to perform well.\n\n\nFeature Scaling\nDoes not require feature scaling.\nRequires feature scaling for optimal performance.\n\n\nNonlinearity\nLimited capability to handle nonlinear relationships.\nExcellent at modeling complex nonlinear relationships.\n\n\nRobustness\nSensitive to small changes in data.\nMore robust to noise in data.\n\n\n\n Method Evaluation\n Method Generalisation\n\n\nGeneralisation is the desired property of a ML model to be able to predict outcome of unseen examples correctly\n\n\nA hypothesis generalises well if it can predict an example coming from the same distribution as the training examples well\n\n\n K-fold Cross Validation\n\nSplit training data into KKK folds.\nFor each fold, train on all other folds and make predictions on the held-out test fold.\nIterate all folds as the test fold\nCombine all the results of KKK folds, so that every data point has been used as a test data point.\n\n Nested K-fold\n\nIn the inner-loop, the same set of hypermeters are used in all folds.\n\n\n Evaluation Metrics (Classification)\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\n\n\n\nMetric\nFormula\nMeaning\n\n\n\n\nAccuracy\nTP+TNTP+FP+TN+FN\\frac{TP+TN}{TP+FP+TN+FN}TP+FP+TN+FNTP+TN​\nCorrectly predicted samples in total\n\n\nPrecision\n$$\\frac{TP}{TP + FP} $$\nTrue positives in predicted positives\n\n\nRecall (Sensitivity)\nTPTP+FN\\frac{TP}{TP + FN}TP+FNTP​\nTrue positives in actual positives\n\n\nSpecificity\nTNTN+FP\\frac{TN}{TN + FP}TN+FPTN​\nTrue negatives in actual negatives\n\n\nBalanced Accuracy\nSensitivity+Specificity2\\frac{\\text{Sensitivity} + \\text{Specificity}}{2}2Sensitivity+Specificity​\nAverage of sensitivity and specificity\n\n\n\n F Measure\n\nComparing different approaches is difficult when using multiple evaluation measures\nF measure combines recall and precision into a single measure.\nF measure ranges from 0 to 1, 1 indicates perfect precision and recall.\nF1 is commonly used. Beta=1, P: Precision, R: Recall\nBeta could make either precision or recall more important\n\nfβ=(1+β2)PR(β2P)+Rf_\\beta =(1+\\beta^2)\\frac{PR}{(\\beta^2P)+R}fβ​=(1+β2)(β2P)+RPR​\n Receiver Operator Characteristic (ROC) Curve\n\n\nCalculated by varying decision threshold of a classifier\n\n\nArea under the ROC curve (AUROC or AUC) is a widely used measure of goodness\n\n\n\n Confusion Matrix\n\nA visualisation tool used to present the results attained by a learner.\nEasy to see if the system is commonly mislabelling one class as another\n\n\n Evaluation Metrics (Regression)\n\n\nMean Squared Error\n\n\nMSE=1n∑i=1n(Yi−Y^i)2MSE=\\frac{1}{n}\\sum\\limits_{i=1}^n(Y_i-\\hat Y_i)^2MSE=n1​i=1∑n​(Yi​−Y^i​)2\n\n\nThe squaring has the effect of inflating or magnifying large errors.\n\n\n\n\nRoot Mean Squared Error\n\nRMSE=MSERMSE=\\sqrt {MSE}RMSE=MSE​\nPunish more on large errors but retains the original unit of the target value.\n\n\n\nMean Absolute Error\n\n\nMAE=∑i=1n∣Yi−Y^i∣nMAE=\\frac{\\sum_{i=1}^n|Y_i-\\hat Y_i|}{n}MAE=n∑i=1n​∣Yi​−Y^i​∣​\n\n\nRetains the original unit of the target value, and the changes are linear.\n\n\n\n\n Statistical Tests (Comparing two methods)\n\n\n\nFeature\nT-Test\nWilcoxon Signed-Rank Test\n\n\n\n\nPurpose\nCompare the means of two groups\nCompare the medians of two groups\n\n\nAssumption\nData follows a normal distribution\nNo assumption on data distribution\n\n\nApplicable Data Type\nOnly for paired data\nCan be used for both paired and unpaired data\n\n\nNull Hypothesis\nThe means of the two groups are the same\nThe medians of the two groups are the same\n\n\nRequirement for Normality\nRequired\nNot required\n\n\nSensitivity to Outliers\nSensitive to outliers\nRobust to outliers\n\n\nSignificance Likelihood\nMore likely to show significance if data is normal\nLess likely to show significance if data is normal\n\n\n\n Confidence Interval\n\n\nOur sampled data is never enough. CI helps to indicate how confidence of our estimation\n\n\nCI=mean±margin of errorCI=mean±margin \\,of\\, errorCI=mean±marginoferror\n\n\nIf the data follows a normal distribution, z-distribution can be used to find the critical values\n\n\nBootstrapping: random sampling with replacement. Calculate the mean and standard deviation, which is applied to the testing process\n\n\n Practical Issues\n Data\nMachine Learning algorithms are data-driven\n\n\n\nIssue\nDescription\nImpact\n\n\n\n\nNoisy Data\nPoor repeatability/reliability, contains redundant information\nDecreases model performance\n\n\nIncorrect Data\nWrong labels or incomplete labels\nMisleads the model during training\n\n\nSmall Data\nInsufficient sample size\nOverfitting and overly optimistic results\n\n\nBiased Data\nOne class overwhelms other classes\nIgnoring minority classes\n\n\nUnrepresentative Data\nDistribution of training data mismatch the test data\nReduces generalization ability, poor performance on real-world data\n\n\n\n Difference of Training, Validation and Test Sets\n\n\nTraining set\n\nrun the ML algorithm on.\n\n\n\nValidation set\n\ntune parameters, select features, and make other decisions regarding the learning algorithm.\n\n\n\nTest set\n\nevaluate the performance of the algorithm.\n\n\n\nDon’t simply use random 70%/10%/20% split or cross validation\n\nThey does not guarantee that the validation and test sets reflect future data distributions\nMay result in over- or under-estimation of model performance on the test set\n\n\n\nValidation set needs to be large enough to detect the performance difference, but not necessarily much larger\n\n\nTest set needs to be large enough to provide a confident estimate\n\n\n Error Mitigation\n\nBias Error: Model underfitting, fails to learn data patterns\n\nImprove feature engineering (e.g. outlier removal)\nImprove model architecture or try a better method\nReduce regularisation\nIncrease model size\n\n\nVariance Error: Model overfitting, relies too much on training data\n\nAdd regularisation or decrease model size\nImprove feature selection (reduce dimension, pick a subset)\nAdd more training data\n\n\nMismatch Error: Training and test data distributions are different\n\nUnderstand the difference between training and test sets\nAdd more training data that are similar to the test cases\n\n\n\n Analyse the Source of Error before Improving the Method\n\n\nYour algorithm must perform well on the training set before you can expect it to perform well on the validation and test sets\n\n\nManually examine a subset of training set. Correct the labels or exclude the examples that are wrongly labelled, if necessary\n\n\nManually examine a subset of validation set manually, check the wrong cases and identify the problem that potentially makes the biggest improvement\n\n\n Large Data set\n\n\nIf luckily you have a large dataset.\n\nDon’t use them all at once.\nAs soon as you used the test set to improve your model, it is no longer a valid test set.\n\n\n\nDivide the test set and validation set:\n\nTest Set: Fully retained for final evaluation.\nValidation Set: Split into Eyeball Set (manual analysis) and Blackbox Set (quantitative analysis).\n\n\n\nAvoid overfitting:\n\nWhen the Eyeball Set is used frequently to improve the model, a new Eyeball Set should be drawn from the Blackbox Set.\n\n\n\nSize of Eyeball Set:\n\nThe lower the error rate, the larger the Eyeball Set needed to collect enough error samples.\n\n\n\n Choose the Right Model!\n\n Unsupervised Learning (Cluster Analysis)\n\n\n\n\n\n\n\n\n\nA type of algorithm that learns hidden patterns from unlabelled data\nMethods:\n\nCluster Analysis: divide data into meaningful groups\nDimensionality reduction: Principal Component Analysis, Autoencoder, Generative models, etc.\nMost cases we combine the above two.\n\n Similarity Function\nGiven two column vectors AAA and BBB that contains features of two data samples\n\nEuclidean distance\n\nSquared root of the summed difference of AAA and BBB.\nSensitive to magnitude, suitable for continuous data\nEuclidean Distance=∑i=1n(Ai−Bi)2Euclidean \\,Distance=\\sqrt{\\sum\\limits_{i=1}^n(A_i-B_i)^2}EuclideanDistance=i=1∑n​(Ai​−Bi​)2​\n\n\nCosine distance\n\nLarger value indicates AAA and BBB share similar direction\nFocuses on direction, ignores magnitude\nCosine Distance=1−A⋅B∣∣A∣∣ ∣∣B∣∣Cosine \\,Distance=1-\\frac{A\\cdot B}{||A|| \\,||B||}CosineDistance=1−∣∣A∣∣∣∣B∣∣A⋅B​\n\n\nManhattan distance\n\nMean absolute difference of A and B.\nPath-based distance, good for sparse data and high damensional data\nManhattan Distance=∑i=1n∣Ai−Bi∣Manhattan \\,Distance=\\sum\\limits_{i=1}^{n}|A_i-B_i|ManhattanDistance=i=1∑n​∣Ai​−Bi​∣\n\n\nJaccard distance\n\nOverlapping over union of sets A and B.\nMeasures set similarity, suitable for binary data\nJaccard Distance=1−∣A∩B∣∣A∪B∣Jaccard \\,Distance=1-\\frac{|A\\cap B|}{|A\\cup B|}JaccardDistance=1−∣A∪B∣∣A∩B∣​\n\n\n\n K-means Algorithm\n Algorithm:\n12345Randomly select K points as the initial centroidsRepeat    Assigning each point to its closest centroid.    Re-compute the centroid of each cluster.Until centroids do not change\n Pros/Cons\n\n✓ Simple and efficient\n× Solution dependent on the initialisation\n× Need to specify number of clusters\n× Sensitive to outliers\n\n Category\nPartitional, Exclusive, Complete\n Agglomerative Hierarchical Clustering\n Algorithm:\n123456Treat each data as a cluster. Compute the similarity matrix between each pair of data.Repeat    Merge the closest two clusters.    Update the similarity matrix.Until Only one cluster remains\n Pros/Cons\n\n✓ Flexible with number of clusters\n✓ Can capture hierarchical relationship\n× Solution is local optimum, dependent on subject functions (e.g. min, max, group average etc.)\n× Requires larger memory and longer computational time.\n\n Category\nHierarchical, Exclusive, Complete\n Density-based Spatial Clustering of Applications with Noise (DBSCAN)\n Algorithm:\n123456Find the neighbourhood points of every point with distance E.Identify the core points with more than minPoints neighboursConnect the core points if they are within distance E.Make each group of connected core points into a separate cluster.Assign each non-core point to a nearby cluster if they are within distance E, called border points.Unassigned points are noise points.\n Pros/Cons\n\n✓ Robust to outliers\n✓ Learn non-regular population density patterns\n✓ Automatically determine the number of clusters\n× Non robust to variable density clusters (due to single E is used)\n× Computationally expensive\n× Sensitive to parameter settings\n\n Category\nPartitional, Exclusive, Partial\n Expectation Maximisation\n Pros/Cons\n\n✓ Soft clustering\n× Restricted by the distribution model\n× Sensitive to initialisation\n× Need to specificity the number of clusters\n\n Category\nPartitional, Fuzzy, Complete\n Categories of Cluster Analysis\n\n\n\n\nCategory\nOptions\nDescription\n\n\n\n\nStructure\nHierarchical vs Partitional\nHierarchy or direct division into K clusters\n\n\nMembership\nExclusive vs Overlapping vs Fuzzy\nWhether data points are exclusive or overlap/fuzzy\n\n\nCompleteness\nComplete vs Partial\nWhether all data points must belong to a particular cluster\n\n\n\n Evaluation of Cluster Analysis\nIf the clustering algorithm separates dissimilar data samples apart and similar data samples together, then it has performed well\n Methods\n\n\nSilhouette Coefficient (-1 to 1):\ns(i)=b(i)−a(i)max(a(i),b(i))s(i)=\\frac{b(i)-a(i)}{max(a(i), b(i))}s(i)=max(a(i),b(i))b(i)−a(i)​\n\n-1 incorrect clustering, 0 overlapping clusters, 1 highly dense separated clusters.\na: the mean distance between a sample and all other points in the same cluster\nb: the mean distance between a sample and all other points in the next nearest cluster\n\n\n\nDunn’s Index:\nD=mini,jd(i,j)maxkd′(k)D=\\frac{min_{i, j}d(i, j)}{max_kd&#x27;(k)}D=maxk​d′(k)mini,j​d(i,j)​\n\nd(i,j)d(i,j)d(i,j) represents the inter-cluster distance (centroids between two clusters) between iii and jjj.\nd’(k)d’(k)d’(k) measures the intra-cluster distance of cluster kkk.\n\n\n\n Deep Convolutional Neural Networks\nBased on the principle of Artificial Neural Networks but more (deeper) layers, more neurons, more data.\n\nNeuron is arranged in “3D”, each neuron is only connected by a small region of previous layer\nFeatures are learned in a hierarchical structure: from low level features to high level features\n\n Basic Components of Convolutional Neural Network\n\nConvolution operator\nPooling\nActivation function\nFully connected layer\nLoss functions\nOptimisation methods\n\n Convolution Operator\n Property\n\nConvolution is the sum of multiplications of the input data (feature) and a filter (weights).\nIt is vector dot product.\nSimilar to MLP but in a local region\n\n Calculation\n\nConvolve the input image with a set of learnable small size filters\nInput size(WWW); filter size (FFF); zero padding (PPP); stride (SSS);\nParameters to be optimised; output feature map size calculation (W−F+2P)/S+1(W-F+2P)/S+1(W−F+2P)/S+1;\n\n\n Pooling\n\nReduce the spatial size of the input size and reduce the amount of parameters\nEffectively down-sampling the input to increase the receptive field size\nMax operation with stride of 2 is a poplar choice\nNew researches show better performance without pooling layer but using larger stride in convolutional layer\n\n Fully Connected Layer and Output Activation\n\nPerform global feature learning: fully connected to all activations in the previous layer, as the same in MLP.\nOutput activation:\n\nSoftmax for classification\nLinear for regression\n\n\n\n Loss Functions\n\nNNN: number of data samples\nCCC: number of classes\ny^\\hat yy^​: predictions\nyyy: true labels\nppp: all pixels in an image\nMSE for regression:\n\nL=12N∑i=1N(yi−y^i)2L=\\frac{1}{2N}\\sum\\limits_{i=1}^{N}(y_i-\\hat y_i)^2L=2N1​i=1∑N​(yi​−y^​i​)2\n\n\nCross Entropy for Classification:\n\nL=−∑n=1N∑k=1C[yknlog2(y^kn)]L=-\\sum\\limits_{n=1}^{N}\\sum\\limits_{k=1}^C [y_{kn}log_2 (\\hat y_{kn})]L=−n=1∑N​k=1∑C​[ykn​log2​(y^​kn​)]\n\n\nCross Entropy or Dice Coefficient for Image Segmentation\n\nL=1−2∑pyy^∑py2+∑Py^2L=1-\\frac{2\\sum_p y\\hat y}{\\sum_p y^2 + \\sum_P \\hat y^2}L=1−∑p​y2+∑P​y^​22∑p​yy^​​\n\n\n\n Optimisation\n\n\nOptimisation methods\n\nGradient descent with momentum: avoid local minimum\nRMSProp: adaptive learning rate for each parameter\nAdam is the most popular optimisation method used in deep learning: combines RMSProp and Momentum\n\n\n\nBack propagation with chain rule to optimise weight w and bias b\n\n\nOptimisation Strategy using Big Data\n\nNormally intrinsic parameters are updated based on the averaged loss value after seeing all the training examples.\n\nToo slow to compute when we have a lot of training examples.\n\n\nStochastic Gradient Descent\n\nRandomly select ONE training example for gradient calculation.\n\n\nMini Batch Gradient Descent\n\nRandomly select a batch of training example for gradient calculation\n\n\n\n\n\n Deep Convolutional Neural Networks-Applications\n Overfitting is a big problem in deep learning\n Problem:\n\nAlexnet has 60 million parameters\nVGG16 has a total of 138 million parameters\nResnet101 has 44.5 million parameters\n\n\n Method to reduce overfitting:\n Data Augmentation\n\nData augmentation increases the number of training samples by varying the original image’s geometry and appearance.\nData augmentation is normally performed during training not beforehand\n\n Drop out\n\nRandomly disable certain neurons during training.\nNormally in fully connected layer, e.g. 20% dropout rate\n\n Transfer learning\n\n\nTrain the Deep CNN using a large dataset (e.g. imageNet).\n\n\nThen freeze the parameters of the first few layers, only retrain (fine tune) the high level feature layers or fully connected layers for a new application with small number of training samples.\n\n\n Restrictions using transfer learning\n\n\nDomain Gap:\n\n\nIf the pre-training dataset and the target dataset differ significantly, the migration may be less effective\n\n\nImageNet is a nature image while the target is a medical image\n\n\n\n\nChoice of Freezing Layers:\n\n\nFreezing too many layers may limit the model’s ability to adapt to new tasks;\n\n\nFreezing too few can lead to overfitting and high computational costs.\n\n\n\n\nTask complexity:\n\nIf the target task is very complex, more fine-tuning or even retraining may be required\nnew types of features need to be learnt\n\n\n\n Generative Models\n Variational Autoencoders (VAEs)\n\nVAE is an encoder-decoder structure, constructed by neural networks.\nIt learns to reduce the dimensionality of the input data\nThe learned latent space (z) from encoder needs to be regularised, so that new examples can be generated by the learned decoder.\n\n Regularisation\n\n\nWithout regularisation, the encoder-decoder model can be overfitted to the input data\n\nEncoder may map the input data to isolated points (discontinuous space)\nDecoder can only reconstruct training data well and cannot generate realistic new examples\n\n\n\nIn VAE, the encoder learns to map the input data into a normal distribution\n\n\nSo that we can then sample from this normal distribution to generate new examples using the decoder\n\n\n Loss Function in VAE\n\nReconstruction term: makes the generated output as similar as to the input.\nRegularisation term: makes the learned latent features from the encoder as close as to a Gaussian distribution\n\nThere is a trade-off between the regularisation and reconstruction loss.\n\nLarger regularisation makes the reconstruction error larger, hence reconstructed data is less realistic\n\ne.g. image blurry\n\n\n\n Advantages and Disadvantages\n\nAdvantages:\n\nLearned latent space is well constrained and easy to sample from.\nEasier to train compared to other generative models: smaller training set.\nBroad application scenarios: e.g. anomaly detection, data synthesis, etc.\n\n\nDisadvantages:\n\nGenerated images tend to be blurry\n\n\n\n Generative Adversarial Network (GAN)\nGAN has two parts:\n\nGenerator: learns to generate plausible instances that looks realistic (i.e. synthesise data).\nDiscriminator: learns to distinguish the generator’s fake data from real data, which penalises the generator for producing unreal examples.\nBoth Generator and Discriminator are neural networks, which are trained iteratively\n\n Discriminator\nThe discriminator is simply a classifier, which could use any network architecture appropriate to the type of data it’s classifying (e.g. VGG)\n\nReal data: from real samples in the training set\nFake data: synthesised data from the generator\n\nDuring discriminator training:\n\nThe discriminator classifies both real data and fake data.\nThe discriminator loss penalises the discriminator for misclassifying a real and a fake instance.\nThe discriminator updates its weights through backpropagation from the discriminator loss\n\n Generator\nThe generator part of a GAN learns to create fake data by incorporating feedback from the discriminator.\nIt learns to make the discriminator classify its output as real\nDuring generator training:\n\nInput random noise (e.g. vector in a small dimension sampled from uniform distribution)\nThe input goes through the generator network to generate a data instance\nThe discriminator classifies the generated data with an output\nCalculate the loss from discriminator classification, which penalises the generator for failing to fool the discriminator based on the output\nBackpropagate through both the discriminator and generator to obtain gradients\nUse gradients to change only the generator weights but don’t update the discriminator\n\n GAN’s iterative training process\n\nThe discriminator trains for one or more epochs.\nThe generator trains for one or more epochs.\nAlternate the above two steps to continue to train the generator and discriminator networks.\nThe generator improves with training and the discriminator performance gets worse. If the generator succeeds perfectly, then the discriminator has a 50% accuracy.\nThe discriminator feedback gets less meaningful over time makes GAN training unstable\n\n Problems of GAN\n\nTough to train:\n\nIf the discriminator behaves badly, the generator does not have accurate feedback and the loss function cannot represent the reality.\nIf the discriminator does a great job, the gradient of the loss function drops down to close to zero and the learning becomes super slow or even jammed.\n\n\nMode Collapse:\n\nDuring the training, the generator may collapse to a setting where it always produces similar outputs with low variety.\n\n\nTricks to improve GAN:\n\nUse Wasserstein distance to replace KL divergence: discriminator no longer discriminate between real and fake but calculate a smooth distance of their probability distribution\n\n\n\n Reinforcement Learning\n\n\n\n\n\n\n\n\n\nRL trains an agent to take actions in an environment by sending reward and state\n\nEmploys trial and error to find a solution\nMake a sequence of decisions\nDo not require labelled input/output pairs but rules of reward and penalty.\nAim to take actions by maximising reward\n\n Key Elements of RL\n\n\n\nTerm\nDescription\n\n\n\n\nEnvironment\nPhysical world in which the agent operates\n\n\nAgent\nLearn to act that maximises cumulative reward\n\n\nState\nCurrent situation of the agent\n\n\nReward\nFeedback from the environment\n\n\nPolicy\nMethod to map agent’s state to actions\n\n\nValue\nFuture reward that an agent would receive by taking an action in a particular state\n\n\n\n Q-Learning\n\nQ-learning is a method to find the next best action (aaa) given a current state (sss), Q(s,a)Q(s,a)Q(s,a) that aims to maximise cumulative reward.\nQ value defines the quality of State/Action pair\nBellman Equation: determine the value of a particular state and deduce how good it is to be in that state.\n\n Q-Learning Process\n\nInitialise Q-values in a Q-table (e.g. 0s)\nChoose an action a for state s (best Q-value)\nPerform action a, results in a new state s’\nMeasure reward R\nUpdate Q with Bellman Equation\n\nDrawback: computationally expensive in both training and inference\n Exploration vs Exploitation\n\nAgent don’t normally know the environment.\n\nIntroduce some randomness on selecting actions\n\n\nEpsilon-Greedy Exploration Strategy\n\nAt every time step when it’s time to choose an action, roll a dice\nIf it has a probability less than epsilon, choose a random action\nOtherwise take the best-known action at the agent’s current state\n\n\n\n Deep Q-Learning (DQN)\nDQN: a neural network maps input states to action/Q-value pair\n\nInitialise Q-values (network)\nNetwork predict an action aaa for state sss\nPerform action aaa, results in a new state s’s’s’\nMeasure reward RRR\nUpdate weights of Neural Network using the Bellman Equation: R(s,a)+γmaxQ′(s′,a′)R(s,a)+\\gamma_{max}Q&#x27;(s&#x27;,a&#x27;)R(s,a)+γmax​Q′(s′,a′).\n\n Policy Gradients (PG)\n\nPG uses a policy network to directly estimate actions rather than quality values\nInput states (image, coordinates), output actions\n\n Training Policy Network\n\nInitialise the network with random weights\nForward passing the network to generate a possible action\nWith this action, keep executing the game until end (e.g. game over).\n\nFor a positive reward, encourage the selected action by backpropagating a positive gradient\nFor a negative reward, discourage the selected action by backpropagating a negative gradient\n\n\nPlay N episodes (e.g. rounds of games), updates the weights based on positive/negative gradients.\n\n Summary and Challenges in Using RL\n\nModel free method is the way forward and more flexible in dealing with unknown environment but slow to train.\nPolicy gradients normally performed better than deep Q learning.\nRequires a large data to train, as no direct controlling of the agent but only through rewarding.\nReaching a local optimum: performing not as expected but the agent thinks it’s doing well.\nThe agent finds a shortcut in getting the rewards without completing the designed task.\n\n Sequence Models (RNN &amp; LSTM)\nSequential Data Learning\n\nOne data item is dependent on those that come before or after it (not independently and identically distributed\nMachine learning models that input or output data sequences are known as sequence models.\n\n Recurrent Neural Networks\nht=fw(ht−1,xt)h_t=f_w(h_{t-1},x_t)ht​=fw​(ht−1​,xt​), ht=tanh(whhht−1+wxhxt+bht)h_t=tanh(w_{hh}h_{t-1}+w_{xh}x_t+b_{ht})ht​=tanh(whh​ht−1​+wxh​xt​+bht​), yt=Whyht+byty_t=W_{hy}h_t+b_{yt}yt​=Why​ht​+byt​\n Long Short Term Memory Networks (LSTM)\n\n\nRNN suffers from vanishing/exploding gradient problem.\n\n\nIt is explicitly designed to capture long term dependency\n\n\nRemembering the long sequences for a long period of time.\n\n\nBesides hidden state h, LSTM also has a cell state c to remember things from previous knowledge.\n\nLong term memory: cell states (non-learnable)\nShort term memory: hidden states (learnable)\n\n\n\nLSTM back propagate gradient much more efficiently than RNN\n\n\nSigmoid functions act as gates to switch on/off the information passing\n\nThe forget gate determines how much (%) long term memory to retain.\nThe input gate determines how much (%) of the short-term information should be contributed to the long-term memory CtC_tCt​.\nThe output gate determines how much (%) of the new long-term memory should be contributed to\nthe current output yt=hty_t=h_tyt​=ht​.\n\n\n\n Applications:\n\nText Generation\nImage Captioning\n\n Transformers\n\nLSTM is able to capture long term memory but not long enough.\nTransformers is able to capture dependencies between each input words and between all input words to outputs in a sentence.\n\n Overview of Transformers\n\nEncoder-decoder architecture\nThe Encoder output is a continues vector representation of the inputs\nThe Decoder takes this continues vector representation to generate output one by one.\nApplications: language translation, chatbot, etc.\nTransformers has been used everywhere (e.g. object classification, image segmentation, etc. )\n\n\n Input Embedding\n\nEach word is mapped to a vector\nAdd a positional encoding to the vector using sine/cosine functions\n\n Encoder Training\n\n\nThe multi-head attention calculates the relationships between all pairwise inputs\n\n\nThe feed forward network (MLP) maps the attention vectors to something can be feed to the Transformer decoder.\n\n\n Multi-Head Attention\n\n\nInput, each word has Query, Key and Value\n\n\nScaled Dot-Product Attention-&gt;Scale-&gt;Softmax\n\n\nFor each input (e.g. word) compute multiple attention vectors (learnable weights WVW_VWV​, WkW_kWk​, WQW_QWQ​) and use the  weighted (WzW_zWz​) average as the final attention vector for each word\n\n\nOutput vector © is the dot product between attention weight and value.\n\nMultiple words are processed in parallel.\n\n\n\n Decoder Training\n\n\nthe target sentence is input to the Masked Multi-head attention, Masked out relationships to future words\n\n\nThen another Multi-head attention that learns the interactions between input words and target words (key and value are from the encoder, query is from decoder)\n\n\nThe output is one-hot encoding\n\n\n Model Inference\n\nThe input sentence is input to the encoder to generate feature vector representation and feed to the decoder\nA “start” input signal is input to the decoder\nThe decoder generate the first possible word in the target sentence.\nThis estimated word is then input the decoder to estimate the next word and iterate until an “end” signal.\nTo estimate the second words onwards the model uses the whole input sentence and all previously generated target words\n\n Chat GPT\nThree main steps in training Chat GPT\n\n\nGenerative pre-training.\n\n\nSolution: Transformer trained on large number of texts.\n\n\nProblem: Generated texts do not necessarily answer the question (prompt).\n\n\n\n\nSupervised fine-tuning.\n\n\n\nSolution: Human trainers to respond to prompt that are used as the label.\nProblem: Machine responses are not matched exactly to human’s response with some false/forbidden/non-sense statements.\n\n\nReinforcement learning from human feedback.\n\n\nSolution: human trainers rank the responses and form a reward function to make machine behave. (proximal policy optimisation).\n\n Trends &amp; Challenges in Machine Learning\n Diffusion Models\nAim: Noise to Image generation\nDrawback:\n\nLack details for high-resolution images.\nComputationally expensive.\n\n\n","slug":"Machine Learning","date":"2025-08-28T07:44:56.000Z","categories_index":"Notes","tags_index":"AI,ML","author_index":"Huaji1hao"},{"id":"0f33ba01d60d1829677c62e2b86ee042","title":"High Output Management Reading Notes","content":" High Output Management Reading Notes\n Introduction\n\nMotto\n\nYou need to develop a higher tolerance for disorder\nLet chaos reign, then rein in chaos\nThe output of a manager is the output of the organizational units under his or her supervision or influence\nAre one-on-one meetings still needed? Absolutely.\nYou are not an employee - you are in a business with one employee: yourself.\n\n\n\n 1. Context of the Book (1983)\n\nGrove wrote the book based on two decades of experience, mainly focusing on the essentials of management relevant to middle managers.\nDespite the passage of time, Grove finds that most fundamental managerial principles remain effective.\n\n 2. Significant Changes in the 1980s\nTwo major shifts impacted management practices:\n\nJapanese Competition in DRAM: Japanese companies became dominant in the memory (DRAM) industry due to superior manufacturing capabilities, challenging American firms like Intel.\nRise of E-mail: E-mail transformed communication, reducing information delays and enhancing coordination within organizations.\n\n 3. Intel’s Response to Competition\n\n\nIntel, initially a major DRAM producer, struggled against Japan’s aggressive pricing and high-quality production.\nLesson:\n\n“being second best in a tough environment is just not good enough”\n\n\n\nThis competition forced Intel to pivot, focusing on microprocessors instead, which was a painful yet crucial strategic shift.\nLesson:\n\nAdaptation is essential in a competitive environment;\nsticking to old strengths may no longer be viable.\n\n\n\n 4. Globalization’s Impact\n\nGlobalization means that both capital and labor can easily move across borders.\nThis results in global competition, where employees must compete with others worldwide, making time and efficiency critical differentiators.\n\n 5. Japanese Management Model\n\nJapanese firms were noted for their quick decision-making, often sitting in proximity for instant communication.\nHowever, American companies adapted to the digital age more quickly, leveraging e-mail for rapid, far-reaching communication.\n\n 6. The New Managerial Environment\n\nBusinesses must now operate in an increasingly unpredictable environment due to globalization and rapid information flows.\nManagers are encouraged to “let chaos reign, then rein in chaos”—embracing disorder as a temporary state to adapt and innovate.\n\n 7. Middle Managers as “Micro CEOs”\n\nMiddle managers should view themselves as CEOs of their teams, responsible for performance and productivity, independent of higher-level company-wide decisions.\n\n 8. Core Ideas of the Book\n\n\nOutput-Oriented Management:\nApplying manufacturing principles to management, focusing on measurable output.\n\n\nManagerial Leverage:\nIncreasing a manager’s effectiveness by influencing the productivity of their team or organization.\n\n\nPeak Performance in Teams:\nStriving to elicit the best performance from team members consistently.\n\n\n 9. Individual Competitive Advantage\n\nIn a globalized, competitive landscape, every employee must think of themselves as a sole proprietor, responsible for maintaining their own “individual competitive advantage.”\nCareer success today requires continuous learning, value creation, and adaptability, unlike in past decades where stable corporations took care of their employees’ careers.\n\n 10. Key Questions for Self-Assessment\nGrove encourages managers to reflect on three main questions to evaluate their effectiveness:\n\nAre you adding real value? – Consider if you’re truly enhancing the output of those you manage or merely moving information along.\nAre you plugged in? – Stay connected with developments in your organization and industry; don’t rely on others to keep you informed.\nAre you trying new ideas? – Actively test new techniques and technologies rather than waiting for others to innovate.\n\n 11. An Optimistic Yet Realistic Outlook\n\nGrove sees potential for increased productivity and wealth but acknowledges that people often resist change.\nsurvival requires adding value continually. He believes this adaptability is key to thriving amidst rapid change.\n\n 12. Grove’s Philosophy on Management\n\n\nBased on his experience at Intel, Grove argues that using methods of:\n\nProduction – Applying structured, output-oriented approaches to management.\nManagerial Leverage – Maximizing the impact of managerial actions on the team’s productivity.\nPeak Performance – Motivating teams to consistently perform at their best.\n\n\n\nThese principles are universally applicable across professions, from lawyers to engineers, and not limited to traditional business managers.\n\n\n\n Foreword to the Vintage Books Edition\n\nMotto\n\nA  manager′s  output=the  output  of  the  organization+the  output  of  the  neighboring  organizations  under  one′s  influenceA \\; manager&#x27;s \\; output = the \\;output\\;of \\;the \\;organization+the\\; output \\;of \\;the \\;neighboring \\;organizations\\; under\\; one&#x27;s \\;influenceAmanager′soutput=theoutputoftheorganization+theoutputoftheneighboringorganizationsunderone′sinfluence\nWhen a person is not doing his job, there can be two reasons for it.\n\nThe person either can’t do it or won’t do it (not capable or not motivated)\n\n\nOne-on-one is the best source for organizational knowledge that a manager can get it\nManagers who don’t have one-on-ones understand very little about what’s happening in their organizations\nIs it better to be a hands-on or hands-off manager? The answer is that it depends\nThe subordinate did poor work. My associate’s reaction: ‘He has to make his own mistakes. That’s how he learns!’ is absolutely wrong.\nIf you are not training, then you are basically neglecting half the job\nCEOs always act on leading indicators of good news, but only act on lagging indicators of bad news\nIn order to build anything great, you have to be an optimist\nOptimists most certainly do not listen to leadingindicators of bad news\n\n\n\n 1. Historical Context and Influence\n\nHigh Output Management was highly influential in Silicon Valley when it first appeared, long before TED Talks and business blogs became common.\nVenture capitalists and entrepreneurs valued it as a practical guide on management, particularly for the lessons it offered from Intel’s success in transitioning from memory to microprocessors.\n\n 2. Andy Grove’s Background\n\nGrove’s journey from a Jewish refugee to a Ph.D. holder and the CEO of Intel highlights his resilience and dedication.\nHis personal story added a unique depth to the book; Grove himself authored High Output Management without ghostwriters, giving readers his direct insights.\n\n 3. Approach to Management and Leverage\n\nGrove’s management philosophy centers on maximizing managerial leverage—using one’s skills to increase the output of their team or organization.\nHe defines management’s purpose as “the output of a manager = the output of their organization + the output of neighboring organizations under their influence.”\n\n 4. Key Principles in Management\n\nTask-Relevant Feedback: Managers must be proactive in providing feedback that’s directly related to an employee’s tasks, as this has a major impact on performance.\nOne-on-One Meetings: Essential for managers to understand their team’s work and gain organizational knowledge. Managers without regular one-on-ones may lack critical insights.\nMotivation and Training: Grove emphasizes that if a person isn’t performing, they are either not capable or not motivated. It’s the manager’s job to either train or motivate—neglecting these duties means failing half of the management role.\n\n 5. Hands-On vs. Hands-Off Management\n\nGrove stresses that management style depends on employee maturity. New employees may need hands-on guidance, while experienced ones benefit more from delegation.\nAn example Grove uses: it’s wrong to assume an employee will learn from mistakes without adequate support, as mistakes can negatively impact customers.\n\n 6. Leadership Qualities and Optimism\n\nGrove believes that optimism is essential for leadership; to accomplish challenging goals, a leader must stay hopeful.\nCEOs, he says, often act on “leading indicators of good news” but may ignore early signs of bad news due to their optimism—a trait necessary for driving significant change.\n\n 7. Practical Wisdom and Depth\n\nGrove’s book is considered a masterpiece because it provides not just principles but actionable tools for managers to excel.\nHis emphasis on task-relevant feedback, managerial leverage, and the importance of one-on-one meetings has made High Output Management a timeless guide for managers\n\n","slug":"High Output Management Reading Notes","date":"2024-10-27T16:41:45.000Z","categories_index":"Notes","tags_index":"Management","author_index":"Huaji1hao"},{"id":"35e0a70213c605a9cb06172ffb21a27c","title":"IELTS Reading, Listening and Speaking","content":" IELTS Reading, Listening and Speaking\n 阅读技巧\n 判断题\n 思路树:\n\n 总结:\n\n\n\n\n\n\n\n\n\n总的来说，先看原文和问题命题是否可共存的，不可共存则选False，或者\n\n同义替换后，其中一部分短语被替换为不同意思的词\n时间顺序相反，或者时间与发生的事情不匹配\n\n若意义相同或者原文表达的范围更小，则选True，\n其他都是Not Given\n 错误总结\n\n全部完成后，先检查填空单词拼写\n选择题先找同义替换，再理解选择，选项读不懂可以尝试拆分成短语\n非常明显一样的不一定是答案，同义替换正确的概率更高\n坚定自己的选择，不要为了凑顺序而修改答案\n\n 听力技巧\n 选择题\n\n\n\n\n\n\n\n\n\n更加关注某一个人否定某一话题之后提出的东西，那更大概率是正确的\n 匹配题\n 人名或地点\n\n\n\n\n\n\n\n\n\n\n第一感觉不一定对，最好听完找同义替换\n相信自己听到的短语的偏好选项\n尽量不要修改前面听到的答案，除非很确定\n大概率是按顺序的，因此应该以不变的专有名词(人名，地点等)为基准，听到的专有名词顺序有大概率可能就是题目的答案的排序(除了可能重复的那个专有名词需要更多加关注)\n\n 填空题(Part 4)\n 特点：\n\n答案词会被读得很清晰\n不出题的句子很少被替换\n填空的句子前或后大概率有替换\n\n\n\n\n\n\n\n\n\n\n通过不出题的句子来定位\n 口语技巧\n Part 1 示例回答\nAre you studying or working?\nI’m still a student. I’m doing my graduates in University of Nottingham.\nWhat subjects are you studying?\nI’m majoring in computer science with a focus on artificial intelligence.\nDo you like your subject?\nYes, I really enjoy it. I have been fascinated by artificial intelligence since I was young. Studying this subject helps me understand the principles behind intelligence, how it interacts with humans, and its future developments.\nWhy did you choose to study computer science with artificial intelligence?\nInitially, I was studying computer science without a focus. In my second year, I had the option to specialize in many directions. My mother consulted with her boss, a successful executive, who advised that artificial intelligence has great potential for future development. Based on this advice, I decided to focus on AI.\nDo you think your subject is popular in your country?\nYes, it is very popular, especially with the recent rise of technologies like ChatGPT. Many companies in China are developing their own AI models, which has significantly impacted the business sector\n Part 2\nThank you so much for your patience, I’m all set now.\nToday, based on this question xxxx, I am going to delve into the story of xxx.\n 口语句型场景\n 时间，亲情，友情 (priceless)\nWell I think time is priceless and invaluable, it transends the value itself. Because it cannot be measured by money or finacial success, instead it is the yardstick of how meaningful your life is.\n 有氧运动\nI would like to play basketball with my friends coz you know that’s a great aerobic exercise\n 兴趣\nSomething adds a new dimension to my life\n 有收获的经历/穿戴贵重物品/帮助他人\nWhen I xxx, it is also a really rewarding experience for me. I can also feel a sense of achievement and self worth and that feeling can just boost my confidence and self esteem.\nYou know that would be really helpful for my mental health.  / That would be good on sb’s personal development and growth.\n 框架：\n\n情景： I was with sb doing Sth\n特定事实：that’s when I found\n印象：I was like: “….”\n产生交集：so decided to\n验证印象：it turns out that\n\n","slug":"IELTS_Reading_and_Writing","date":"2024-06-13T23:03:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"dd102e7597f052d8f60bce0e3074afe4","title":"IELTS Writing Task 1","content":" Writing Task 1\n\n\n\n\n\n\n\n\n\n核心：用你的语言呈现图中的信息\n(尽量避免总结，个人观点和思考)\n注意：\n\n第三人称单数的s！\n单词拼写的正确性\n冠词an和a\n\n 数据类模板\n 第一段，介绍图表：\nThe given xxx chart/graph illustrates/demonstrates the details concerning xxx (time…)\n 第二段，明显内容：\nIn general/Generally, it could be obviously witnessed that xxx.\nWhat is more, xxx\nMoreover, it is noteworthy that …\n 第三段，细节数据：\nUpon closer examination of the data, it is discernible that …\n 第四段，细节数据：\nFurthermore, xxx.\nMoreover, it is palpable and noteworthy that xxx.\n 词汇\n\n上升：\n\nsurge 急速上升\nmodest rise 适度的上升\nincrease 增加\nclimb 攀升\ngrow 增长\nescalate 上升\nspike 猛增\nsoar 飙升\njump 跳跃式上升\n\n\n\n\n下降：\n\ndecline 下降\ndecrease 减少\ndrop 下跌\nfall 下降\nplummet 暴跌\nreduce 减少\nsink 下沉\nslide 滑落\ndip 轻微下降\n\n\n\n\n稳定：\n\nrelatively stable 相对稳定的\nsteady 稳定的\nexhibit minimal fluctuations 波动很少\nremain constant 保持不变\nunchanged 不变的\nmaintain 保持\nflat 稳定的\nlevel off 趋于平缓\nplateau 达到稳定期\n\n\n\n\n波动：\n\nshow considerable volatility 展现了相当大的波动性\nfluctuate 波动\noscillate 震荡\nswing 摇摆\nvary 变化\nundulate 波动\nebb and flow 起伏\ninstability 不稳定\nerratic 变化无常的\n\n\n\n 第二段具体句式(总体)\n\n持续地上升，下降：\n\nThe number/percentage/xxx of A exhibits a persistent upward/downward trajectory from 2000 to 2005.\nThe data reveals a consistent rise in xxx.\nThe xxx exhibited a consistent growth pattern during the period between 2000 and 2005\nThe graph depicts a continuous upward trend in xxx.\n\n\n\n\n急剧下降:\n\nThe figure plummeted dramatically in xxx.\n\n\n\n\n稳定:\n\n\nThe figures plateaued at xxx.\n\n\nThere has been little to no fluctuation in xxx.\n\n\n\n\n\n正负相关:\n\n\nThe number/percentage/xxx is positively/negatively related to xxx.\n\n\nThe two sets of figures move in tandem/opposite directions.\n\n\n\n\n\n中途超过:\n\n\nThe number/percentage/xxx of A surpassed that of B from 2000 to 2005.\n\n\nA outpaces B in terms of xxx.\n\n\nA surpass B but a noticeable margin.\n\n\n\n\n 第三段具体句式（细节）\n\n数据是多少\n\nThere was a reported figure of 100 for xxx in 2000.\n\n\n\n\n数据比谁高多少\n\nThe number of A was higher than that of B by 100.\nThe data indicates that A significantly outstripped B, with A’s figure reaching xxx compared to B’s xxx.\n\n\n\n\n倍数关系\n\nThe quantity of A was threefold that of B, which an accounting for xxx, a tripling compared to B’s xxx.\nThe changes of A generally mirrored those of B, except …\n\n\n\n\n数据变到了多少\n\nThe number of A experienced a remarkable surge, elevating sharply from 100 to 200.\nThere is a significant surge in the number of A, escalating from xxx in 2000 to xxx in 2005.\nxxx began with a significant increase of 6% in January, followed by a sharp decline to -3% in June. (转折)\n\n\n\n\n数据波动\n\nThe data for A demonstrated considerable fluctuations, oscillating between xxx and xxx throughout the 10 years.\n\n\n\n\n数据稳定\n\n\nThe figures for A remained stable, consistently hovering around from xxx to xxx.\n\n\nThe xxx remained relatively constant, with an average of roughly …\n\n\n\n\n 连接词\n顺承: subsequently, additionally\n对比: in contrast, conversely\n强调: indeed, notably\n举例: for instance\n原因 : owing to, in view of\n结果: consequently\n 范文\n\nLine Graph\nThe graph below gives information about the percentage of the population in four Asian countries living in cities from 1970 to 2020, with predictions for 2030 and 2040. Summarise the information by selecting and reporting the main features, and make comparisons where relevant.\nWrite at least 150 words.\n\nThe line graph illustrates the percentage of the population living in cities in four Asian countries from 1970 to 2020, with predictions for 2030 and 2040.\nIn general, it is evident that the urban populations in all four countries followed persistent upward trajectories from 1970 to 2020, and are predicted to continue growing steadily. Moreover, the trends for the Philippines and Thailand follow a comparable pattern, while Indonesia and Malaysia exhibit similar increasing patterns.\nUpon closer examination, the urban populations of both Malaysia and the Philippines began at approximately 31 percent in 1970. However, while Malaysia’s urban population is predicted to surge to 81 percent by 2040, the Philippines is expected to see a more modest rise, reaching 53 percent.\nFurthermore, Thailand’s urban population rate started at 18 percent, and Indonesia’s at 14 percent in 1970. However, by 2020, Indonesia had surpassed Thailand, and it is projected to continue growing rapidly to 60 percent by 2040, while Thailand’s is predicted to reach 48 percent.\n\nThe graph below gives information on the numbers of participants for different activities at one social centre in Melbourne, Australia for the period 2000 to 2020.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\nThe line graph provides information on the number of participants in various activities at a social center in Melbourne, Australia, from 2000 to 2020.\nIn general, the film club consistently had the largest proportion of participants compared to other activities. Additionally, table tennis was moderately popular before 2010, but the number of participants surged significantly in the following decade.\nUpon closer examination, the number of participants in martial arts remained relatively stable, hovering between 32 and 38 from 2000 to 2020. In contrast, the number of participants in amateur dramatics started at around 27 before 2005, followed by a dramatic decline to only 6 participants by 2020.\nIn addition, the musical performances club had no participants until 2005, after which the number steadily grew to 19 by 2020. The number of table tennis participants started at 17 in 2000 and surpassed amateur dramatics in 2010, eventually reaching 53 participants by 2020.\n\nThe line graph shows the sales of children’s books, adult’s fictions and educational books between 2002 and 2006 in one country.\n\nThe given line graph depicts the figures for three kinds of books in a certain nation between the years 2002 and 2006.\nIn general, it is observable that the sales of children’s book exhibited a consistent growth pattern during the aforementioned period. Moreover, it is noteworthy that the sales of educational books remained lower than those of others from 2002 to 2005.\nUpon closer examination of the data, it is discernible that the sales of children’s book began at approximately 32 million dollars, escalated to around 46 million dollars in 2005, and culminated at its highest point in 2006. Conversely, the sales of educational books remained relatively constant, with an average of roughly 28 million dollars.\nFurthermore, the sales of adults’ fiction commenced at about 45 million dollars in 2002 and underwent a reduction of about 9 million dollars in 2003. Then, a marginal increase was observed followed by a drop of approximately 10 million dollars in the subsequent years.\n\nThe graph below shows the average monthly change in the prices of three metals during 2014. Summarise the information by selecting and reporting the main features,and make comparisons where relevant.\n\nThe given line graph illustrates the average monthly percentage change in the prices of copper, nickel, and zinc during 2014.\nIn general, it is evident that the average monthly percentage changes in the prices of nickel and zinc fluctuated throughout the year, while copper prices remained relatively stable. Specifically, the percentage change in zinc prices rose towards the end of the year, whereas nickel prices experienced a decline. Moreover, the stability in the percentage changes of copper prices is noteworthy compared to the other two metals.\nUpon closer examination of the data, it is clear that the price of nickel began with a significant increase of 6% in January, followed by a sharp decline to -3% in June. After maintaining a steady trajectory for three months, there was a rapid increase to 1% in November. Furthermore, the changes in zinc prices generally mirrored those of nickel, except in January when zinc showed a modest rise of 1%. Additionally, the price of copper exhibited minimal fluctuations, hovering between -0.5% and 2% throughout the year.\nIn conclusion, while nickel and zinc prices showed considerable volatility in 2014, copper prices remained relatively stable, highlighting the distinct price behaviors of these metals.\n\n\n\n\nBar Chart\nThe chart below shows the number of households in the US by their annual income in 2007, 2011 and 2015. Summarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\nThe bar chart provides information about the number of US households categorized by different annual income brackets in the years 2007, 2011, and 2015.\nIn general, it can be observed that lower-income groups had a higher number of households compared to wealthier groups, with the exception of the highest income bracket. Additionally, the second-highest income group consistently had the fewest households throughout the period.\nLooking more closely at the data, the number of households in the lowest income group (less than $25,000) and the second-lowest income group ($25,000-$49,999) were quite similar, with both standing at around 26 million in 2007. By 2015, both groups saw a modest increase, reaching approximately 28 million. Meanwhile, households with an annual income between $50,000 and $74,999 remained relatively stable, hovering around 22 million over the same period.\nFurthermore, the second-highest income group ($75,000-$99,999) experienced little fluctuation, remaining steady at approximately 14 million households between 2007 and 2015. Households earning $100,000 or more represented the largest group in 2015, increasing from 29 million in 2007 to 33 million by 2015.\n\n\n 流程图\n 第一段，介绍流程\nThe diagram demonstrates the sequence of events in the xxx process.\nThe flowchart exhibits the various steps required to complete the xxx process.\nThe diagram demonstrates the intricate details of xxx, providing a comprehensive overview of its integral components.\n 第二段，宏观信息\nGenerally, it could be witnessed that totally 18 steps in relation to the whole process.\nWhat is more, xxx\nThe process can be meticulously broken down into a total xxx distinct phrases, each of which plays a pivotal role in the overall sequence.\n 第三第四段，具体流程\nUpon closer examination towards the process, xxx.\nMoreover, xxx.\n\n直线类\n流程从A开始，到B结束\n\n\nStarting from A, the process moves through several steps and ends at B.\n\n\nCommencing with the initiation of A, the progression then methodically transitions through a series of intricately interwoven stages, culminating in the finalization at B.\n\n\n在进入下一个步骤之前，xxx发生了\n\nBefore advancing to the next phase, xxx occurs.\n\nA结束后，步骤就到了B\n\nOnce A is finished, the process moves on to B\n\n\n\n\n分支类\n\n\nIn the mean time, xxx is happening alongside xxx.\n\n\nOnce reaching the juncture at xxx, the process splits into two divergent pathways: one leading towards xxx and the other navigating towards xxx.\n\n\n\n\n\n循环类\nA cyclic pattern is evident, with xxx recurrently looping back to xxx, thereby perpetuating a continuous cycle of operations.\n\n\n\nArticles\nThe diagram below shows how a biofuel called ethanol is produced. Summarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\nThe given chart illustrates the process of producing ethanol from plants and trees. This cycle for biofuel production involves three main stages: harvesting and preparing plant material for processing, producing ethanol by chemical processing, and then using the fuel for transportation before the cycle begins again.\nIn the first stage, plants and trees absorb sunlight and capture carbon dioxide from atmosphere to grow. Once mature, those plants are harvested using agricultural machinery, after which they undergo pre-processing equipment where they are converted into cellulose, which is a crucial component for ethanol production.\nIn the following stage, the cellulose is transported into a chemical factory for further processing, and then the cellulose is degraded into sugars which are subsequently transformed into bioethanol with the addition of microbes. The produced ethanol can be utilized as fuel for various vehicles, including cars, trucks, and airplanes.\nDuring the combustion of ethanol in engines, carbon dioxide is released into the atmosphere. This carbon dioxide is then reabsorbed by plants and trees, continuing the same procedure cycle.\n\n\n 位置图\n 第一段\nThe map highlights the notable changes took place in xxx from 2002 to 2003.\nThe maps illustrates the changes that have taken places in xxx over the years, specially between xxx and xxx.\nThe provided map portrays the evolving landscape of xxx(地区) over the whole period.\n 第三第四段\nIn the central region of A, a new xxx has been constructed.\nxxx被修建在A区域的中间\nDuring the period, the A area has experienced substantial growth and improvement, including the new-building xxx.\nA区域扩张了，包括一个新建的xxx\n\n某物在某位置\n\nNestled in the xxx quadrant of the depicted scheme, a prominent xxx emerges.\nIn the xxx part of the diagram, you can see a noticeable xxx.\nCommanding a central position within the illustration, the xxx is conspicuously situated.\nPerched at the edge of the delineated region, the xxx marks its presence.\n\n\n\n\n旁边有什么\nAbutting the A, B asserts itself.\n\n\n\n面积增大\nA palpable enlargement in the domain of the xxx is observed, suggesting spatial expansion.\nThere is a noticeable increase in the size of the xxx, indicating it has expanded\n\n\n\n某设施更换\nWhere the xxx once stood in xxx, now the xxx makes its mark\n\n\n\n某设施数量增加\nxxx now boasts an increased array of xxx\n\n\n\nArticles\nThe maps below show an industrial area in the town of Norbiton, and planned future development of the site.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\n\nThe maps illustrate the current layout of the Norbiton industrial area and the proposed future development of the region.\nIn general, it is evident that more residential areas and facilities will replace the current factories. Additionally, a bridge will be constructed over the river, linking the northern farmland to the central area.\nUpon closer examination, the main circular road will remain intact, while an additional smaller roundabout will be added to the southern part of the primary road, with a medical center located to its northeast. A road branching off to the right will lead to a school, and another road will extend northwards from the center. A playground will be established in this area, with houses surrounding the street.\nAdditionally, a new road will extend to the northwest from the central roundabout, with three housing units planned for the north of this road. A shopping area will also be constructed to the west.\n\nThe plans below show a harbour in 2000 and how it looks today.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\n\n\nThe maps illustrate the changes in Porth Harbour between the year 2000 and the present day.\nIn general, the overall layout of the harbour has remained largely unchanged, except for some modifications in land use and the addition of new structures. Moreover, the main dock for passenger ferries remains the same, though an additional dock has been constructed to accommodate more boats.\nUpon closer examination, traveling along the main road from north to south, the first left turn still leads to a car park, with the original showers and toilets to the north and new ones to the south. Additionally, the marina for private yachts and the fishing boats area have swapped positions, with the yachts now located where the fishing boats used to be in the south.\nFurthermore, the road leading to the lifeboat has been renovated. The southern car park is now only accessible from the main road, as the old branch road has been removed. A new southern road has been added, leading to a hotel that has replaced the disused castle. Cafes and shops have also been constructed west of the lifeboat, and the public beach in the south has been converted into a private beach for hotel guests.\n\n\n","slug":"Writing_Task_1","date":"2024-06-11T15:55:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"a5201f72282a8a0869129d066b95cef6","title":"Language and Computing","content":" Turing Problem\nType-0-languages = Recursively Enumerable Languages = Semi-decidable Languages\nDecidable languages = Recursive languages\n\nif there is a Turing Machine that accepts it and always stop on any word\n\nContext-sensitive languages(Type-1-languages): subset of recursive languages\n\nThese are grammars where the left-hand side of a production is always shorter than the right-hand side\n\nHalt problem: the language of encodings of Turing machines that will always stop\nThere are languages that are accepted by a TM (i.e., type 0 languages) but that are undecidable\n DFA\nA deterministic finite automaton (DFA) A=(Q,Σ,δ,q0,F)A = (Q,Σ, δ, q_0, F)A=(Q,Σ,δ,q0​,F) is given by:\n\nA finite set of statesstatesstates QQQ\nA finite set of input symbols, the alphabet,Σalphabet, Σalphabet,Σ\nA transitiontransitiontransition functionfunctionfunction δ∈Q×Σ→Qδ \\in Q \\times Σ \\rightarrow Qδ∈Q×Σ→Q\nAn initialinitialinitial statestatestate q0∈Qq_0 \\in Qq0​∈Q\nA set of finalfinalfinal statesstatesstates F⊆QF \\sube QF⊆Q\n\n The language of DFA\nUsing the extended transition function δ^\\hat{\\delta}δ^, we define the language L(A)L(A)L(A) of a DFA AAA formally:\n\n\nδ^(q,ϵ)=q\\hat{\\delta}(q,\\epsilon) = qδ^(q,ϵ)=q\n\n\nδ^(q,xw)=δ^(δ(q,x),w)\\hat{\\delta} (q, xw) = \\hat{\\delta}(\\delta(q,x),w)δ^(q,xw)=δ^(δ(q,x),w)\n\n\nL(A)={w  ∣  δ^(q0,w)∈F}L(A) = \\{w \\;|\\; \\hat{δ}(q_0,w) \\in F\\}L(A)={w∣δ^(q0​,w)∈F}\n\n\n NFA\nA nondeterministic finite automaton (NFA) A=(Q,Σ,δ,S,F)A = (Q,Σ, δ, S, F)A=(Q,Σ,δ,S,F) is given by:\n\nA finite set of statesstatesstates QQQ,\nA finite set of input symbols, the alphabet,Σalphabet, Σalphabet,Σ\nA transitiontransitiontransition functionfunctionfunction δ∈Q×Σ→P(Q)δ \\in Q \\times Σ \\rightarrow \\mathcal{P}(Q)δ∈Q×Σ→P(Q),\nA set of initialinitialinitial statesstatesstates S⊆QS \\sube QS⊆Q,\nA set of finalfinalfinal (or acceptingacceptingaccepting) states F⊆QF \\sube QF⊆Q.\n\n The language of NFA\nδ^∈P(Q)×Σ∗→P(Q)\\hat{δ} \\in \\mathcal{P}(Q)\\timesΣ^∗ \\rightarrow \\mathcal{P}(Q)δ^∈P(Q)×Σ∗→P(Q), δ^(P,w)\\hat{δ}(P,w)δ^(P,w) is set of states that are reachable from one of states in PPP on word www\n\nδ^(P,ϵ)=P\\hat{\\delta}(P, \\epsilon) = Pδ^(P,ϵ)=P\nδ^(P,xw)=δ^(⋃{δ(q,x)  ∣  q∈P},w)\\hat{\\delta}(P, xw) = \\hat{\\delta}(\\bigcup\\{\\delta(q,x)\\;|\\;q\\in P\\}, w)δ^(P,xw)=δ^(⋃{δ(q,x)∣q∈P},w)\nL(A)={w  ∣  δ^(S,w)∩F≠∅}L(A) = \\{w\\;|\\;\\hat{\\delta}(S,w)\\cap F\\not =\\empty\\}L(A)={w∣δ^(S,w)∩F=∅}\n\n Context-free Grammar\nA context-free grammar G=(N,T,P,S)G = (N, T, P, S)G=(N,T,P,S) is given by\n\nA finite set NNN of nonterminalnonterminalnonterminal symbolssymbolssymbols or nonterminalsnonterminalsnonterminals.\nA finite set TTT of terminalterminalterminal symbolssymbolssymbols or terminalsterminalsterminals.\nN∩T=∅N \\cap T = \\emptysetN∩T=∅; i.e., the sets NNN and TTT are disjoint.\nA finite set P⊆N×(N∪T)∗P \\sube N \\times(N \\cup T)^∗P⊆N×(N∪T)∗ of productions. A production (A,α)(A, α)(A,α), where A∈NA \\in NA∈N and α∈(N∪T)∗α \\in (N \\cup T)^∗α∈(N∪T)∗ is a sequence of nonterminal and terminal symbols. It is written as A→αA \\rightarrow αA→α in the following.\nS∈NS\\in NS∈N: the distinguished start symbol.\n\n The language of a grammar\nL(G)⊆T∗L(G) \\sube T^∗L(G)⊆T∗, consists of all terminal sentential forms:\n\nL(G)={w∈T∗  ∣  S⇒G∗w}L(G) = \\{w\\in T^*\\;|\\;S\\xRightarrow[G]{*}w\\}L(G)={w∈T∗∣S∗G​w}\n\n Pushdown Automaton\nA Pushdown Automaton P=(Q,Σ,Γ,δ,q0,Z0,F)P = (Q,Σ, Γ, δ, q_0,Z_0, F)P=(Q,Σ,Γ,δ,q0​,Z0​,F) is given by the following data\n\nA finite set QQQ of states,\nA finite set ΣΣΣ of input symbols (the alphabet),\nA finite set ΓΓΓ of stack symbols,A\ntransition function\nδ∈Q×(Σ∪{ϵ})×Γ→Pfin(Q×Γ∗)δ \\in Q \\times (Σ \\cup \\{ϵ\\}) \\times Γ \\rightarrow P_{fin}(Q \\times Γ^∗)δ∈Q×(Σ∪{ϵ})×Γ→Pfin​(Q×Γ∗)\nHere Pfin(A)P_{fin}(A)Pfin​(A) are the finite subsets of a set; i.e., this can be defined as\nPfin(A)={X∣X⊆A∧X is finite.}P_{fin}(A) = \\{X | X \\sube A \\wedge X \\text{ is finite.}\\}Pfin​(A)={X∣X⊆A∧X is finite.}\nThus, PDAs are in general nondeterministic because they may have a choice of transitions from any state. However, there are always only finitely many choices.\nAn initial state q0∈Qq_0 \\in Qq0​∈Q,\nAn initial stack symbol Z0∈ΓZ_0 \\in ΓZ0​∈Γ,\nA set of final states F⊆QF \\sube QF⊆Q.\n\n ID(Instantaneous Description)\n\n\n\n\n\n\n\n\n\nSuch a triple (q,w,γ)∈Q×Σ∗×Γ∗(q,w, γ) \\in Q\\timesΣ^∗\\timesΓ^∗(q,w,γ)∈Q×Σ∗×Γ∗ is called an Instantaneous Description (ID)\n Acceptance by final state\n\nL(P)={w  ∣  (q0,w,Zo)⊢∗(q,ϵ,γ)∧q∈F}L(P) = \\{w\\;|\\;(q_0,w,Z_o)\\vdash^*(q,\\epsilon,\\gamma)\\wedge q \\in F\\}L(P)={w∣(q0​,w,Zo​)⊢∗(q,ϵ,γ)∧q∈F}\n\n Acceptance by empty stack\n\nL(P)={w  ∣  (q0,w,Z0)⊢∗(q,ϵ,ϵ)}L(P)=\\{w\\;|\\;(q_0,w,Z_0)\\vdash^*(q,\\epsilon, \\epsilon)\\}L(P)={w∣(q0​,w,Z0​)⊢∗(q,ϵ,ϵ)}\n\n Deterministic PDAs\n\n∣δ(q,x,z)∣+∣δ(q,ϵ,z)∣≤1,q∈Q,x∈Σ,z∈Γ|\\delta(q,x,z)| +|\\delta(q,\\epsilon,z)| \\leq 1, q \\in Q,x\\in \\Sigma,z\\in \\Gamma∣δ(q,x,z)∣+∣δ(q,ϵ,z)∣≤1,q∈Q,x∈Σ,z∈Γ\n\n Turing Machine\nA Turing Machine M=(Q,Σ,Γ,δ,q0,B,F)M = (Q,Σ, Γ, δ, q_0,B, F)M=(Q,Σ,Γ,δ,q0​,B,F) is:\n\nA finite set QQQ of states;\nA finite set ΣΣΣ of symbols (the alphabet);\nA finite set ΓΓΓ of tape symbols s.t. Σ⊆ΓΣ \\sube ΓΣ⊆Γ. This is the case because we use the tape also for the input;\nA transition function\nδ∈Q×Γ→{stop}∪Q×Γ×{L,R}δ \\in Q \\times Γ \\rightarrow \\{stop\\} \\cup Q \\times Γ \\times \\{L, R\\}δ∈Q×Γ→{stop}∪Q×Γ×{L,R}\nThe transition function defines how the machine behaves if is in state qqq and the symbol on the tape is xxx. If δ(q,x)δ(q, x)δ(q,x) = stop then the machine stops otherwise if δ(q,x)=(q′,y,d)δ(q, x) = (q′, y, d)δ(q,x)=(q′,y,d) the machines gets into state q′q′q′, writes yyy on the tape (replacing xxx) and moves left if d=Ld = Ld=L or right, if d=Rd = Rd=R;\nAn initial state q0∈Qq_0 \\in Qq0​∈Q;\nThe blank symbol B∈ΓB \\in ΓB∈Γ but B∉ΣB \\not\\in ΣB∈Σ. Initially, only a finite section of the tape containing the input is non-blank;\nA set of final states F⊆QF \\sube QF⊆Q.\n\n ID(Instantaneous Description)\n\n\n\n\n\n\n\n\n\nAn element (γL,q,γR)∈ID(γ_L, q, γ_R) \\in ID(γL​,q,γR​)∈ID describes a situation where the TM is in state QQQ, the non-blank portion of the tape on the left of the head is γLγ_LγL​ and the non-blank portion of the tape on the right, including the square under the head, is γRγ_RγR​\n The language of a Turing Machine\n\nL(M)={w∈Σ∗  ∣  (ϵ,q0,w)⊢∗(γL,q′,γR)∧q′∈F}L(M) = \\{w\\in \\Sigma^*\\;|\\;(\\epsilon, q_0,w)\\vdash^*(γ_L, q&#x27;, γ_R)\\wedge q&#x27; \\in F\\}L(M)={w∈Σ∗∣(ϵ,q0​,w)⊢∗(γL​,q′,γR​)∧q′∈F}\n\n Predictive parsing\nConsider productions for a nonterminal X\n\nX→α  ∣  βX \\rightarrow \\alpha \\;|\\;\\betaX→α∣β\n\n1234parseX (t : ts) =| t ∈ first(α) -&gt; parse α| t ∈ first(β) -&gt; parse β| otherwise -&gt; Nothing\nSuppose it can be the case that\n\nβ⇒∗ϵ\\beta \\xRightarrow[]{*}\\epsilonβ∗​ϵ\n\n1234parseX (t : ts) =| t ∈ first(α) -&gt; parse α| t ∈ first(β) ∪ follow(X) -&gt; parse β| otherwise -&gt; Nothing\n Disambiguating context-free grammars\n\nE→E+E  ∣  E∗E  ∣  E↑E  ∣  (E)  ∣  NE \\rightarrow E + E \\;|\\; E * E \\;|\\;E \\uparrow E \\;|\\; (E) \\;|\\; NE→E+E∣E∗E∣E↑E∣(E)∣N\nN→0  ∣  1  ∣  2N\\rightarrow0\\;|\\;1\\;|\\;2N→0∣1∣2\n$\\uparrow(right) ;&gt;;*(left);&gt;;+(left) $\n\n The subexpressions of expressions of the highest precedence\n\nE→E1+E1  ∣  E1E\\rightarrow E_1+E_1\\;|\\;E_1E→E1​+E1​∣E1​\nE1→E2∗E2  ∣  E2E_1\\rightarrow E_2*E_2\\;|\\;E_2E1​→E2​∗E2​∣E2​\nE2→E3↑E3  ∣  E3E_2\\rightarrow E_3 \\uparrow E_3\\;|\\;E_3E2​→E3​↑E3​∣E3​\nE3→(E)  ∣  NE_3\\rightarrow (E)\\;|\\;NE3​→(E)∣N\nN→0  ∣  1  ∣  2N\\rightarrow 0\\;|\\;1\\;|\\;2N→0∣1∣2\n\n Make the corresponding productions left- and right-recursive\n\nE→E+E1  ∣  E1E\\rightarrow E+E_1\\;|\\;E_1E→E+E1​∣E1​\nE1→E1∗E2  ∣  E2E_1\\rightarrow E_1*E_2\\;|\\;E_2E1​→E1​∗E2​∣E2​\nE2→E3↑E2  ∣  E3E_2\\rightarrow E_3 \\uparrow E_2\\;|\\;E_3E2​→E3​↑E2​∣E3​\nE3→(E)  ∣  NE_3\\rightarrow (E)\\;|\\;NE3​→(E)∣N\nN→0  ∣  1  ∣  2N\\rightarrow 0\\;|\\;1\\;|\\;2N→0∣1∣2\n\n Elimination of left recursion\n\n","slug":"lac","date":"2024-06-09T15:39:56.000Z","categories_index":"Notes","tags_index":"Language,Automata","author_index":"Huaji1hao"},{"id":"e8690cb5cccdb863caaf29d6fe84aa6a","title":"AI Methods","content":" Introduction, Heuristic search (introduction), Pseudo-random numbers\n Preliminaries\n Decision support\nThis term is used often and in a variety of contexts related to decision making\n System\n\nDegree of dependence of systems on the environment\n\nClosed systems are totally independent\nOpen systems dependent on their environment\n\n\nEvaluations of systems\n\nSystem effectiveness: the degree to which goals are achieved, i.e. result, output\nSystem efficiency: a measure of the use of inputs (or resources) to achieve output, e.g., speed\n\n\n\n Solving problems by searching\n\nSearch for paths to goals\n\ntypical algorithms are the depth firstsearch, breadth first search, uniform cost search, branch and bound, A*\n\n\nSearch for solutions (optimisation)\n\nmore general class than searching for paths to goals\nefficiently finding a solution to a problem in a large space of candidate solutions\nsubsumes the first type, since a path through a search tree can be encoded as a candidate solution\n\n\nSearch in Continuous vs Discrete Space\n\n Solving an (mathematical) optimisation problem - steps\n\nFirst choose a quantity (typically a function of several variables –objective function) to be maximised or minimised, which might be subject to one or more constraints (constraint optimisation)\nNext choose a mathematical or search method to solve the optimisation problem (searching the space of solutions and detecting absolutely the best/optimal solution)\n\n Optimization\n\n\nFundamental problem of optimization is to arrive at the best possible (optimal) decision/solution in any given set of circumstances\n\n\nGlobal Optimization\nGlobal optimization is the task of finding the absolutely best set of admissible conditions to achieve your objective, formulated in mathematical terms\n\n\nIn most cases “the best” (optimal) is unattainable\n\n\nGlobal vs Local Optimum\n\nGlobal Optimum- better than all other solutions (best)\nLocal Optimum- better than all solutions in a certainneighbourhood\n\n\n\n Problem and Problem Instance\n\nProblem refers to the high level question or optimization issue to be solved\nAn instance of this problem is the concrete expression, which represents the input for a decision or optimization problem\n\n Combinatorial optimization problems (COP)\n\nRequire finding an optimal object from a finite set of objects\nFor NP-hard COPs, the time complexity of finding solutions can grow exponentially with instance size\n\n Optimization/Search Methods\n\n\nExact/Exhaustive/Systematic Methods\n\ne.g., Dynamic Programming, Branch&amp;Bound, Constraint Satisfaction, …\n\nlimitations: only work if the problem is structured - in many cases for small problem instances\nquite often used to solve sub-problems\n\n\n\n\n\nInexact/Approximate/Local Search Methods\n\ne.g., heuristics,metaheuristics, hyper-heuristics,…\n\n\n\n Search Paradigms\n\nPerturbative←→Constructive\n\n\nstart from complete solutions\n\n\nstart from partial solutions\n\n\n\ndeterministic ←→ stochastic\n\nprovide the same solution regardless of how many times\ncontain a random component and may return a different solution at each time\n\n\nsystematic ←→ local search\nsequential ←→ parallel\nsingle objective ←→ multi-objective\n\n Heuristic Search/Optimization\n Heuristic Search Methods\n\n\n\n\n\n\n\n\n\nA heuristic is a rule of thumb method derived from human intuition.\n\n\nA heuristic is a problem dependent search method which seeks good, i.e. near-optimal solutions, at a reasonable cost (e.g. speed) without being able to guarantee optimality\n\n\nGood for solving ill-structured problems, or complex well-structured problems (large-scale combinatorial problems that have many potential solutions to explore)\n\n\n Case study: Traveling Salesman Problem (TSP)\n\n\n\n\n\n\n\n\n\n&quot;Given a list of cities and the distances between each pair of cities,what is the shortest possible route that visits each city and returns to the origin city?” – NP hard\nExamples heuristics for TSP\n\nThe nearest neighbour (NN) algorithm - Constructive(Stochastic, Systematic)\nA Constructive Stochastic Local Search Algorithm for TSP(based on NN algorithm)\n\nStep 1: Choose a random city\nStep 2: Apply nearest neighbour to construct a complete solution\nStep 3: Compare the new solution to the best found so far and update the best solution as appropriate\nStep 4: Go-to Step 1 and repeat while the maximum number of iterations is not exceeded (parameter)\nStep 5: Return the best solution\n\n\nPairwise exchange (2-opt) - Perturbative (Stochastic, Local Search)\nA Perturbative Stochastic Local Search Algorithm for TSP(based on 2-opt)\n\nStep 1: Create a random current solution (build a permutation array and shuffle its content)\nStep 2: Apply 2-opt: swap two randomly chosen cities forming a new solution\nStep 3: Compare the new solution to the current solution and if there is improvement make the new solution current solution, otherwise continue\nStep 4: Go-to Step 2 and repeat while the maximum number of iterations is not exceeded (parameter)\nStep 5: Return the current solution\n\n\n\n Drawbacks of Heuristic Search\n\nThere is no guarantee for the optimality of the obtained solutions\nUsually can be used only for the specific situation for which they are designed\nOften, heuristics have some parameters\n\nPerformance of a heuristic could be sensitive to the setting of those parameters\n\n\nMay give a poor solution\n\n Pseudo-random numbers\n Some problems with pseudo-random numbers\n\nShorter than expected periods for some seed states; such seed states may be called ‘weak’ in this context\nLack of uniformity of distribution (e.g., 0.17 appears 100 times in10000 successive numbers while 0.29 appears 5 times more)\nCorrelation of successive values\nThe distances between where certain values occur are distributed differently from those in a random sequence distribution\n\n Components of heuristic search, Hill climbing (HC) , Performance analysis\n Main components of heuristic search methods\n\nRepresentation\nEvaluation function (objective function)\nInitialization (e.g., random)\nNeighborhood relation (move operators)\nSearch process (guideline)\nMechanism for escaping from local optima\n\n Representation\n\n\n\n\n\n\n\n\n\nEncoding of candidate solutions\n Characteristics\n\ncompleteness: all solutions associated with the problem must be represented\nconnexity: a search path must exist between any two solutions of the search space. Any solution of the search space, especially the global optimum solution, can be attained\nefficiency: The representation must be easy to manipulate by the search operators\n\n Example\n\nBinary encoding (e.g. 10110010110010…1011)\n\nGiven a binary string of length N (representing N items), search space size is 2^N\n\n\nPermutation encoding (e.g. for TSP: 1 5 3 2 6 4 7 9 8)\n\nGiven N cities (pubs), search space size is N!\n\n\nInteger encoding (e.g. 1 3 4 5 5 5 4 1 1 … 2 2 1)\n\nFor a general problem with M composite materials to form an N-layer composite structure, search space size is M^N\n\n\nValue encoding (e.g. ATFCTTCGG) (e.g. 1.2324 5.3243 …) (e.g.&lt;back, back, right, forward, left, …&gt;)\nNonlinear encoding (e.g. tree encoding - genetic programming)\nSpecial encodings (e.g. random key encoding)\n\n Evaluation function\n\n\n\n\n\n\n\n\n\nIndicates the quality of a given solution, distinguishing between better and worse solutions\n\nAlso referred to as objective , cost, fitness, penalty, etc.\nServes as a major link between the algorithm and the problem being solved\n\nprovides an important feedback for the search process\n\n\nMany types: (non)separable, uni/multi-modal, single/multi-objective, etc.\nEvaluation functions could be computationally expensive\nExact vs. approximate\n\nCommon approaches to constructing approximate models: polynomials, regression, SVMs, etc.\nConstructing a globally valid approximate model remains difficult, and so beneficial to selectively use the original evaluation function together with the approximate model\n\n\n\n Evaluation Function - Delta (Incremental) Evaluation\n\nKey idea: calculate effects of differences between current search position S and a neighbour S’ on the evaluation function value.\nEvaluation function values often consist of independent contributions of solution components; hence,\nf(S’) can be efficiently calculated from f(S) by differences between S and S’ in terms of solution components\nCrucial for efficient implementation of heuristics/metaheuristics/hyper-heuristics\n\n Neighborhoods\n\n\n\n\n\n\n\n\n\nA neighborhood of a solution is a set of solutions that can be reached from by a simple operator (move operator/heuristic)\n Example neighborhood for binary representation:\nBit-flip operator\n\nflips a bit in a given solution\nHamming Distance\n\nBetween two bit strings (vectors) of equal length is the number of positions at which the corresponding symbols differ.\ne.g. HD(011, 010) = 1\n\n\nIf the binary string is of size n, then the neighborhood size is n\n\nA discrete value is replaced by any other character of the alphabet\n\nIf the solution is of size n and alphabet is of size k , then the neighborhood size is (k - 1) * n\n\nAdjacent pairwise interchange\n\nswap adjacent entries in the permutation (e.g. 5 1 4 3 2 -&gt; 1 5 4 3 2)\nIf permutation is of size n, then the neighborhood size is n - 1\n\nInsertion operator\n\n\ntake an entry in the permutation and insert itin another position (e.g. 5 1 4 3 2 -&gt; 1 4 5 3 2)\n\n\nNeighborhood size: n * (n - 1)\n\n\nExchange operator\n\narbitrarily selected two entries are swapped(e.g. 5 4 3 1 2 -&gt; 1 4 3 5 2)\n\nInversion operator\n\nselect two arbitrary entries and invert the sequence in between them (e.g. 1 4 5 3 2 -&gt; 1 3 5 4 2)\n\n Summary of components\n\nChoosing an appropriate encoding to represent a candidate solution is crucial in heuristic optimisation\nInitialisation could influence the performance of an optimisation algorithm\nEvaluation function guides the search process and fast evaluation is important\n\n Hill climbing algorithms\n Search paradigm\n\n\n\n\n\n\n\n\n\nPerturbative heuristics/operators:\n\nMutational (diversification/exploration) vs.\nHill-climbing (intensification/exploitation)\n\n\n\nMutational heuristics/operator:\nProcesses a given candidate solution and generates a solution which is not guaranteed to be better\n\n\nHill climbing heuristics/operator:\nProcesses a given candidate solution and generates a better or equal quality solution\n\n\n Minimisation problem &amp; Maximisation problem\n\n\nA local search algorithm which constantly moves in the direction of decreasing level/objective value (for a minimisation problem) to find the nadir/the lowest point of the landscape or best/near optimal solution to the problem\n\nThe hill climbing algorithm halts when it detects a nadir value (where no neighbour has a lower value)\n\n\n\nA local search algorithm which constantly moves in the direction of increasing level/objective value (for a maximisation problem) to find the peak/the highest point of the landscape or best/near optimal solution to the problem\n\nThe hill climbing algorithm halts when it detects a peak value (where no neighbour has a higher value)\n\n\n\n Pseudocode\n\n\nPick an initial starting point (current state) in the search space\n\n\nRepeat\n\n\nConsider the neighbors of the current state\n\n\nCompare new point(s) in the neighborhood of the current state with the current state using an evaluation function and choose a new point with the best quality(among them) and move to that state\n\n\n\n\nUntil there is no more improvement or when a predefined number of iterations is reached\n\n\nReturn the current state as the solution state\n\n\n Note of algorithm\n\n\nInitial starting points may be chosen\n\nrandomly\nuse a constructive heuristic/operator(s)\naccording to some regular pattern\nbased on other information (e.g. results of a prior search)\n\n\n\nVariations of hill-climbing algorithms differ in the way for selecting a new solution compared to the current solution\n\n\nimproving vs. non-worsening ( tmpEval &lt; bestEval vs. tmpEval &lt;=bestEval )\n\n\nWhen to stop\n\nIf the target objective is known, then the search can be stopped when that target objective value is achieved\nHill climbing could be applied repeatedly until a termination criterion is satisfied\n\nHowever, there is no point in applying Best Improvement,Next Improvement and Davis’s (Bit) Hill Climbing if there is no improvement after any single pass over a solution\nRandom Mutation Hill Climbing requires consideration\n\n\n\n\n\n Simple hill climbing heuristic\n\n\nSimple Hill Climbing examining neighbors:\n\nBest improvement (steepest descent/ascent)\nFirst improvement (next descent/ascent)\nTrade-off between the number of search steps required for finding a local optimum and the computation time for each search step.\n\nTypically, for First Improvement search steps can be computed more efficiently than when using Best Improvement, since especially as long as there are multiple improving search steps from a current candidate solution, only a small part of the local neighborhood is evaluated by First Improvement. (Best improvement has larger search range)\nHowever, the improvement obtained by each step of First Improvement local search is typically smaller than for Best Improvement and therefore, more search steps have to be applied to reach a local optimum.\nAdditionally, Best Improvement benefits more than First Improvement from the use of caching and updating mechanisms for evaluating neighbors efficiently.\n\n\n\n\n\nStochastic Hill Climbing (randomly choose neighbors)\n\nDavis’s (bit) hill-climbing (DBHC)\nRandom selection/mutation hill climbing\n\n\n\nRandom-restart (shotgun) hill climbing is built on top of hill climbing and operates by changing the starting solution for the hill climbing, randomly and returning the best\n\n\n Hill climbing vs. Random walk\n\nA Hill-climbing method exploits the best available solution for possible improvement but neglect exploring a large portion of the search space\nRandom walk explores the search space thoroughly but misses exploiting promising regions\n\n Advantage:\nVery easy to implement, requiring:\n\na representation;\nan evaluation function;\na measure that defines the neighborhood around a point in the search space.\n\n Disadvantage:\n\nLocal Optimum: If all neighboring states are worse or the same. The algorithm will halt even though the solution may be far from satisfactory\nPlateau (neutral space/shoulder): All neighbouring states are the same as the current state. In other words the evaluation function is essentially flat. The search will conduct a random walk\nRidge/valley: The search may oscillate from side to side, making little progress. In each case, the algorithm reaches a point at which no progress is being made. If this happens, an obvious thing to do is start again from a different starting point\nAs a result, hill climbing algorithm may not find the optimal solution and may get stuck at a local optimum\nNo information as to how much the discovered local optimum deviates from the global (or even other local optima)\nUsually no upper bound on computation time\nSuccess/failure of each iteration depends on starting point\n\n Question Example\nexample: “Assume that Davis’s Bit Hill Climbing , First Improvement Hill Climbing and Steepest Descent Hill Climbing\nalgorithms are applied to a MAX-SAT problem instance resulting in average objective values of 12.4, 34.3 and 25.7, respectively, over 30 runs.”\n\nDavis’s Bit Hill Climbing would perform the best for solving MAX-SAT problems assuming a minimisation problem formulation (✕ -Any comment for 1 instance is valid only for 1 problem instance,not for the whole algorithm)\nDavis’s Bit Hill Climbing performs the best based on the average objective value on this problem instance (✕ - do not know it’s a maximisation / minimisation problem)\nAssuming that the problem is formulated as a maximisation problem, then First Improvement Hill Climbing performs the best based on the average objective value on this problem instance (✓)\n\n Statistical tests\n\nThe null hypothesis states the results are due to chance and are not significant in terms of supporting the idea being investigated\nA p-value/ probability value, is a number describing how likely it is that your data would have occurred by random chance (i.e. that the null hypothesis is true)\nApply non-parametric statistical test - one tailed:\n\nGiven two algorithms: X vs. Y, X &gt; Y (X &lt; Y) denotes that X(Y) is better than Y(X) and this performance difference is statistically significant within a confidence interval of 95% and X &gt;= Y ( X &lt;= Y) indicates that X(Y) performs better on average than  Y(X) but no statistical significance\nA stronger conclusion can be provided for one instance\nAlways repeat the experiments more than or equal to 30 times for any given instance for a meaningful statistical comparison\n\n\n\n\n Boxplots\n\n\n\n\n\n\n\n\n\nBoxplots illustrates groups of numerical data through their quartiles\n\n Notched boxplots\n\n\n\n\n\n\n\n\n\nNotched boxplots allows you to evaluate confidence intervals (by default 95% confidence interval) for the medians of each boxplot\n\n\nSince the notches in the boxplots A vs. B vs. C do not overlap,you can conclude that with 95% confidence that the true medians do differ between each pair of those algorithms on current instance: A performs significantly better than B as well as C, and B performs significantly better than C\n\n Progress plot - per instance\n\n\n\n\n\n\n\n\n\nObjective value from a run or mean of objective values from multiple runs per iteration/time unit\n\n Metaheuristic\n\n\n\n\n\n\n\n\n\nA metaheuristic is a high-level problem independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic optimization algorithms\n Components of metaheuristic\n\nRepresentation of candidate solutions\nEvaluation function\nInitialisation: E.g., initial candidate solution may be chosen\n\nrandomly use a constructive heuristic\naccording to some regular pattern\nbased on other information (e.g. results of a prior search), and more\n\n\nNeighborhood relation (move operators)\nSearch process (guideline)\nStopping conditions\nMechanism for escaping from local optima\n\n\n Mechanism for escaping from local optima\n\nIterate with different solutions, or restart (reinitialise search whenever a local optimum is encountered)\n\nInitialisation could be costly\nrestart could be partial (e.g. change 10% of previous solution)\ne.g. Iterated Local Search (ILS), GRASP\n\n\nChange the search landscape\n\nChange the objective function (e.g. Guided Local Search)\nUse (mix) different neighborhoods (e.g. Variable Neighbourhood Search, Hyper-heuristics)\n\n\nUse Memory\n(e.g. tabu search (TS))\nAccept non-improving moves\nallow search using candidate solutions with equal or worse evaluation function value than the one in hand\n\nCould lead to long walks on plateaus (neutral regions) during the search process, potentially causing cycles – visiting of thesame states\n\n\nNone of the mechanisms is guaranteed to always escape effectively from local optima\n\n Stopping conditions (examples)\n\nStop if a fixed maximum number of iterations, or moves, or objective function evaluations, or a fixed amount of CPU time is exceeded\nStop if consecutive number of iterations since the last improvement in the best objective function value is larger thana specified number\nStop if evidence can be given than an optimum solution has been obtained (i.e. optimum objective value is known)\nStop if no feasible solution can be obtained for a fixed number of steps/time (a solution is feasible if it satisfies all constraints in an optimisation problem)\n\n Deal with (in)feasible solution\n\n\nsimply reject infeasible solution\n\n\nUse a problem domain specific repair operator\n\ne.g. for 0/1 Knapsack Problem with constraints of 15kg, randomly flip a bit to 0 until the solution in hand feasible:1 1 0 1 0: $16 (18 kg, ✕) -&gt; 1 0 0 1 0: $14 (16 kg, ✕) -&gt; 1 0 0 0 0: $4 (12 kg, ✓)\n\n\n\nPenalise each constraint violation for the infeasible solutions such that they can’t be better than the worst feasible solution for a given instance\n\nSet a fixed (death) penalty value poorer than the worst\n\ne.g., f′(s)f&#x27;(s)f′(s)= if s is infeasible, then min{pi,∀i}/2min\\{p_i,\\forall i\\}/2min{pi​,∀i}/2, pip_ipi​ is the profit from the i th item\n\n\nDistinguish the level of infeasibility of a solution with the penalty\n\ne.g., f′(s)f&#x27;(s)f′(s)= if s is infeasible, then min{pi,∀i}/(2∗(total_weight−capacity))min\\{p_i,\\forall i\\}/(2*(total\\_weight-capacity))min{pi​,∀i}/(2∗(total_weight−capacity))\n\n\n\n\n\n Single Point Based Iterative Search - Local Search Metaheuristics  - Stochastic Local Search\n Pseudocode\n12345678910s0; // starting solutions* = initialise(s0) // e.g., improve s0 or use the same repeatRepeat\t// generate a new solution    s&#x27; = makeMove(s*, memory); // choose a neighbour of s*    accept = moveAcceptance(s*, s&#x27;, memory); // remember s_best    if(accept) s* = s&#x27;; // else reject new solution s&#x27;    Until (termination conditions are satisfied)\n\n\nMove Acceptance decides whether to accept or reject the new solution considering its evaluation/quality\n\n\nAccepting non-improving moves could be used as a mechanism to escape from local optimum\n\n\nEffective search techniques provide a mechanism to balance exploration and exploitation\n\nExploration aims to prevent stagnation of search process getting trapped at a local optimum\nExploitation aims to greedily increase solution quality or probability , e.g., by exploiting the evaluation function\n\n\n\nAim is to design search algorithms/metaheuristics that can\n\nescape local optima\nbalance exploration and exploitation\nmake the search independent from the initial configuration\n\n Iterated Local Search (ILS) - Local Search Metaheuristics - Stochastic Local Search\n1234567891011121314//random or construction heuristics0 = GenerateInitialSolution()s* = LocalSearch(s0) //not always usedRepeat\t// random move    s&#x27; = Perturbation(s*, memory)    // hill climbing    s&#x27; = LocalSearch(s&#x27; )    // remember s_best    s* = AcceptanceCriterion(s*, s&#x27;, memory)     // the conditions that the new local optimum    // must satisfy to replace the current solutionUntil (termination conditions are satisfied)return s*\n Based on visiting a sequence of locally optimal solutions by\n\n\nPerturbing the current local optimum (exploration)\n\n\nA perturbation phase might consist of one or more steps\n\n\nThe perturbation strength is crucial\n\nweak perturbations usually lead to shorter local search phases than strong perturbations, because the iterative improvement algorithm takes less steps to identify a local optimum\nToo small / weak: may generate cycles, fall back into the local optimum just visited leading to a stagnation of the search process\nToo big / strong: good properties of the local optima are lost , similar to a random restart of the search process\n\n\n\n\n\napplying local search/hill climbing(exploitation) after starting from the modified solution\n\n\n Acceptance criteria\n\nExtreme in terms of intensification: accept only improving solutions &lt;-&gt; Extreme in terms of diversification: accept any solution\nOther: deterministic (like threshold), probabilistic (like Simulated Annealing)\n\n Memory\nVery simple use: restart search if for a number of iterations noimproved solution is found\n Guidelines\n\nInitial solution should be to a large extent irrelevant for longer runs\nThe interactions among perturbation strength and acceptance criterion can be particularly important\n\nit determines the relative balance of intensification anddiversification\nlarge perturbations are only useful if they can be accepted\n\n\nAdvanced acceptance criteria may take into account search history,\n\ne.g. by occasionally reverting to incumbent solution\n\n\nAdvanced ILS algorithms may change nature and/or strength of perturbation adaptively during search\nLocal search should be as effective and as fast as possible.\n\nBetter local search generally leads to better ILS performance\n\n\nChoose a perturbation operator whose steps cannot be easily undone by the local search\n\n\n Tabu Search (TS) - Local Search Metaheuristics - Stochastic Local Search\n Basic idea\n\n\n\n\n\n\n\n\n\nuses history (memory structures) to escape from local minima/ maxima\n Pseudocode\n1234567determine initial candidate solution sWhile termination criterion is not satisfied\tdetermine set N&#x27; of non-tabu neighbours of s\tchoose a best improving candidate solution s&#x27; in N&#x27;\tupdate tabu attributes based on s&#x27;\ts = s&#x27;\n\n\nIn each step, move to ‘non-tabu’ best neighbouring solution (admissible neighbours), although it may be worse than current one\n\n\nTo avoid cycles, TS tries to avoid revisiting previously seen solutions\n\n\nTo avoid storing complete solutions, TS bases the memory on attributes of recently seen solutions\n\n\nTabu solution attributes are often defined via local search moves\n\n\nTabu-list contains moves which have been made in the recent past\n\n\nTabu tenure/tabu list length:\nthe length of time/number of steps ttt for which a move is forbidden\n\nttt too low - risk of cycling\nttt too high - may restrict the search too much\nttt = 7 has often been found sufficient to prevent cycling\nt=nt = \\sqrt{n}t=n​\nnumber of tabu moves: 5 ~ 9\n\n\n\nSolutions which contain tabu attributes are forbidden for a certain number of iterations\n\n\n\n\nOften, an additional aspiration criterion is used: this specifies conditions under which tabu status may be overridden (e.g. if considered step leads to improvement in incumbent solution)\n\nIf a tabu move is smaller than the aspiration level then we accept the move (use of aspiration criteria to override tabu status)\n\n\n\n 3 main components\n\nForbidding strategy: control what enters the tabu list\nFreeing strategy: control what exits the tabu list and when\nShort-term strategy: manage interplay between the forbidding strategy and freeing strategy to select trial solutions\n\n Memory\nheavily relies on the use of an explicit memory of the search process\n\nsystematic use of memory to guide search process\nmemory typically contains only specific attributes of previously seen solutions\nsimple tabu search strategies exploit only short term memory\nmore complex tabu search strategies exploit long term memory\n\n Introduction to Scheduling\n\n\n\n\n\n\n\n\n\nScheduling deals with the allocation of resources to tasks over given time periods and its goal is to optimize one or more objectives. The resources and tasks in an organization can take many different forms.\n Framework &amp; Notation\n\n\njobs j=1,2,...,nj = 1, 2, ..., nj=1,2,...,n (number of jobs are assumed to be finite)\n\n\nmachines i=1,2,...,mi = 1, 2, ..., mi=1,2,...,m (number of machines are assumed to be finite)\n\n\n(i,j)(i, j)(i,j)- processing step, or operation of job j on machine i\n\n\nscheduling problem - α | β | γ\n\nα - machine characteristics (environments)\nβ - processing/job characteristic\nγ - optimality criteria (objective to be minimised)\n\n\n\n Sample Machine Characteristics (α)\n\n\n111 Single machine\n\n\nPmPmPm Identical machines in parallel\n\nmmm machines in parallel\nJob jjj requires a single operation and may be processed on any of the m machines\n\n\n\nQmQmQm Machines in parallel with different speeds\n\n\nRmRmRm Unrelated machines in parallel machines have different speeds for different jobs\n\n\n Sample Job Characteristics (β)\n\nProcessing time pijp_{ij}pij​ - processing time of job jjj on machine iii (if a single machine then pjp_jpj​)\nDue date djd_jdj​ - committed shipping or completion (due) date of job jjj\nWeight wjw_jwj​ - importance of job jjj relative to the other jobs in the system\nRelease date rjr_jrj​ - earliest time at which job jjj can start its processing\nPrecedence precprecprec – Precedence relations might be given for the jobs. If kkk precedes lll, then starting time of lll should be not earlier than completion time of kkk.\nSequence dependent setup times sjks_{jk}sjk​ - setup time between jobs jjj and kkk\nBreakdowns brkdwnbrkdwnbrkdwn - machines are not continuously available\n\n Sample Optimality Criteria (γ)\n\n\nCijC_{ij}Cij​ completion time of the operation of job jjj on machine iii\n\n\nCjC_jCj​ time when job jjj exits the system\n\n\nCmaxC_{max}Cmax​ makespan is the time difference from the start (often, t=0) to\nfinish when the last job exits the system\n\n\nLj=Cj−djL_j = C_j - d_jLj​=Cj​−dj​ lateness of job jjj\n\n\nTj=max(Cj−dj,0)T_j = max(C_j - d_j , 0)Tj​=max(Cj​−dj​,0) tardiness of job jjj\n\n\nU_j = \n\\begin{cases}\n1 &amp; \\text{if  } C_j &gt; d_j\\\\\n0 &amp; otherwise\\\\\n\\end{cases}$$**unit penalty** of job $j$\n\n\n\n\n\n\n1∣prec∣Cmax−A1 | prec | C_{max} - A1∣prec∣Cmax​−A\nA single machine, general precedence constraints, minimising makespan (maximum completion time)\n\n\nP3∣dj,sjk∣∑Lj−3P3 | d_j, s_{jk} | \\sum L_j - 3P3∣dj​,sjk​∣∑Lj​−3\n3 identical machines, each job has a due date and sequence dependent setup times between jobs,minimising total lateness of jobs\n\n\nR∣∣∑CjR||\\sum C_jR∣∣∑Cj​\nvariable number of unrelated parallel machines, no constraints, minimising total completion time\n\n\n1∣dj∣∑wjTj1|d_j|\\sum w_jT_j1∣dj​∣∑wj​Tj​\nGiven nnn jobs to be processed by a single machine, each job jjj with a due date djd_jdj​ , processing time pjp_jpj​ , and a weight wjw_jwj​ , find the optimal sequencing of jobs producing the minimal weighted tardiness wjTjw_jT_jwj​Tj​\n\n\n Move Acceptance in Local search Metaheuristics, Parameter Setting issues\n Move Acceptance Methods of Local Search Metaheuristics\n\n Parameter setting mechanisms in Move Acceptance\n\n\nStatic - either there is no parameter to set or parameters are set toa fixed value (e.g. IoM=5 )\n\n\nDynamic - parameter values vary with respect to time/iteration count. Given the same candidate and current solutions at the same current elapsed time or iteration count, the acceptance threshold or acceptance probability would be the same irrespective of search history\n(e.g. IoM=round(1+(itercurrent/itermax)∗4)IoM = round(1 +(itercurrent / itermax) * 4)IoM=round(1+(itercurrent/itermax)∗4))\n\n\nAdaptive - Given the same candidate and current solutions at the same current elapsed time or iteration count, the acceptance threshold or acceptance probability is not guaranteed to be the same as one or more components depend on search history\n(e.g. if for 100 steps best solution found so far cannot be improved, then IoM++, and after any improvement, reset IoM=1)\n\n\n Non-stochastic &amp; Basic Move Acceptance Methods\n\nReuse the objective values of previously encountered solutions for the accept/reject decisions\nstatic\n\nall moves f′(s)f&#x27;(s)f′(s)\nimproving moves only f(s′)&lt;f(s)f(s&#x27;) &lt; f(s)f(s′)&lt;f(s)\nimproving and equal f(s′)≤f(s)f(s&#x27;) \\leq f(s)f(s′)≤f(s)\n\n\ndynamic: none\nadaptive\n\nLate Acceptance: compares the quality of the solution with that of the solution accepted/visited LLL iterations previously st−Ls_{t-L}st−L​, and accepts the move if and only if f(s′)≤f(st−L)f(s&#x27;) \\leq f(s_{t-L})f(s′)≤f(st−L​)\n\nInitialisation: assign all elements of the list to be equal to the initial cost (objective value)\nList implementation: List for the history of the objective values of the recent solutions is implemented as a circular queue\n\n\n\n\n\n Non-stochastic &amp; Threshold Move Acceptance Method\nDetermine a threshold which is in the vicinity of a chosen solution quality, e.g. the quality of the best solution found so far or current solution, and accept all solutions below that threshold pseudocode for minimisation\n Pseudocode for minimisation\n12345678910s0 = generateInitialSolution();s, s_best = s0;// initialise other relevant parameters if there is anyREPEAT\ts&#x27; = makeMove(s, memory); // choose a neighbour of s*\tthreshold = moveAcceptance-&gt;getThreshold(s, s&#x27;, memory);\tif(f(s&#x27;) &lt;= threshold) s = s&#x27;; // else reject new solution s&#x27;\ts_best = updateBest(s, s&#x27;); //keep track of s_bestUNTIL(termination conditions are satisfied):RETURN s_best\n\n\nstatic\n\nAccept a worsening solution if the worsening of the objective value is no worse than a fixed value\n\n\n\ndynamic\n\n\nGreat Deluge\n\n\n\nFlex Deluge\n\n\n\n\nAdaptive\n\nRecord to record travel (RRT)\nExtended Great Deluge\n\nbased on Great Deluge\nFeedback is received during the search and decay-rate is updated/reset accordingly whenever there is no improvement for a long time\n\n\nModified Great Deluge\n\n\n\n Stochastic Move Acceptance\n Psuedocode\n12345678910s0 = generateInitialSolution();s, s_best = s0;REPEAT\ts&#x27; = makeMove(s, memory); // choose a neighbour of s\tP = moveAcceptance-&gt;getAcceptanceProbability(s, s&#x27;, memory);\tr = getRandomValue(); // a uniform random value between [0, 1]\tif(f(s&#x27;).isBetterThan(f(s)) || r &lt; P) s = s&#x27;; // else reject new solution s&#x27;\ts_bset&lt;-updateBest(s, s&#x27;); // keep track of s_bestUNTIL(termination conditions are satisfied);RETURN s_best;\n\n\nstatic\n\nNaive Acceptance: P is fixed, e.g. if improving P = 1.0, else P = 0.5\n\n\n\ndynamic\n\n\nSimulated Annealing : P changes in time with respect to the difference in the quality of current and previous solutions\n\n\nadvantages:\n\neasy to implement\nachieves good performance given sufficient running time\n\n\n\ndrawbacks:\n\nrequires a good parameter setting for improved performance\nHas interesting theoretical properties (convergence),but these are of very limited practical relevance\n\n\n\nPseudocode\n123456789101112131415INPUT: 𝑇0, 𝑇𝑓𝑖𝑛𝑎𝑙𝑠0 ← 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝐼𝑛𝑖𝑡𝑖𝑎𝑙𝑆𝑜𝑙𝑢𝑡𝑖𝑜𝑛();𝑇 ← 𝑇0; // initialise temperature to 𝑇0S_𝑏𝑒𝑠𝑡 ← 𝑠0; 𝑠 ← 𝑠0; // set 𝑠 and S_𝑏𝑒𝑠𝑡 to initial solutionREPEAT    𝑠′ ← 𝑝𝑒𝑟𝑡𝑢𝑟𝑏𝑎𝑡𝑖𝑜𝑛(𝑠) ; // choose a neighbouring solution of 𝑠    Δ = 𝑓(𝑠′) − 𝑓(𝑠);    𝑟 ← 𝑟𝑎𝑛𝑑𝑜𝑚 ∈[0,1]; // get a uniform random number in the range [0,1)    if(Δ &lt; 0 || 𝑟 &lt; 𝑃(Δ,𝑇) ) &#123; // if solution is non-worsening or in Boltzmann probability        s ← 𝑠′;    &#125;    S_best ← 𝑢𝑝𝑑𝑎𝑡𝑒𝐵𝑒𝑠𝑡(); // keep track of best solution    𝑇 ← 𝑐𝑜𝑜𝑙𝑇𝑒𝑚𝑝𝑒𝑟𝑎𝑡𝑢𝑟𝑒(); // decrease the temperature according to cooling scheduleUNTIL (Termination conditions are satisfied);Return S_𝑏𝑒𝑠𝑡;\n\n\nAccepting moves\n\nΔ=F(snew)−F(sold)Δ = F(s_{new}) - F(s_{old})Δ=F(snew​)−F(sold​)\nImproving moves (i.e. Δ &lt; 0, assuming minimisation, below are same) are accepted\nWorsening moves are accepted using the Metropolis criterion at a given temperature TTT\n\nfor Δ &gt; 0 accept with a Boltzman probability of P(Δ,T)=e−ΔTP(Δ, T) = e^{\\frac{-Δ}{T}}P(Δ,T)=eT−Δ​\nU(0,1)U(0, 1)U(0,1) generates a random number in [0, 1)\naccept if U(0,1)&lt;P(Δ,T)U(0, 1) &lt; P(Δ, T)U(0,1)&lt;P(Δ,T)\n\n\n\n\n\nCooling / Annealing\n\n\nAs the temperature T decreases, the probability of accepting worsening moves decreases\n\n\nStarting Temperature (T0)\n\nhot enough: to allow almost all neighbors\nnot so hot: random search for sometime\nEstimate a suitable starting temperature:\n\nReduce quickly to 60% of worse moves are accepted\nUse this as the starting temperature\n\n\n\n\n\nFinal Temperature\n\nUsually 0, however in practice, not necessary\nT is low: accepting a worse move is almost the same as T=0\nThe stopping criteria: either be a suitably low T, or “frozen” at the current T (i.e. no worse moves are accepted)\n\n\n\nTemperature Decrement\n\nLinear: $$T = T - x$$\nGeometric: $$T = T * α$$\n\nExperience: α is typically in the interval [0.9, 0.99]\n\n\nLundy Mees: $$T = \\frac{T}{1 + βT}$$\n\nOne iteration at each T, but decrease T very slowly.\nExperience: β is typically a very small value, that is close to 0 (e.g., 0.0001)\n\n\n\n\n\nIterations at each temperature\n\nOne iteration at each TTT\nA constant number of iterations at each TTT\nCompromise\n\nEither: a large number of iterations at a few TTTs, or\nA small number of iterations at many TTTs, or\nA balance between the two\n\n\nDynamically change the no. of iterations\n\nAt higher TTTs: less no. of iterations\nAt lower TTTs: large no. of iterations, local optimum fully exploited\n\n\n\n\n\nReheating\n\nIf stuck at a local optimum for a while, increase the current temperature with a certain rate\n\n\n\n\n\n\n\n\n\nadaptive\n\nSimulated Annealing with reheating:\nP is modified time to time causing partial restart –increasing the probability of acceptance of non-improving solutions\nSimulated Annealing using the best found solution so far(or in a phase) for acceptance with a cooling schedule\n\n\n\n Parameter Setting Issues and Tuning Methods\n Parameter types\n\nCategorical/symbolic/structural parameters - e.g. choice of initialisation method, choice of mutation, …\nOrdinal parameters - e.g. neighborhoods (e.g., small, medium,large),…\nNumerical/behavioural parameters\n\ninteger, real-valued, …\ne.g. population sizes, evaporation rates, …\nvalues may depend on the setting of categorical or ordinal parameters\n\n\n\n\n Parameter Setting Methods\n\n\nParameter tuning (Off-line setting):\nFinding the best initial settings for a set of parameters before the search process starts (off-line)\nE.g., fixing the mutation strength in ILS, mutation probability in genetic algorithms, etc.\nThe initial parameter setting influences the performance of a metaheuristic.\n\nSequential tuning\nDesign of Experiments\nMeta-optimisation\n\n\n\nParameter control (Online setting):\nManaging the settings of parameters during the search process (online) (dynamic, adaptive, self-adaptive)\nE.g., changing the mutation strength in ILS, changing the mutation probability in genetic algorithms during the search process\nControlling parameter setting could yield a system which is not sensitive to its initial setting\n\nDynamic\nAdaptive\n\nSelf-adaptive\n\n\n\n\n\n Parameter Tuning methods\n\n\nTraditional approaches\n\nUse of an arbitrary setting\nTrial&amp;error with settings based on intuition\nUse of theoretical studies\nA mixture of above BOOKMARK\n\n\n\nSequential tuning: fix parameter values successively (e.g., fix A=20 and tune B that is try {0, 0.3, 0.5, 0.8, 1.0 }}, then fixing the best setting for B from the previous trials and tune A that is try {20, 40,50, 60, 80})\n\n\nDesign of experiments (DoE)\n\n\nA systematic method (controlled experiments) to determine the relationship between controllable and uncontrollable factors (inputs to the process, variables) affecting a process(e.g. running of an algorithm), their levels (settings) and the response (output) of that process (e.g. quality of solutions obtained performance of an algorithm)\n\n\nImportant outcomes are measured and analysed to determine the factors and their settings that will provide the best overall outcome\n\n\nFractional Factorial Designs - designed to draw out valuableconclusions from fewer runs (&lt;-&gt; a number of factors is k in an n level factorial design can results in n^k runs for even a single replicate)\n\nKey observation: Responses are often affected by a small number of main effects and lower order interactions,while higher order interactions are relatively unimportant\n\n\n\nSampling - whenever factorial design is not possible, sampling is performed\n\n\nrandom\n\nGenerate each sample point independently (M)\n\n\n\nLatin Hyper-cube\n\nDecide the number of sample points(M) for N variables and for each sample point remember in which row and column the sample point was taken\n\n\n\n\n\n\nOrthogonal\n\nThe sample space is divided into equally probable subspaces. Sample points simultaneously,ensuring they form an ensemble of Latin Hypercube sample\n\n\n\n\n\n\nTaguchi Orthogonal Arrays Method for Parameter Tuning\n\n\n\n\n\n\nMeta-optimisation: use a metaheuristic to obtain “optimal”parameter settings\n\n\nTaguchi Orthogonal Arrays Method for Parameter Tuning\n(Parameter Tuning methods - Design of experiments (DoE) - Sampling - Orthogonal)\n\nAim : make a “product” or “process” less variable (more robust) in the face of variation over which we have little or no control\n\n\n\n      \n Parameter Control\n\n\nStatic:\n\n\n\n\n\n\n\n\n\nfixed  parameter value\nExample:  Accept a worsening solution if the worsening of the objective value is no worse than a fixed value\n\n\nDynamic:\n\n\n\n\n\n\n\n\n\nparameter value vary with respect to time/iteration count\nExample:  Great Deluge,  Flex Deluge, Simulated Annealing\n\n\nAdaptive:\n\n\n\n\n\n\n\n\n\ndepend on search history\nExample:  Record to record travel (RRT), Extended Great Deluge, Modified Great Deluge, Simulated Annealing with reheating\n\n\n\n\n Evolutionary Algorithms (EAs) (I/II): Genetic Algorithms (GAs),Memetic Algorithms (MAs), Benchmark functions\n Evolutionary Algorithms (EAs)\n\n\n\n\n\n\n\n\n\nEAs simulate natural evolution (Darwinian Evolution) of individual structures at the genetic level using the idea of survival of the fittest via processes of selection, mutation, and reproduction (recombination)\n\nEA includes GA, GP, EP\n\n\n\nAn individual/chromosome represents a candidate solution for the problem at hand\n\n\nA collection of individuals currently “alive”, called population (set of individuals/chromosomes) is evolved from one generation (iteration) to another depending on the fitness of individuals in a given environment , indicating how fit an individual is, (how close it is to the optimal solution)\n\n\nHope: last generation will contain the final solution\n\n\n History\n\n\nGenetic Algorithms\n\nMemetic Algorithms\n\n\n\n\n\n\n\n\n\n\nevolves bit strings\n\n\nEvolutionary Programming\n\n\n\n\n\n\n\n\n\nevolves parameters of a program with a fixed structure\n\n\nEvolution Strategies\n\n\n\n\n\n\n\n\n\nvectors of real numbers\n\n\nGenetic Programming\n\n\n\n\n\n\n\n\n\nevolves computer programs in tree form\n\n\nGene Expression Programming\n\n\n\n\n\n\n\n\n\ncomputer programs of different sizes are encoded in linear chromosomes of fixed length\n\n\nGrammatical Evolution\n\n\n\n\n\n\n\n\n\nevolves solutions wrt a specified grammar\n\n\n\n\n Key Features of EAs\n\npopulation based search approaches\n\nBe independent of initial starting point(s) - Start search from many points in the search space\nConduct search in parallel over the search space - implicit parallelism\n\n\nAvoid converging to local optima\n\n Weakness\n\nLimited theoretical and mathematical analyses - this is a growing field of study\nConsidered slow for online applications and even for some large offline problems\n\n Genetic Algorithms(GAs)\n Pseudocode\n1234567891011begingenerate initial population; // initialisecalculate fitness values; //evaluate populationdo&#123;\tperform reproduction; //select parents\trecombine pairs with p_c; // apply corssover\tapply mutation with p_m; // mutate offspring\tcalculate fitness values; // eval. population\treplace current population;&#125; while termination criteria not satisfied;end\n Basic components of GAs\n\nA genetic representation (encoding) for candidate solutions(individuals) to the problem at hand\nAn initialisation scheme to generate the first population (set) of candidate solutions (individuals)\nA fitness (evaluation) function that plays the role of the environment, rating the solutions in terms of their fitness\nA scheme for selecting mates (parents) for recombination\nCrossover (recombination) exchanges genetic material between mates producing offspring children\nMutation perturbs an individual creating a new one\nReplacement strategy to select the surviving individuals for the next generation\nTermination Criteria\nValues for various parameters that GA uses (population size, probabilities of applying genetic operators, etc)\n\n Representation\n\nHaploid structure: each individual contains one chromosome\nchromosome contain a fixed number of genes: chromosome length\neach individual is evaluated and has an associated fitness value\nTraditionally binary encoding is used for each gene: Allele value ∈\\in∈ {0, 1}\na population contains a fixed number of individuals: population size\neach iteration is referred as generation\n\n Initialisation\n\nRandom initialisation\n\nPopulation size number of individuals are created randomly\nEach gene at a locus of an individual is assigned an allele value 0 or 1 randomly\n\n\n\n Fitness calculation\n\nFitness value indicates “how fit the individual is to survive and reproduce under the current conditions”\ni.e. how much the current solution meets the requirements of the objective function\nis obtained by applying the fitness function to the individual’s chromosome (candidate solution)\ni.e. genotype (e.g. 101110) to phenotype (e.g.1) mapping\n\n Reproduction\n\n\nselecting individuals : apply selection pressure considering the fitness of individuals in the population (e.g. roulette wheel selection, tournament selection, rank selection, truncation selection, Boltzmann selection, …)\n\nSelection pressure means the individuals with better fitness have higher chance for being selected\n\n\n\nusually 2 parents (individuals/candidate solutions) are selected using the same method, which will go under the crossover operation\n Roulette Wheel Selection\n\nFitness level is used to associate a probability of selection(probiprob_iprobi​) with each individual chromosome\nExpected number of representatives of each individual in the pool is proportional to its fitness\n“While candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be”\n\n Tournament Selection\n\nThis method involves running a number of &quot;tournaments&quot;among randomly chosen individuals (of tour size)\nselecting the one with best fitness at the end\n\n\n\n Crossover / Recombination\n\n\napplied with a crossover probability pcp_cpc​ which in general is chosen to 1.0\n One Point Crossover(1PTX)\n\nGenerate a random number in [0, 1), if it is smaller than a crossover probability pcp_cpc​ Then\n\nSelect a random crossover site in [1, chromosome length]\nSplit individuals at the selected site\nExchange segments between pairs to form two new individuals\n\n\nElse\n\nCopy the individuals as new individuals\n\n\n\n\n\n\n 2 Point Crossover (2PTX)\n K-point Crossover\n Uniform Crossover (UX)\n\nconsiders each bit in the parent strings for exchange with a probability of 0.5\n\n\n\n\n\n\n Mutation\n\nprovides diversity and allows GA to explore different regions of the search space (escaping)\nLoop through all the alleles of all the individuals one by one, and if that allele is selected for mutation with a given probability you can either change it by a small amount or replace it with a new value (for binary representation, flipping a gene value)\n\nMutation rate is typically chosen to be very small (e.g. 0.001)\nChoosing pmp_mpm​ as (1 / chromosome length) implies on average a single gene will be mutated for an individual\n\n\n\n Replacement\n\n\nGeneration gap (ααα) controls the fraction of the population to be replaced in each generation, where $α \\in [1/N,1.0] $Number of offspring produced at each generation is g=α∗Ng=α*Ng=α∗N\n\n\n(Trans-) Generational GA\nNNN individuals produce αNαNαN offspring, so (N+αN)→N(N + αN) → N(N+αN)→N\n\nαNαNαN replaces worst αNαNαN of NNN\n\nlargest generation gap where α=1.0α=1.0α=1.0 yields g=Ng=Ng=N.\nGA relies on improvement of average objective values from one population to another\n\nIt is always a good idea not to loose the best solution found so far.\n\n\n\n\nsort (N+αN)(N + αN)(N+αN) and choose the NNN best (elitism)\n\n\n\nSteady-State GA (g=2, that is α=2/N)\nTwo offspring replace two individuals from the old generation.\n\nMethod#1: two offspring replace two parents\nMethod#2: two offspring replace worst two of the population\nMethod#3: best two of (parents and offspring) replace two parents (elitism)\nMethod#4: best two of (parents and offspring) replace worst two of the population (strong elitism)\n\n\n\n Termination Criteria\n\nA predefined maximum number of generations is exceeded\nA goal is reached (e.g. Expected fitness is achieved, Population converges)\nBest fitness does not change for a while\nA condition is satisfied depending on a combination of above\n\n Convergence - definition\n\nDefined as the progression towards uniformity (individuals become alike)\nGene convergence: a location on a chromosome is converged when 95% of the individuals have the same gene value for that location\nPopulation (Genotypic) convergence: a population is converged when all the genes have converged (all individuals are alike they might have different fitness)\nPhenotypic Convergence: average fitness of the population approaches to the best individual in the population (all individuals have the same fitness)\n\n Memetic Algorithms (MAs)\n\n\n\n\n\n\n\n\n\nMeme: contagious piece of information\n\nMemes are similar to local refinement/local search\nGene vs. Meme\n\nMemes can change, evolve using rules and time scales other than the traditional genetic ones\n\n\n\n\nMAs aim to improve GAs by embedding local search\n\nMAs are much faster and accurate than GAs on some problems\n\n\nMAs make use of exploration capabilities of GAs and exploitation capabilities of local search (i.e. an explicit mechanism to balance exploration and exploitation)\n\n\n Benchmark functions\n\n\n\n\n\n\n\n\n\nBenchmark functions serves as a testbed for performance comparison of (meta/hyper)heuristic optimisation algorithms\n Why use benchmark functions\n\nTheir global minimum are known\nThey can be easily computed\nEach function is recognised to have certain characteristics potentially representing a different real world problem\n\n Classification\n\n\nContinuity (Differentiability)\n\n\nf(x)=∣x∣f(x)=∣x∣f(x)=∣x∣, continuous but not differentiable\n\n\ndiscontinuous vs. continuous\n\n\n\n\nDimensionality (Scalability)\n\n\nModality\n[The number of ambiguous peaks in the function landscape]\n\nUnimodal\nMultimodal with few local minima\nMultimodal with exponential number of local minima\n\n\n\nSeparability\n[each variable of a function is independent of the other variables; A function of variables is separable if it can be rewritten as a sum of functions of just one variable (note: see examples for comprehensive understanding)]\n\nseparable functions allows delta evaluation\n\n Examples\n\nf(x)=∑i=1nxi2f(x) = \\sum_{i=1}^{n}x_i^2f(x)=∑i=1n​xi2​\n\ncontinuous, differentiable, separable, scalable\n\n\nf(x)=∑i=1n(⌊∣xi∣⌋)f(x) = \\sum_{i=1}^{n}(⌊∣x_i∣⌋)f(x)=∑i=1n​(⌊∣xi​∣⌋)\n\ndiscontinuous, non-differentiable, separable, scalable\n\n\nf(x,y)=−20exp(−0.20.5(x2+y2)−exp(0.5(cos(2πx)+cos(2πy)))+e+20f(x, y) = -20exp(-0.2\\sqrt{0.5(x^2+y^2)}-exp(0.5(cos(2\\pi x) +cos(2\\pi y)))+e+20f(x,y)=−20exp(−0.20.5(x2+y2)​−exp(0.5(cos(2πx)+cos(2πy)))+e+20\n\ncontinuous, differentiable, non-separable, non-scalable\n\n\n\n\n\n Evolutionary Algorithms (EAs) (II/II): MA &amp; GA (cont.), Multimeme Memetic Algorithms (MMAs)\n Case study of GAs/MAs\n Binary Coding vs. Gray Coding\n\nGray encoding ensures a Hamming distance of 1 for the adjacent numbers\nShown to be useful in GAs empowering the algorithm to mutate a solution in the right direction\n\n Binary Representation for Encoding Permutation / Permutation basedGenetic Operators\n\n Partially Mapped Crossover (PMX)\n\n Order Crossover (OX)\n\n\n Cycle Crossover (CX)\n\n Multimeme Memetic Algorithms (MMAs)\n Self Adaptation\n\n\n\n\n\n\n\n\n\nDeciding which operators and settings to use on the fly whenever needed receiving feedback during the evolutionary search process\n\n\ni.e. co-evolve genetic and memetic material\n\nMultimeme Algorithm can indeed learn how to choose an operator and relevant settings through the evolutionary process (co evolution)\nThere is a trade off as learning requires time and a memetic algorithm with a single setting could perform better\n\n\n\nMemes represent instructions for self improvement\n\nSpecify set of rules, programs, heuristics, strategies,behaviors, etc.\n\n\n\nMeme of each operator can be combined under a memeplex\n\n\nGrammar for memeplex - Compound of Memes\n\n\n\nExample\n\n\n\n Mutating Memes during evolution\n\n\nInnovation rate IR∈[0,1]IR\\in [0, 1]IR∈[0,1] : the probability of mutating the memes\n\nIR=0IR = 0IR=0 - no innovation\n\nif a meme option is not introduced in the initial generation, it will not be reintroduced again\n\n\nIR=1IR=1IR=1\n\nAll different strategies implied by the available MMM memes might be equally used\n\n\n\n\n\nConcentration of a meme Ci(t)C_i(t)Ci​(t)\n\ntotal number of individuals that carry the meme iii at a given generation ttt\nCrude measure of a meme success; gives no information about continual usage of a meme\n\n\n\nEvolutionary activity of a meme ai(t)a_i(t)ai​(t)\n\nthe accumulation of meme concentration until a given generation\na_i(t) = \\left\\{\\begin{array}{**lr**}  \\int_0^t c_i(t)dt, &amp; \\text{if } c_i(t) &gt; 0 \\\\ 0, &amp; \\text{otherwise}\\\\  \\end{array}   \\right.\nSlope in a plot represents the rate of increase of a meme concentration\n\n\n\n Hyper-heuristics (I/II): Motivation / Characteristics / Classification /Misconceptions of Hyper-heuristics, Selection Hyper-heuristic controlling Perturbative Heuristics\n Hyper-heuristics\n\n\n\n\n\n\n\n\n\nA hyper-heuristic is a search method or learning mechanism for selecting or generating heuristics to solve computationally difficult problems\nA class of methodologies for cross domain (i.e. no domain knowledge) search\n\n Motivation\nraise the level of generality of search methods / cross domain research\n Characteristics of Hyper heuristics (initial framework)\n\nOperate on a search space of heuristics (neighborhood operators) rather than directly on a search space of solutions\nAim is to take advantage of strengths and avoid weaknesses of each heuristic (operator)\nNo problem specific knowledge is required during the search over the heuristics (operator) space\nEasy to implement, practical to deploy (easy, cheap, fast)\nExisting (or computer generated) heuristics (operators) can be used within hyper heuristics\n\n Classification of Hyper-heuristics\n\n\nGeneration hyper-heuristics\nautomatically construct new heuristics from given components\nSelection hyper-heuristics\nchoose and control a predefined set of heuristics\nOffline learning hyper-heuristics\n\nusually trained on a set of selected instances and then generalise to unseen instances\n\n\nOnline learning hyper-heuristics\n\nreceive feedback during the search process while solving a given instance of a problem\n\n\n\n Misconceptions about Hyper-heuristics\n\nHyper-heuristics do not require parameter tuning (✕)\n\nrequire parameter tuning (unless have a parameter control mechanism)\n\n\nHyper-heuristics are all tested under a fair setting (HyFlex) (✕)\n\nTime allocated (and instances used) for tuning - no one seems to take into account the time spent for parameter tuning, much time on tuning\n\n\nApplying a hyper heuristic to a new domain is easy (✕)\n\nthe hyper-heuristic part itself is easy, but decide who is going to implement these new operators in the new domain / choose of domain-specific operators/heuristics is hard\n\n\nDomain specific information should not be passed to the hyper-heuristics (objective value is not a domain specific information, all others are) (✕)\n\nobjective value is domain specific information, and, the hyper-heuristic, as an interface, should allow such information to be passed, and, with domain-specific information hyper-heuristic can be more capable\n\n\n\n\n Selection Hyper-heuristic controlling Perturbative Heuristics\n A Selection Hyper heuristic Framework - Single Point Search\n1234567generate initial candidate solution pwhile(termination critera not satisfied)&#123;\tselect a heuristic (or subset of heuristic) h from &#123;H1,..., Hn&#125;\tgenerate a new solution (or solutions) s by applying h to p\tif(s is accepted) tehn p = s; // decide whether to accept s or not&#125;return p;\n\n Hyper-heuristics Flexible Interface (HyFlex) - Multi-Point / Population Search\n\n\nHeuristic types: mutational (MU), ruin-recreate (RC), local search(HC), crossover (XO)\nParameters: intensity of mutation, depth of search\n\n Heuristic (Operator) selection methods\n\n\nSimple Random / Random Permutation - with no learning\n\n\nGreedy (GR) - with learning\n\nApply each low level heuristic to the candidate solution and choose the one that generates the best objective value\n\n\n\nReinforcement Learning (RL) - with learning\n\nA machine learning technique\nConcerned with how an agent ought to take actions in an environment to maximize some notion of long term reward\nreward and punishment\nMaintains a score for each heuristic\nIf an improving move then increase (e.g. +1) other wise decrease (e.g. - 1) the score of the heuristic\n\n\n\nChoice Function (CF) - with learning\n\nmaintains a record of the performance of each heuristic with 3 criteria\n\nits individual performance (f1f_1f1​)\nhow well it has performed with other heuristics (f2f_2f2​)\nthe elapsed time since the heuristic has been called (f3f_3f3​)\n\n\nFn(hj)=αnf1(hj)+βnf2(hk,hj)+γnf3(hj)F_n(h_j) = \\alpha _n f_1(h_j) + \\beta _n f_2(h_k, h_j) + \\gamma _n f_3(h_j)Fn​(hj​)=αn​f1​(hj​)+βn​f2​(hk​,hj​)+γn​f3​(hj​)\n\n\n\nAn Iterated Multi-stage Selection Hyper-heuristic\n\n\nCrossover operators are ignored\n\n\n\n\n\nStage 1 Hyper-heuristic (S1HH)\n\n\nselect a low level heuristic iii with probability scorei/∑∀kscorekscore_i / \\sum _{\\forall k} score_kscorei​/∑∀k​scorek​\n\n\napply the chosen heuristic\n\n\nAccept/reject based on an adaptive threshold acceptance method\n\n\n\nStage 1 terminates if a duration of is exceeded without any improvement\n\n\n\n\nStage 2 Hyper-heuristic (S2HH)\n\nGiven NNN LLHs, pair up all (which increase the number of LLHs to N+N2N + N^2N+N2), then reduce the number of LLHs (N+N2→nN+N^2\\rightarrow nN+N2→n)  and assign probabilities\nparameter tuning needed (6 parameters for this specific framework)\n\n\n\n\n\n Hyper-heuristics (II/II): Selection Hyper-heuristic controlling Constructive Heuristics, Generation Hyper-heuristics (Genetic Programming (GP))\n Selection Hyper-heuristic controlling Constructive Heuristics\n A Graph-based Hyper-heuristic (GHH)\n\n\nGraph colouring problem - no two adjacent vertices share the same colour (vertex colouring)\n\n\nminimum colouring problem (an NP-hard problem)\n\n\ndegree of a vertex: number of edges connected to that vertex\n\n\nsaturation degree of a vertex: number of differently coloured vertices already connected to it\n\n(note : 1. coloured; 2.differently)\n\n\n\n\n\n Graph colouring heuristics\n\nlargest degree\n\nsort the vertices from largest degree to smallest\ncolour the first vertex in the list with an colour\n\n(starting with the first) that is different than its neighbours\n\n\ndelete the vertex from the list go to the previous step unless no vertices left\n\n\nsaturation degree\n\nuse saturation degree instead of degree in largest degree\n\n\n\n Case study: examination timetabling problem\n\n\nA general framework employing a set of low level constructive graph colouring heuristics\n\nLow level heuristics: sequential methods that order events by the difficulties of assigning them\n\n\n\nmodel the problem as a graph colouring problem\n\nNodes: exams\nEdges: adjacent exams have common students\nColours: time periods\nObjective: assign colours (time periods) to nodes (exams),adjacent nodes with different colour, minimising time periods used\n\n\n\nSelect low level heuristics\n\n\n\n\n\n\n\na specific pseudo-code (i.e. only one specific way) of Tabu Search graph based hyper-heuristic\n\n(note:‘events’=‘exams’)\n\n12345678910111213141516171819initial heuristic list hl = &#123;h1, h2, h3,..., hk&#125;// Begin of Tabu Searchfor i = 0 to i = (5 * the number of events) // number of iterations\th = change two heuristic in hl // a move in Tabu search\tif h does not match a heuristic list in &#x27;failed list&#x27;\t\tif h is not in the tabu list // h is not recently visited\t\tfor j = 0 to j = k // h is used to construct a complete solution\t\t\tschedule the first 2 events in the event list ordered using hj\t\t\tif no feasible solution can be obtained\t\t\t\tstore h into &#x27;failed list&#x27; // update &quot;failed list&quot;\t\t\telse if cost of solution c &lt; the best cost c_g obtained\t\t\t\tsave the best solution, c_g = c //keep the best solution\t\t\t\tadd h into the tabu list\t\t\t\tremove the first item from the tabu list if its length &gt; 9\t\thl = h\t//end if\tDeepest descent on the complete solution obtained//end of Tabu searchoutput the best solution with cost of c_g\n\nNeighborhood operator: randomly change two heuristics in the heuristic list\nObjective function: quality of solutions built by the corresponding heuristic list\nTabu list: visits to the same heuristic lists forbidden\n\n\n\nother high-level search strategies: steepest descent, Variable neighborhood search -&gt; best performing, iterated steepest descent, …\n\n\n Generation Hyper-heuristics\n Genetic programming(GP)\n\n\nGP provides a method for automatically creating a working computer program from a high level problem statement of the problem\n\ni.e. program synthesis / program induction\n\n\n\nGP iteratively transforms a population of computer programs into a new generation of programs via evolutionary process\n\n\nrepresent a computer program as a parse tree\n\n\n\nGP is an evolutionary algorithm containing the same algorithmic components , including\n\nRandom generation of the initial population of possible solutions (programs)\n\nRandomly generate a program that takes two (or more) arguments and uses basic arithmetic to return an answer\n\nFunction set = {+,−,∗/}\\{+, -, * /\\}{+,−,∗/}\nTerminal set = {integers,X,Y}\\{integers, X, Y\\}{integers,X,Y}\n\n\nRandomly select either a function or a terminal to represent our program\nIf a function was selected, recursively generate random programs to act as arguments\n\n\nGenetic crossover of two promising solutions to create new possible solutions (programs)\n\nPick a random node in each program\nSwap the two nodes\n\n\nMutation of promising solutions to create new possible solutions (programs)\n\nFirst pick a random node\nDelete the node and its children, and replace with a randomly generated program\n\n\nFitness measure that is used to evaluate a given evolved program\n\nGP is often operated in a train and test fashion training on selected sample instances could take long time while application to unseen instances is generally fast\nEach tree generated by GP can be evaluated using an indicator showing how good it is in building high quality solutions to the sample problem instances, such as, mean quality of solutions over the sample instances\n\n\nTermination criteria\n\n\n\nCase study: Genetic Programming for Packing\n\nError: GP at a specific time (note: ‘%’ should be ‘/’)\n\n      \n\nNow\n\nNow putting the item of size 70 into the bin\ncalculate the fitness measure based on the program,\nfind the max fitness value\nNext put item of size 70 into bin 4\nNext start search for item of size 85\n\n\nafter putting all items in bins (i.e. the program finishes on the test instance)\n\nrecord number of bins used and compare with the best\nthen do crossover / mutation …for new programs\n\n\n\n\n\n\n Advanced topics\n Case study: Policy Matrix Evolution for Generation of Heuristics on 1D Online Bin Packing Problem\n Index Policy\n\n\n\n\n\n\n\n\n\nEach choice option is given a score, or “index value” independently of the other options\n\nindex policies for 1D online bin packing\n\nscore of bin is f(r,s)f(r, s)f(r,s) where rrr is the remaining capacity of bin and sss is item size\nGiven a new item of size then place into bin with largest value of f(r,s)f(r, s)f(r,s)\n\n\n\n Open / Close\n\nA new empty bin is always available ( open )\nA bin is closed if it can take no more items\n\ne.g. if residual space is smaller than size of any item\n\n\n\n Potential General Method for 1D Online Bin Packing\n\n\n(i.e. method to assign a new item upon its arrival to one of the open bins)\n\n\non arrival of new item, inspect the current set of open bins, and simultaneously use the entire set of residual spaces in the open bins to pick where to place the new item\n\n\ndifficult expensive (in general)\n\n\n Generating Heuristic\n\nWithin search methods, often have score functions, “index functions” to help make some choice\n\ndifficult to invent successful ones; want to automate this\n\n\nGP approach: evolve arithmetic score functions\n\nUse Genetic Programming to learn f(r,s)f(r,s)f(r,s)\nf(r,s)f(r,s)f(r,s) is represented as arithmetic function tree\nAutomatically creates functions that at least match FF, BF\n\n\nChallenge:\n\nis hard to understand\npotentially biased because of the choice of representation\nsome perfectly good functions might have “bloated” representations\n\n\n\n Matrix View of Policies/Heuristics\n\n\nSince all item sizes (sss) and residual capacities (rrr) are integer,then f(r,s)f(r,s)f(r,s) is simply a large (C∗CC*CC∗C) matrix M(r,s)M(r,s)M(r,s)of parameter values\n\n\n\nUniform (random) Instances\n\nproblem generator on Uniform Bin Packing Problems: UBP(C,smin,smax,N)UBP(C,s_{min}, s_{max}, N )UBP(C,smin​,smax​,N)\nCCC - bin capacity\nNNN number of items with integer sizes taken randomly from range [smin,smax][s_{min}, s_{max}][smin​,smax​]\n\n\n\nCreating Heuristics via Many Parameters (CHAMP)\n\n\nBasic idea:\n\nTake values in matrix M(r,s)M(r,s)M(r,s) to be integers\nDo (meta-)heuristic search to find good choices for M(r,s)M(r,s)M(r,s): Evaluation is by simulation\n\n\n\nOriginal Expectation\n\nthe matrix will tweak the functions from GP and might slightly improve performance\n\n\n\nPotential expected disadvantages\n\nmatrices can be much more verbose than functions\nthey fail to take into account of the good structure captured by functions\n\n\n\n\n\n\nConclusion\n\nPolicies exist that out-perform standard heuristics\nFinding the policies is easier than expected\nThere are many different policies with similar performance\nThe policies are “weirder” than expected\n\nThe good policies could have “random” structures\nNot necessarily easy to capture with an algebraic function of GP\n\n\nThe results can be “analysed” (inspected) to produce simple policies that out-perform standard ones\n\nand that then scale to larger problems\n\n\n\n\n\n A Data Science Improved Hyper-heuristic\n\n\n\n\n\n\n\n\n\nMany real-world data are multidimensional\n\nVery high-dimensional (big) with a large amount of redundancy\n\n Proposed Approach – Ideas\n\nThe balance between exploration and exploitation is crucial\nMix move acceptance methods\nUse machine learning to partition the low level heuristics associated with each method\n\n Summary\n\nHyper-heuristic research originated from a job shop scheduling application and has been rapidly growing since then.\nGeneration hyper-heuristics are commonly used in the area\n\nTrain and test fashion\n\nDoes the selected subset of training instances is sufficiently representative of the test set?\nTraining is time-consuming (delta/incremental evaluation, surrogate functions)\n\n\nThe generated/evolved heuristics might not be easy to interpret, yet they can outperform human designed heuristics\n\n\nThere is empirical evidence that machine learning/analytics/ data science help to improve the hyper-heuristic search process\n\nProblem features vs solution/state features\nOffline versus online learning – Life long learning\n\n\nThere is still a lack of benchmarks\nAutomated design of search methodologies is extremely challenging\n\nAddressed in almost complete absence of a mathematical and theoretical understanding\n\n\n\n","slug":"aim","date":"2024-06-09T15:37:56.000Z","categories_index":"Notes","tags_index":"AI,Heuristics,Optimization","author_index":"Huaji1hao"},{"id":"e36df18b7817d5ce63296b0aa4c51d03","title":"Algorithms, Data structure & Efficiency","content":" Algorithms, Data structure &amp; Efficiency\n\nAlgorithm: step-by-step procedure for solving a problem in a finite amount of time.\n\nNo “MAGIC steps” allowed! (No “Zeno machines”)\n\n\nData structures: lists, binary trees, heaps, Hash maps, graphs, etc.\nEfficiency: We will start with methods to analyse, classify and describe the efficiency of algorithms\n\nNeeded in order to be able to select “best algorithms”\n\n\n\n Big-Oh family\n Big-Oh Notation\n\n\n\n\n\n\n\n\n\nDefinition: Given positive functions f(n)f(n)f(n) and g(n)g(n)g(n), then we say that\nf(n)f(n)f(n) is O(g(n))O( g(n) )O(g(n))\nif and only if there exist positive constants ccc and n0n_0n0​ such that\nf(n)≤cg(n)f(n)\\leq c g(n)f(n)≤cg(n) for all n≥n0n \\geq n_0n≥n0​\n\n\ni.e. “exists-exists-forall” structure:\n\n∃c&gt;0,∃n0,such  that  ∀n≥n0,f(n)≤cg(n)\\exist c&gt;0,\\exist n_0,such \\;that\\;\\forall n \\geq n0, f(n) \\leq c g(n)∃c&gt;0,∃n0​,suchthat∀n≥n0,f(n)≤cg(n)\n\n\n\n‘OOO’ expresses “grows at most as fast as”\n\n\n Big-Oh as a set\n\nBig Oh as a binary relation is reflexive and transitive but not symmetric\n\nIt behaves like “⊆⊆⊆”, “∈∈∈” or “≤≤≤”, not like “===”\nOne might say n∈O(n)n ∈O(n)n∈O(n), and 2n+3∈O(n)2n+3 ∈O(n)2n+3∈O(n), etc\n\n\n\nSo may help to think of “O(n)O(n)O(n)” as a set of functions, with each function f in the set, f∈O(n)f ∈O(n)f∈O(n), satisfying “fff is O(n)O(n)O(n)”.\n\nOr can say: {f}⊆O(n)\\{f\\} ⊆O(n){f}⊆O(n)\nSo then O(1)⊆O(n)O(1) ⊆O(n)O(1)⊆O(n)\n\n\n Rules for Finding big-Oh\n “Multiplication Rule” for big-Oh\n\nSuppose\n\nf1(n)f_1(n)f1​(n) is O(g1(n))O( g_1(n ) )O(g1​(n))\nf2(n)f_2(n)f2​(n) is O(g2(n))O( g_2(n) )O(g2​(n))\n\n\nThen, from the definition, there exist positive constants c1,c2,n1,n2c_1,c_2,n_1,n_2c1​,c2​,n1​,n2​ such that\n\nf1(n)≤c1g1(n)f_1(n) \\leq c_1g_1(n)f1​(n)≤c1​g1​(n) for all n≥n1n ≥ n_1n≥n1​\nf2(n)≤c2g2(n)f_2(n) \\leq c_2g_2(n)f2​(n)≤c2​g2​(n) for all n≥n2n ≥ n_2n≥n2​\n\n\nLet n0=max(n1,n2)n_0= max(n1, n2)n0​=max(n1,n2), then multiplying gives\nf1(n)f2(n)≤c1c2g1(n)g2(n)f_1(n) f_2(n) \\leq c_1 c_2g_1(n) g_2(n)f1​(n)f2​(n)≤c1​c2​g1​(n)g2​(n) for all n≥n0n ≥ n_0n≥n0​\nSo f1(n)f2(n)f_1(n) f_2(n)f1​(n)f2​(n) is O(g1(n)g2(n))O( g_1(n) g_2(n) )O(g1​(n)g2​(n))\n\n Big-Oh Rules: Drop smaller terms\n\nIf f(n)=(1+h(n))f(n)= ( 1 + h(n) )f(n)=(1+h(n)) with h(n)→0h(n) →0h(n)→0 as n→∞n →∞n→∞\nThen f(n)f(n)f(n) is O(1)O(1)O(1)\n(The utility will be to combine with the multiplication rule).\n\n Summary for finding big-Oh\nIf f(n)f(n)f(n) is a polynomial of degree ddd, (with positive largest term) then f(n)f(n)f(n) is O(nd)O(n^d)O(nd), i.e.,\n\nDrop lower-order terms\nDrop constant terms\n\nNote: degree of a polynomial is the highest power e.g. 5n4+3n25 n^4+ 3 n^25n4+3n2 is degree 444 and so will be O(n4)O(n^4)O(n4)\n Big-Omega\n\n\n\n\n\n\n\n\n\nDefinition: Given functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is Ω(g(n))Ω( g(n) )Ω(g(n))\nif there are (strictly) positive constants ccc and n0n_0n0​ such that\nf(n)≥cg(n)f(n)≥c g(n)f(n)≥cg(n) for all n≥n0n \\geq n_0n≥n0​\n\n\nNote that need c&gt;0c &gt; 0c&gt;0, and we not allowed c=0c=0c=0\n\n\nNote that ccc must be constant (cannot depend on nnn)\n\n\nf(n)f(n)f(n) is Ω(g(n))Ω(g(n))Ω(g(n)) says that “f(n)f(n)f(n) grows at least as fast as g(n)g(n)g(n) at large nnn”\n\n\n Linking big-Oh and Big-Omega\n\nThat is: 𝑓∈𝑂(𝑔)→𝑔∈Ω(f)𝑓∈𝑂(𝑔)→𝑔∈Ω(f)f∈O(g)→g∈Ω(f)\nSimilarly: 𝑓∈Ω(𝑔)→𝑔∈O(f)𝑓∈Ω(𝑔)→𝑔∈O(f)f∈Ω(g)→g∈O(f)\nNote: similar to: x&lt;=y→y&gt;=xx &lt;= y →y &gt;= xx&lt;=y→y&gt;=x\n\n Big-Theta\n\n\n\n\n\n\n\n\n\nDefinition: Given functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is Θ(g(n))Θ( g(n) )Θ(g(n))\nif there are positive constants c’c’c’, c’’c ’’c’’ and n0n_0n0​ such that\nf(n)≤c’g(n)≤c’’g(n)f(n)≤c’g(n) \\leq c ’’g(n)f(n)≤c’g(n)≤c’’g(n)\nfor all n≥n0n \\geq n_0n≥n0​\n\nΘΘΘ expresses “grows ‘exactly’ as fast as\n\n Θ is an equivalence relation\nAny relation that is Reflexive &amp; Symmetric &amp; Transitive is an “equivalence relation”\n\nRoughly speaking: it behaves like a “equality”:\n\nIt is reasonable to write “𝑓=Θ(𝑔)𝑓=Θ(𝑔)f=Θ(g)”\n Little-Oh\n\n\n\n\n\n\n\n\n\nDefinition: Given (positive) functions f(n)f(n)f(n) and g(n)g(n)g(n), we say that\nf(n)f(n)f(n) is o(g(n))o( g(n) )o(g(n))\nif for all positive (real) constants c&gt;0c &gt; 0c&gt;0\nthere exists n0n_0n0​ such that\nf(n)&lt;cg(n)f(n)&lt;c g(n)f(n)&lt;cg(n) for all n≥n0n  \\geq n_0n≥n0​\n\n\nSpot the difference from big-Oh?\n\n“for all c&gt;0c &gt; 0c&gt;0” rather than “there exists c&gt;0c &gt; 0c&gt;0”\n(The change of “≤≤≤” to“&lt;&lt;&lt;“ is much less important – see later.)\n\n\n\nSays (roughly) that the ratio f(n)/g(n)→0f(n)/g(n) →0f(n)/g(n)→0 as n→∞n →\\infinn→∞\n\n\nNote that n0n_0n0​ is allowed to depend on ccc\n\n\nooo expresses “strictly less than”\n\n\n\n Linked list\n Singly Linked List\n\n\n\n\n\n\n\n\n\nA singly linked list is a concrete data structure consisting of a sequence of nodes\nEach node stores\n\nelement e.g.\n\nReference to an Object\nA primitive date type (int,…)\n\n\n“link”: a reference (pointer) to the next node\n\n\n Inserting at the Head\n\n\nAllocate a new node\n\n\nInsert new element\n\n\nHave new node point to old head\n\n\nUpdate head to point to new node\n\n\nWhat is the complexity (with n elements in list)?\n\nAnswer: O(1)\nVery efficient!\n\n Removing at the Head\n\nUpdate head to point to next node in the list\nAllow garbage collector to reclaim the former first node\n\nOr do explicit free in C/C++\n\n\n\nAgain, the operation is O(1), and so efficient.\n Inserting at the Tail\n\nAllocate a new node\nInsert new element\nHave new node point to null\nHave old last node point to new node\nUpdate tail to point to new node\n\nComplexity: O(1)\n Removing at the Tail\nTo find new tail we must walk the list from the head\n\nThere is no constant-time way to update the tail to point to the previous node\n\nComplexity: O(n)\n Doubly Linked List\n\n\n\n\n\n\n\n\n\nA doubly linked list provides a natural implementation of a List\n\nNodes implement “Position” and store:\n\nelement\nlink to the next node\nlink to the previous node\n\n\nDeletion at the tail is now O(1)\nBut uses more memory\n\n\n Insertion Algorithm\n12345678Algorithm addAfter(p,e):    Create a new node v    v.setElement(e)    v.setPrev(p) //link v to its predecessor    v.setNext(p.getNext()) //link v to its successor    (p.getNext()).setPrev(v) //link p’s old successor to v    p.setNext(v) //link p to its new successor, v    return v //the position for the element e\n\n Simple Sorting Algorithm\n Bubble Sort\n Basic Idea\n\nOuter loop:\n\nRepeated scans through array\n\n\nInner loop: on each scan do comparison with immediate neighbour\n\nthink of air bubbles rising in water\ndo swaps to make sure that the largest number “bubbles up” to the end of the array\n\n\n\n Pseudocode\n12345678910111213void bubbleSort(int arr[])&#123;    int i;    int j;    int temp;    for(i= arr.length - 1; i &gt; 0; i--)&#123;        for(j = 0; j &lt; i; j++)&#123;            if(arr[j] &gt; arr[j+1])&#123;            temp = arr[j];            arr[j] = arr[j+1];            arr[j+1] = temp;        &#125;// end inner loop    &#125;//end outer loop&#125;// end bubble sort\n Complexity of bubble sort\nAll together: t((n−1)+(n−2)+...+1)+k+t1(n−1)t ((n-1) + (n-2) + ... + 1)+ k + t_1(n-1)t((n−1)+(n−2)+...+1)+k+t1​(n−1)\n\n\nwhere ttt is the time required to do one comparison, one swap, check the inner loop condition and increment jjj.\n\n\nWe also spend constant time kkk declaring i,j,tempi,j,tempi,j,temp and initialising iii. Outer loop is executed n−1n-1n−1 times, suppose the cost of checking the loop condition and decrementing iii is t1t_1t1​.\n\n\n(worst-case) complexity O(n2)O(n^2)O(n2) by taking c=t+t1+kc = t + t_1+ kc=t+t1​+k and n0=1n_0= 1n0​=1.\n\n\n Selection Sort\n Basic Idea\nInstead of always try to move the “greatest element so far” immediately, we just remember its location and move it at end of scan\n Why one want to do this?\n\nSuppose that the entries are large then a swap operation might be quite expensive\nSo might want to reduce the number of swaps by directly moving entries to “the right place”\n\n Pseudocode\n123456789101112131415void selectionSort(int arr[])&#123;    int i, j, temp, pos_greatest;    for( i = arr.length-1; i &gt; 0; i--)&#123;        pos_greatest = 0;        for(j = 0; j &lt;= i; j++)&#123;        \tif( arr[j] &gt;= arr[pos_greatest])        \t\tpos_greatest = j;        &#125;//end inner for loop        if ( i != pos_greatest ) &#123;            temp = arr[i];            arr[i] = arr[pos_greatest];            arr[pos_greatest] = temp;         &#125;    &#125;//end outer for loop&#125;//end selection sort\n Complexity of selection sort\nCompared to bubble sort:\n\nSame number of iterations\nSame number of comparisons in the worst case\nfewer swaps (one for each outer loop = n-1)\nHence, also O(n2)O(n^2)O(n2)\n\n Insertion Sort\n Basic Idea\nKeep the front of the list sorted, and as we move through the back, elements we insert them into the correct place in the front\n Pseudocode\n1234567891011void insertionSort(int arr[])&#123;for(int j = 1; j &lt; arr.length; j++)&#123;    int temp = arr[j];    int i = j; // range 0 to j-1 is sorted    while(i &gt; 0 &amp;&amp; arr[i-1] &gt; temp)&#123;        arr[i] = arr[i-1];        i--;    &#125;    arr[i] = temp;    &#125; // end outer for loop&#125; // end insertion sort\n Complexity of insertion sort\n\nIn the worst case, has to make n(n−1)/2n(n-1)/2n(n−1)/2 comparisons and shifts to the right\nalso O(n2)O(n^2)O(n2) worst case complexity\nBest case: array already sorted\n\nBackwards walk of inner loop stops immediately; no shifts.\nBecomes O(n)O(n)O(n)\n\n\n\n Adaptive Sort\nThis is a (bizarre) name for what asking what happens to the complexity when the lists are already “nearly sorted”.\nIn many applications lists might already be close to being sorted.\n\nE.g. maybe they were from a list that was sorted and then some corrections were made.\nIt is then natural to ask, for each algorithm, whether the efficiency improves.\n\n Sorting “Stability”\n\n\ncompare(o1,o2)==0 means objects o1 and o2 are\n\nequal with respect to the desired ordering\nbut not necessarily that they have the same contents\n\ni.e. are not identical !\n\n\n\n\n\nIf sorting a spreadsheet, then might sort by one column then another.\n\n\nDo not want the sorting to unnecessarily change the order of the rows, as this can be annoying and confusing.\n\n\n“Sort by column A, followed by a stable sort on column B” means that still will have a secondary sort on column A\n\n\n Sorting on Lists\n\n\nBubble sort is just as efficient (or rather inefficient) on linked lists.\n\nWe can easily bubble sort even for a singly linked list.\n\n\n\nSelection sort on linked lists implementation similar to bubble sort; also O(n2)O(n^2)O(n2)\n\n\nInsertion sort is a suitable sorting method (only) for doubly linked lists – as need to walk backwards\n\n\n\n Tree\n Tree Terminology\n\n\nRoot: node without parent (A)\n\n\nInternal node: node with at least one child (A, B, C, F)\n\n\nExternal node (a.k.a. leaf ): node without children (E, I, J, K, G, H, D)\n\n\nAncestors of a node: parent, grandparent, grand-grandparent, etc.\n\n\nDepth of a node: number of ancestors (not counting itself)\n\n\nHeight of a tree: maximum depth of any node = length of longest path from root to a leaf\n\nHeight of tree below = 3\n\n\n\nDescendant of a node: child, grandchild, grand-grandchild, etc.\n\n\nSubtree: tree consisting of a node and its descendants\n\n\n\n Traversals\nGiven a data structure, a common task is to traverse all elements\n\nvisit each element precisely once\nvisit in some systematic and meaningful order\nNote “visit” means “process the contents” but does not include just “passing through using the links”\n\n Preorder Traversal\n\nIn a preorder traversal, a node is visited before its descendants\nApplication: print a structured document\n\n1234Algorithm preOrder(v)    visit(v)    for each child w of v        preorder (w)\n Postorder Traversal\n\nIn a postorder traversal, a node is visited after its descendants\nApplication: compute space used by files in a directory and its subdirectories\n\n1234Algorithm postOrder(v)    for each child w of v    \tpostOrder (w)    visit(v)\n Inorder Traversal\n\n\nIn an inorder traversal a node is visited after its left subtree and before its right subtree\n\n\nApplication: draw a binary tree by (x,y) coords:\n\nx(v) = inorder rank of v\ny(v) = depth of v\n\n\n\n123456Algorithm inOrder(v)    if hasLeft (v)    \tinOrder (left (v))    visit(v)    if hasRight (v)    \tinOrder (right (v))\n Binary Trees\n Properties\n\nEach internal node has at most two children\nThe children of a node are an ordered pair - though one might be “missing”\n\n Consensus\n\nWe call the children of an internal node left child and right child\nAlternative recursive definition: a binary tree is either\n\na tree consisting of a single node, or\na tree whose root has an ordered pair of “children”, each of which is missing (a null) or is the root of a binary tree\n\n\nApplications: searching\n\n\n Proper Binary Trees\n Properties\n\nEach node has either two children or no children\nThe children of a node are an ordered pair\n\n Applications:\n\narithmetic expressions\ndecision processes\n\n\n Application details\n\n Print Arithmetic Expressions\nSpecialization of an inorder traversal\n\n\nprint operand or operator when visiting node\n\n\nprint “(” before traversing left subtree\n\n\nprint “)” after traversing right subtree\n\n\n1234567Algorithm printExpression(v)    if hasLeft (v) print(&quot;(&quot;)        printExpression (left(v))        print(v.element ())    if hasRight (v)        printExpression(right(v))        print (&quot;)&quot;)\n Evaluate Arithmetic Expressions\nSpecialization of a postorder traversal:\n\nrecursive method returning the value of a subtree\nwhen visiting an internal node, combine the values of the subtrees\n\n12345678Algorithm evalExpr(v)    if isExternal (v)        return v.element ()    else        x ← evalExpr(leftChild (v))        y ← evalExpr(rightChild (v))        ◊ ← operator stored at v    return x ◊ y\n Abstract Data Types (ADTs)\n\n\n\n\n\n\n\n\n\nAn abstract data type (ADT) is an abstraction of a data structure\nAn ADT specifies:\n\nData stored\nOperations on the data\nError conditions associated with operations\n\nAn ADT does not specify the implementation itself -hence “abstract”\n Example: ADT modeling a simple stock trading system\n\nThe data stored are buy/sell orders\nThe operations supported are\n\norder buy(stock, shares, price)\norder sell(stock, shares, price)\nvoid cancel(order)\n\n\nError conditions:\n\nBuy/sell a non existent stock\nCancel a non existent order\n\n\n\n Concrete Data Types (CDTs)\n\n\n\n\n\n\n\n\n\nThe actual date structure that we use\nAn ADT might be implemented using different choices for the CDT\n\nThe choice of CDT will not be apparent from the interface: “data hiding” “encapsulation”\n\ne.g. see ‘Object Oriented Methods’\n\n\nThe choice of CDT will affect the runtime and space usage\n\n ADT &amp; Efficiency\n\n\nOften the ADT comes with efficiency requirements expressed in big-Oh notation, e.g.\n\n\n“cancel(order) must be O(1)”\n\n\n“sell(order) must be O(log( |orders| ) )”\n\n\n\n\nHowever, such requirements do not automatically force a particular CDT.\n\nThe underlying implementation is still not specified\n\n\n\nThis is typical of many “library functions”\n\n\n Tree ADT\nWe can use “positions”, p, to abstract nodes\n Generic methods:\n\ninteger size()\nboolean isEmpty()\nIterator iterator()\nIterator positions()\n\n Accessor methods:\n\nposition root()\nposition parent(p)\nIterator children(p)\n\n Query methods:\n\nboolean isInternal(p)\nboolean isExternal(p)\nboolean isRoot(p)\n\n Update method:\n\n\nobject replace (p, o)\n\n\nAdditional update methods may be defined by data structures implementing the Tree ADT\n\n\n Array-Based Representation of Binary Trees\nlet rank(node) be defined as follows:\n\n\nrank(root) = 1\n\n\nif node is the left child of parent(node),\nrank(node) = 2*rank(parent(node))\n\n\nif node is the right child of parent(node),\nrank(node) = 2*rank(parent(node))+1\n\n\n Implementation\nRemember that if think of the rank, r(n) of node n, as a binary number then\n\n\nr(n) = r(par(n)) &lt;&lt; 1 + 0 for left\n\n\nr(n) = r(par(n)) &lt;&lt; 1 + 1 for right\n\n\nE.g. r(par(n)) = 101 gives children at\n\n“101”+”0” = 1010\n“101”+“1” = 1011\n\n\n\nGoing to the parent is a right shift\n\nr(par(n)) = r(n) &gt;&gt; 1\n\n Advantages of the tree-as-array structure:\n\nSaves space as do not have to store the pointers – they are replaced by fast computations\nThe storage can be more compact –“better memory locality” and this can be good because of cache and memory hierarchies, when an array element is accessed then other entries can be pulled into the cache, and so access becomes faster.\n\n Perfect binary trees\n Properties\n\n\nA binary tree is said to be “proper” (a.k.a. “full”) if every internal node has exactly 2 children\n\n\nIt is “perfect” if it is proper and all leaves are at the same depth; hence all levels are full.\n\n\n\nCounting suggests numbers of nodes are:\n\n2d2^d2d at level ddd\n2d+1−12^{d+1}-12d+1−1 at level ddd or less\n\nHeight (hhh) is logarithmic in size (nnn)\n\n\nheight h=log2(n+1)−1h = log_2 (n + 1) - 1h=log2​(n+1)−1 where nnn in the number of nodes.\n\n\nnumber of levels: h+1=log2(n+1)h + 1 = log_2 (n + 1)h+1=log2​(n+1)\n\n\nnodes n=2h+1−1n = 2^{h+1} - 1n=2h+1−1\n\n\nFor a general binary tree on nnn nodes, the height is Ω(log(n))\\Omega(log(n))Ω(log(n)) and O(n)O(n)O(n)\n Merge Sort\n Divide-and-Conquer\nDivide-and-conquer is a general algorithm design paradigm:\n\nDivide: divide the input data SSS in two disjoint subsets S1S_1S1​ and S2S_2S2​\nRecur: solve the subproblems associated with S1S_1S1​ and S2S_2S2​\nConquer: combine the solutions for S1S_1S1​ and S2S_2S2​ into a solution for SSS\n\n Merge-Sort\nMerge-sort on an input sequence (array/list) SSS with nnn elements consists of three steps:\n\nDivide: partition SSS into two sequences S1S_1S1​ and S2S_2S2​ of about n/2n/2n/2 elements each\nRecur: recursively sort S1S_1S1​ and S2S_2S2​\nConquer: merge S1S_1S1​ and S2S_2S2​  into a unique sorted sequence\n\n Implementation\n1234567891011public void recMergeSort (int[] arr, int[] workSpace, int l, int r) &#123;    if (l == r) &#123;    \treturn;    &#125; else &#123;        int m = (l+r) / 2;        recMergeSort(arr, workSpace, l, m);        recMergeSort(arr, workSpace, m+1, r);        merge(arr, workSpace, l, m+1, r);    &#125;&#125;// Initial call is with l=0 and r to be end of the array\n Merge-Sort Tree\nAn execution of merge-sort is depicted by a binary tree\n\neach node represents a recursive call of merge-sort and stores\n\nunsorted sequence before the execution and its partition\nsorted sequence at the end of the execution\n\n\nthe root is the initial call\nthe leaves are calls on subsequences of size 0 or 1\n\n\n Analysis of Merge-Sort\nThe height hhh of the merge-sort tree is O(logn)O(log n)O(logn)\n\nat each recursive call we divide in half the sequence,\n\nThe overall amount of work done at all the nodes at depth iii is O(n)O(n)O(n)\n\nwe partition and merge 2i2^i2i sequences of size n/2in/2^in/2i\nwe make 2i+12^{i+1}2i+1 recursive calls\nthe numbers all occur and are all “used” at each depth\nSo, each depth uses O(n)O(n)O(n) work\n\nThus, the total running time of merge-sort is O(nlogn)O(nlog n)O(nlogn)\n\n Summary\n\nFast sorting method for arrays\nGood for sorting data in external memory – because works with adjacent indices in the array (data access is sequential)\n\nIt accesses data in a sequential manner (suitable for sorting data on a disk)\n\n\nNot so good with lists: relies on constant time access to the middle of the sequence\n\n\n Recurrence Relations\n Example\n\n\nHow would we solve T(n)=2 T(n/2)+bT(n) = 2\\,T(n/2) + bT(n)=2T(n/2)+b with T(1)=1T(1)=1T(1)=1\n\n\nWe know T(1)=1T(1)=1T(1)=1, hence\n\nT(2)=2 T(1)+b=2+bT(2) = 2\\,T(1) + b = 2 + bT(2)=2T(1)+b=2+b\nT(4)=2 T(4/2)+b=2 (2+b)+b=4+(2+1)bT(4) = 2\\,T(4/2) + b = 2\\,( 2 + b) + b = 4 + (2+1)bT(4)=2T(4/2)+b=2(2+b)+b=4+(2+1)b\nT(8)=2 (4+(2+1)b)+b=8+(4+2+1)bT(8) = 2\\,(4 + (2+1)b) + b = 8 + (4+2+1) bT(8)=2(4+(2+1)b)+b=8+(4+2+1)b\n\n\n\nIt seems a good guess\nT(2k)=2k+(2(k−1)+…+1)bT(2k) = 2^k+ (2^{(k-1)}+…+1)bT(2k)=2k+(2(k−1)+…+1)b\n=2k+(2k−1)b= 2^k+ (2^k-1) b=2k+(2k−1)b\n\n\nSo T(n)=n+(n−1)b=(1+b)n−bT(n) = n+(n-1) b = (1+b)n-bT(n)=n+(n−1)b=(1+b)n−b for nnn in {1,2,4,8…}\\{1,2,4,8…\\}{1,2,4,8…}\n\n\nStill Θ(𝑛)Θ(𝑛)Θ(n)\n\n\nClaim: T(2k)=2k+(2k−1)bT(2^k) = 2^k+ (2^k-1) bT(2k)=2k+(2k−1)b\n\n\nProof by induction:\n\n\nBase case: k=0,T(1)=1+(1−1)∗b=1k=0, T(1) = 1 + (1-1)*b = 1k=0,T(1)=1+(1−1)∗b=1\n\n\nStep case: assume true at kkk\n\n\n$T(2^{k+1}) = 2 T(2^k) + b $\n=2(2k+(2k–1)b)+b= 2 (2^k+ (2^k–1) b ) + b=2(2k+(2k–1)b)+b\n=2k+1+(2k+1−2+1)b= 2^{k+1}+ (2^{k+1}-2 + 1) b=2k+1+(2k+1−2+1)b\n=2k+1+(2k+1−1)b= 2^{k+1}+ (2^{k+1}-1) b=2k+1+(2k+1−1)b\n\n\nQED.\n\n\n Master Theorem (MT)\nFor a given recurrence of the form T(n)=a⋅T(n/b)+f(n)T(n)=a \\cdot T(n/b) +f(n)T(n)=a⋅T(n/b)+f(n) the M.T. can tell us the growth rate of T(n)T(n)T(n) according to three cases:\n\n\nCase 1: Recurrence dominates (plus special case that f(n)=0f(n)=0f(n)=0)\nIF f(n)f(n)f(n) is O(nc)O(n^c)O(nc) with c&lt;logbac&lt;log_bac&lt;logb​a,\nTHEN T(n)T(n)T(n) is Θ(nlogba)\\Theta(n^{log_ba})Θ(nlogb​a)\n\n\nCase 2: Neither term dominates\nIF f(n)f(n)f(n) is Θ(nc(log n)k)\\Theta(n^c(log\\,n)^k)Θ(nc(logn)k) with c=logbac = log_bac=logb​a and k≥0k \\geq 0k≥0,\nTHEN T(n)T(n)T(n) is Θ(nc(log n)k+1)\\Theta(n^c(log\\, n)^{k+1})Θ(nc(logn)k+1)\n\n\nCase 3: f(n)f(n)f(n) dominates\nIF f(n)f(n)f(n) is Ω(nc)\\Omega(n^c)Ω(nc) with c&gt;logbac &gt; log_bac&gt;logb​a,\nTHEN T(n)T(n)T(n) is Θ(f(n))\\Theta(f(n))Θ(f(n))\n\n\n Quicksort\nQuick-sort is a (randomized) sorting algorithm based on the divide-and-conquer paradigm:\n\nDivide: pick an element xxx(called pivot) and partition SSS into\n\nLLL: elements less than xxx\n\nHave to be careful it is not empty\n\n\nGEGEGE: elements greater than or equal to xxx\nPivot is often picked as a random element\n\n\nRecur: sort LLL and GEGEGE\nConquer: join LLL, GEGEGE\n\n “In-place” or “extra workspace”?\n\nFor sorting algorithms (and algorithms in general) an important issue can be how much extra working space they need besides the space to store the input\n“In-place” means they only a “little” extra space (e.g. O(1)) is used to store data elements.\n\nThe input array is also used for output, and only need a few temporary variables\nbubble-sort is “in-place”\nPrevious “merge” used extra O(n) array\n\n(can be made in-place, but messy and so we ignore this option)\n\n\n\n\n\n Implementation\n12345678public void recQuickSort(int[] arr, intleft, intright) &#123;    if (right - left &lt;= 0) return;    else &#123;        intborder = partition(arr, left, right); // pivot position        recQuickSort(arr, left, border-1);        recQuickSort(arr, border+1, right);    &#125;&#125;\n Quick-Sort Tree (3-way split)\n\n Worst-case Running Time\nThe worst case for quick-sort occurs when the pivot is the unique minimum or maximum element\n\nOne of LLL and E+GE+GE+G has size n−1n -1n−1 and the other has size 111\nThe running time is proportional to the sum\nn+(n−1)+…+2+1n+(n-1) + … +2 + 1n+(n−1)+…+2+1\nThus, the worst-case running time of quick-sort is O(n2)O(n^2)O(n2)\n\n Best-case  and average-case Running Time\nThe best case for quick-sort occurs when the pivot is the median element\n\nThe LLL and GGG parts are equal –the sequence is split in halves, like in merge sort\nThus, the best-case running time of quick-sort is O(nlogn)O(n log n)O(nlogn)\n\nThe average case for quick-sort: half of the times, the pivot is roughly in the middle\n\nThus, the average-case running time of quick-sort is O(nlogn)O(n log n)O(nlogn) again\n\n Motivations for quicksort\nit can be done “in-place”\n\nUses a small amount of workspace\nBecause the “merge” step is now a lot easier!!\n\nThe “split” is more complicated, and the merge “much” easier – but turns out that the quick-sort split is easier to do in-place than the merge-sort merge\n Comparison-based sorting\nAll the algorithms we have seen so far are comparison based\n\nIf you inspect the code then the decision as to swaps\n\netc. is based only of statements like “if ( A[i] &lt;= A[j] )”\n\n\n\nNot all sorts are comparison-based:\n\nbucket sort: used the actual values to rearrange the items\nRuns in O(n), but relies on knowing the range of values in the sequence\n\n(e.g.“integers between 1 and 1000”).\n\n\n\n\n\n\n\n\n\n\n\n\nComparison-based sorting cannot do better than O(nlogn)O(n log n)O(nlogn)\n\n Vector\n Vector ADT\nAn element can be accessed, inserted or removed by specifying its rank\nMain vector operations:\n\nobject elemAtRank(integer r):\n\nreturns the element at rank r without removing it\n\n\nobject replaceAtRank(integer r, object o):\n\nreplace the element at rank with o and return the old element\n\n\ninsertAtRank(integer r, object o):\n\ninsert a new element o to have rank r\n\n\nremoveAtRank(integer r):\nremoves and returns the element at rank r\n\nAdditional operations size() and isEmpty()\n Applications of Vectors\n\nThere is not an automatic limit on the storage size\n\nunlike arrays of a fixed size\n\n\nDirect applications\n\nSorted collection of objects (elementary database)\n\n\nIndirect applications\n\nAuxiliary data structure for many algorithms\nComponents of other data structures\n\n\n\n Array-based Vector\n Performance\nIn the array-based implementation of a Vector\n\nThe space used by the data structure is O(n)O(n)O(n)\nsize, isEmpty, elemAtRankand, replaceAtRankrun in O(1)O(1)O(1) time\ninsertAtRankand removeAtRankrunin O(n)O(n)O(n) time\npush runs in O(1)O(1)O(1) time, as do not need to move elements\n\nunless need to resize the array\n\n\npop runs in O(1)O(1)O(1) time\n\n Comparison of the Strategies\nIn an insertAtRankoperation, when the array is full, instead of throwing an exception, we can replace the array with a larger one\n\nincremental strategy: increase size by a constant ccc\ndoubling strategy: double the size\n\nWe call amortized time of a push operation the average time taken by a push over the series of operations, i.e., T(n)/nT(n)/nT(n)/n\n Why is amortised analysis different from the average case analysis?\n\n“Amortised”: (long) real sequence of dependent operations\n“Average”: Set of (possibly independent) operations\n\nWe have different measures of the runtime cost:\n\nWorst-case “cost per operation of a sequence”\nnot just\n“Worst case of a single operation”\n\n Incremental Strategy Analysis\nAverage, per push operation, is O(n)O(n)O(n)\n\nWe replace the array $k = n/c $ times\nEach “replace” costs the current size\nThe total time T(n)T(n)T(n) of a series of nnn push operations is proportional to\nn+c+2c+3c+4c+…+kc=n+ c + 2c + 3c + 4c +… + kc =n+c+2c+3c+4c+…+kc=\nn+c(1+2+3+…+k)=n+ c(1 + 2 + 3 + … + k) =n+c(1+2+3+…+k)=\nn+ck(k+1)/2n+ ck(k + 1)/2n+ck(k+1)/2\nSince ccc is a constant, T(n)T(n)T(n) is O(n+k2)O(n+k^2)O(n+k2),i.e., O(n2)O(n^2)O(n2)\nThe amortized time of a push operation is O(n)O(n)O(n)\nThis is bad as the normal cost of a push is O(1)O(1)O(1).\n\n Doubling Strategy Analysis\nGives an average of O(1)O(1)O(1) per operation\n\nWe replace the array k=log2nk = log_2nk=log2​n times\nThe total time T(n)T(n)T(n) of a series of nnn push operations is proportional to\nn+1+2+4+8+…+2k−1=n+ 1 + 2 + 4 + 8 + …+ 2^{k-1}=n+1+2+4+8+…+2k−1=\nn+2k−1=2n−1n+2^k-1 = 2n -1n+2k−1=2n−1\nT(n)T(n)T(n) is O(n)O(n)O(n)\nAmortized time of a single push operation is O(1)O(1)O(1)\nThat is, no worse than if all the needed memory was pre-assigned!\n\n(Big-Oh hides the constant factor ‘2’ extra cost.)\n\n\n\n\n Priority Queues &amp; Heaps\n Priority Queue ADT\n\nA priority queue stores a collection of entries\nEach entry is a pair(key, value)\nMain methods of the Priority Queue ADT\n\ninsert(k, v) inserts an entry with key k and value v\nremoveMin() removes and returns the entry with smallest key\nNote: unlike Map (seen later), there is no requirement of a method find(k).\n\n\nAdditional methods\n\nmin() returns, but does not remove, an entry with smallest key\nsize(), isEmpty()\n\n\nApplications:\n\nStandby passengers\nAuctions\nStock market\nPrinter queues\n\n\nTotal Order Relations\n\nJust need a “compare” operation between the keys and that behaves like &lt;= or &lt; as needed\nTwo distinct entries in a priority queue can have the same key\n\n\n\n Heaps\nA heap is a binary tree storing key-value pairs at its nodes and satisfying the following properties:\n\n\nHeap-Order: for every internal node vvv other than the root,\nkey(v)≥key(parent(v))key(v)\\geq key(parent(v))key(v)≥key(parent(v))\n\n\nComplete Binary Tree: let hhh be the height of the heap\n\nfor i=0,…,h−1i= 0, … , h -1i=0,…,h−1, there are 2i2^i2i nodes of depth iii\nThe last node of a heap is the rightmost node of depth hhh\n\n\n\n Insertion into a Heap\n\nFind the insertion node zzz (the new last node)\nStore kkk at zzz\nRestore the heap-order property\n\nAfter the insertion of a new key kkk, the heap-order property may be violated\nAlgorithm upheap restores the heap-order property by swapping kkk along an upward path from the insertion node\nUpheap terminates when the key kkk reaches the root or a node whose parent has a key smaller than or equal to kkk\nSince a heap has height O(log n)O(log\\, n)O(logn), upheap runs in O(log n)O(log\\, n)O(logn) time\n\n\n\n Removal from a Heap\n\nReplace the root key with the key of the last node www\nRemove www\nRestore the heap-order property\n\nAlgorithm downheap restores the heap-order property by swapping key kkk along a particular downward path from the root\nDownheap terminates when key kkk reaches a leaf or a node whose children have keys greater than or equal to kkk\nSince a heap has height O(log n)O(log \\,n)O(logn), downheap runs in O(log n)O(log\\, n)O(logn) time\n\n\n\n Heap-Sort\nConsider a priority queue with nnn items implemented by means of a heap\n\nthe space used is O(n)O(n)O(n)\nmethods insert and removeMin take O(log n)O(log \\, n)O(logn) time\nmethods size, isEmpty, and min take time O(1)O(1)O(1) time\n\nUsing a heap-based priority queue, we can sort a sequence of nnn elements in O(n log n)O(n\\,log\\, n)O(nlogn) time\n\nInsert all elements into the heap one by one.\n\nThis takes nnn insertions with O(log n)O(log\\,n)O(logn) each for a total of O(n log n)O(n \\,log \\,n)O(nlogn)\n\n\nRemove all elements one by one, using removeMin()removeMin()removeMin(), hence obtaining them in sorted order.\n\nThis takes nnn removals with O(log n)O(log\\, n)O(logn) each for a total of O(n log n)O(n \\,log \\,n)O(nlogn).\n\n\n\n Map\n The Map ADT over pairs &lt;K,V&gt;\n\n\n\n\n\n\n\n\n\nA map models a collection of key-value entries that is searchable `by the key’\nThe main operations of a map are for searching, inserting, and deleting items.\nMultiple entries with the same key are not allowed.\nMap ADT methods:\n\n\nV get(K k):\nif the map M has an entry with key k, return its associated value;\nelse, return null\n\n\nV put(K k, V v): insert entry (k, v) into the map M;\nif key k is not already in M, then return null;\nelse, return old value associated with k\n\n\nV remove(K k):\nif the map M has an entry with key k, remove it from M and return its associated value;\nelse, return null\n\n\nint size(), boolean isEmpty()\n\n\n&#123;K&#125; keys(): return an iterablecollection of the keys in M\n\n\n&#123;V&#125; values(): return an iterablecollection of the values in M\n\n\n&#123;&lt;K,V&gt;&#125; entries(): return an iterablecollection of the entries in M\n\n\n A Simple List-Based Map\nIt is straightforward to implement a map using a list – either singly or doubly-linked\n\n\nThe get(k) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nThe put(k,v) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nThe remove(k) Algorithm\n\nWorst case: O(n)O(n)O(n)\n\n\n\nOverall: the operations are O(n)O(n)O(n) because of needing to traverse the list\n\n\nList version of a Map is “simple but inefficient”\n\n\nEasier to be correct\n\n\nEven though slow, they can be useful\n\nallow the rest of the project to proceed without waiting for the efficient version\ncan be used for a regression test of a “faster but trickier” later version\n\ne.g. use in a “shadow mode” where the “slow-obviously-correct” version is used together with the “fast-possibly-buggy” version and the results checked against each other.\n\n\n\n\n\n Hash Tables\n\n\n\n\n\n\n\n\n\nHash tables are a concrete data structure which is suitable for implementing maps.\nBasic idea: convert each key into an index into a (big) array.\nLook-up of keys and insertion and deletion in a hash table usually runs in O(1)O(1)O(1) time.\n\nNot guaranteed, and design of the table needs to be done carefully if want the access to be “reliably O(1)O(1)O(1)”\n\n Basic Hash Functions\n\nA hash function hhh maps keys of a given type to integers in a fixed interval [0,N−1][0, N - 1][0,N−1]\n\nExample: h(k)=k  mod  Nh(k) = k \\;mod\\; Nh(k)=kmodN is a hash function for integer keys\n\n\nThe integer h(k)h(k)h(k) is called the hash value of key kkk\n\n Collision Handling\nCollisions occur when different elements are mapped to the same cell\n Hash Functions\n\n\nA hash function is usually specified as the composition of two functions:\n\n\nHash code:\nh1h_1h1​: keys → integers\n\n\nCompression function:\nh2h_2h2​: integers → [0,N−1][0, N - 1][0,N−1]\n\n\nDivision:\n\n\nh2(y)=y  mod  Nh_2 (y) = y\\; mod \\;Nh2​(y)=ymodN\n\n\nThe size NNN of the hash table is usually chosen to be a prime\n(hash codes will tend to spread better)\n\n\n\n\nMultiply, Add and Divide (MAD):\n\nh2(y)=(ay+b)  mod  Nh_2 (y) = (ay + b)\\; mod\\; Nh2​(y)=(ay+b)modN\naaa and bbb are nonnegative integers\nsuch that a  mod  N≠0a\\; mod\\; N \\not= 0amodN=0\nOtherwise, every integer would map to the same value bbb\n\n\n\n\n\n\n\nThe hash code is applied first, and the compression function is applied next on the result,\ni.e., h(x)=h2(h1(x))h(x) = h_2(h_1(x))h(x)=h2​(h1​(x))\n\n\nThe goal of the hash function is to “disperse” the keys in an “apparently random” way\n\nWhy disperse?\n\nto reduce numbers of collisions\n\n\nWhy random?\n\nrandom means ‘no pattern’\nif there is an obvious pattern then the incoming data might have a matching pattern that leads to many collisions\n“sometimes ‘no pattern’ is the only safe pattern” (e.g. rock-paper-scissors game)\n\n\n\n\n\n Separate Chaining\nlet each cell in the table point to (e.g.) a linked list of entries that map there\n\nNote: In practice, should use a more efficient Map;\ne.g. a Binary Search Tree (BST)\n\n\n\n\n1234V get(k)&#123;    return A[h(k)].get(k); \t// Simply delegates the “get” to the list-based map at A[h(k)]&#125;\n\n\n123456789101112V put(k,v)&#123;   /*    * If there is an existing entry in our map with key equal to k,     * then we return its value (replacing it with v);     * otherwise, we return null    */    t = A[h(k)].put(k,v); // t is new value or null    // Simply delegates the put to the list-based map at A[h(k)]    if (t = null) n = n + 1; // if k is a new key    return t;&#125;\n\n\n123456V remove(K k)&#123;\tt = A[h(k)].remove(k);    // Simply delegates the remove to the list-based map at A[h(k)]    if (t != null) n = n - 1 // k was found    return t&#125;\n\n\nSeparate chaining is simple and fast, but requires additional memory outside the table.\n\n\nWhen memory is critical then we try harder to remain within the existing memory\n\n\n Open addressing\n\n\n\n\n\n\n\n\n\nthe colliding item is placed in a different cell of the table\n\n\nLinear probing handles collisions by placing the colliding item in the next (circularly) available table cell\n\n“Circular array” – once get to the right-hand end, then start again at the beginning of the array\n\n\n\nDisadvantage: Colliding items lump together, causing future collisions to cause a longer sequence of probes\n\n\nV get(k)\n\nWe start at cell h(k)\nWe probe consecutive locations until one of the following occurs\n\nAn item with key k is found, or\nAn empty cell is found, or\nN cells have been unsuccessfully probed\n\n\n\n\n\nsafely remove an element x\n\n\nFind x using `get’ and set the entry back to blank\n\n\n“Lazy deletion”: don’t mark the entry as a blank, but as a ‘deleted’ and fix the entries later\n\n\nMove such entries by removing them and then re-inserting them all\n\n\n\n\n Double Hashing\nDouble hashing uses a secondary hash function d(k)d(k)d(k) and handles collisions by placing an item in the first available cell of the series\n\n\n(h(k)+j⋅d(k))  mod  N(h(k) + j\\cdot d(k))\\; mod \\;N(h(k)+j⋅d(k))modN for j=0,1,…,N−1j = 0, 1, … , N - 1j=0,1,…,N−1\n\n\nThe secondary hash function d(k)d(k)d(k) cannot have zero values\n\nCommon choice for the secondary hash function:\nd(k)=q−(k  mod  q)d(k) = q - (k \\;mod\\; q)d(k)=q−(kmodq) where\n\nq&lt;Nq &lt; Nq&lt;N\nqqq is a prime\n\n\n\n\n\nLinear probing is just d(k)=1d(k)=1d(k)=1\n\n\nThe table size NNN must be a prime to allow probing of all the cell\n\nWith a prime NNN, then eventually all table positions will be probed\n\n\n\nExample\n\n\n\n Performance of Hashing\n\n\nIn the worst case, searches, insertions and removals on a hash table take O(n)O(n)O(n) time\n\n\nThe worst case occurs when all the keys inserted into the map collide\n\n\nThe load factor a=n/Na = n/Na=n/N affects the performance of a hash table\n\n\nIn Java, maximal load factor is 0.75 (75%) – after that, rehashed\n\n\n Summary\nThe expected running time of all the map ADT operations in a hash table is O(1)O(1)O(1)\n\nIn practice, hashing is very fast provided the load factor is not close to 100%\n\n Re-Hashing\nWhen the table gets too full then “re-hash”: Create a new larger table and new hash function.\n\n\nNeed to (eventually) transfer all the entries from the old table to the new one\n\n\nIf do so immediately, then\n\none can amortise the cost over many entries (as for Vector) and so get an average cost of O(1)O(1)O(1) again\nbut the worst case might be O(n)O(n)O(n) when the table is rehashed, and this might be bad for a real time system\nOption:\ndo not transfer all entries “in one go” but do “a few at a time”\nKeep both tables until the transfer is complete; but only do insertions into the new table\n\n\n\n Applications of Hashing\n\nDirect applications of hash tables:\n\nsmall databases\ncompilers\nbrowser caches\n\n\nHash tables as an auxiliary data structure in a program\n\n\n Binary Search Trees\n\n\n\n\n\n\n\n\n\nA binary search tree is a binary tree storing key-value entries at its internal nodes and satisfying the following “search tree” property:\nLet u,vu, vu,v, and www be any three nodes such that\nuuu is in the left subtree of vvv and\nwww is in the right subtree of $ v$.\n\nThen we must have\nkey(u)≤key(v)≤key(w)key(u) \\leq key(v) \\leq key(w)key(u)≤key(v)≤key(w)\nor, as we will assume there are no duplicate keys:\nkey(u)&lt;key(v)&lt;key(w)key(u) &lt; key(v) &lt; key(w)key(u)&lt;key(v)&lt;key(w)\n\n Search Algorithm and Basic operation\n Pseudocode\n123456789Node TreeSearch(Key k, Node n)    if n.isExternal () // or, “if n == null”    \treturn null    if k &lt; n.key()    \treturn TreeSearch(k, n.left())    else if k = n.key()    \treturn n    else // k &gt; n.key()    \treturn TreeSearch(k, n.right())\n Fundamental Property of Search Tree\n\n\nAn in-order traversal of a (binary) search trees visits the keys in increasing order\n\n\nNote that to access the minimum key, we just need to ‘always go left’\n\n\n Insertion\n\nWe search for key k (using Tree Search)\nIf k is already in the tree then just replace the value\nOtherwise, k is not already in the tree, and let w be the leaf reached by the search\n\nWe “insert k at node w and expand w into an internal node”\n\n\nAgain, only follows a path from the root and so is O(h)O(h)O(h)\n\n Deletion\nFour cases\n\n\nk is not present, nothing to do\n\n\nn has no children, straightforward remove\n\n\nn has one child\n\n\nn has two children\n\n\n Deletion – with one child\nExample: remove 4\n\nTo perform operation remove(4), we search for key 4.\nLet nnn be the node storing 4.\nNode nnn has a null left child, and a real child 5\nWe remove nnn from the tree and connect 5 back to the parent of nnn\n\n      \n Deletion – with two children\nExample: remove 3\nThe key node nnn has two internal children\n\n\nwe find the internal node www that follows nnn in an in-order traversal\n\n\nwe copy key(w)key(w)key(w) into node nnn\n\n\nwe remove node www by means of same procedure as before for “one child”\n\n\n      \n Balanced Trees\nBinary search trees: if all levels filled, then search, insertion and deletion are O(log n)O(log \\, n)O(logn).\n\nAs they are all O(height)O( height )O(height)\n\n Performance\nThe height hhh is O(n)O(n)O(n) in the worst case and O(log n)O(log \\, n)O(logn) in the best case\nCould make trees balanced using a “total rebuild”\n\nBut would require O(n)O(n)O(n), and so very inefficient compared to the desired O(log n)O( log\\, n)O(logn)\nRe-balancing needs to be O(log n)O(log \\, n)O(logn) or O(height)O( height )O(height)\n\n\n Dynamic Programming\nThere are various general methods (“paradigms”) for finding solutions to problems:\n\nBrute force – “generate and test”\nDivide-and-conquer\nHeuristics\nDynamic Programming\n\n Brute Force\n\n\n\n\n\n\n\n\n\nThis is roughly “generate and test”\n\nGenerate all potential solutions\nTest for which ones are actual solutions\n\n\n\nExample: we could do “sorting” by\n\nGenerate all possible permutations\nTest to see which one is correctly ordered\n\nExtremely inefficient, as is O(n!)O(n!)O(n!)\n\n\n\n\n\nCan be useful in some (small) cases\n\nE.g. Due to the simplicity\n\n\n\n Divide and Conquer\n\n\n\n\n\n\n\n\n\nRecursively, break the problem into smaller pieces, solve them, and put them back together\nMerge-sort and Quicksort were classic examples\n Heuristics\n\n\n\n\n\n\n\n\n\n“Heuristic” = “rule of thumb”\n\nGenerally, meant to mean something that gives better decisions, than the naive methods, but still not necessarily optimal\n\n Heuristics in exact methods\nThese are general methods that works in an algorithm that does give exact or optimal answers\n\nBut need the heuristics to decrease the (average/typical) runtime\n\nExamples:\n\n“Admissible heuristic” in A* search – decreases the search time compared to plain search\n“pick a random pivot” in quicksort\n\n Heuristics in inexact methods\nThese are general methods that (generally) are not be guaranteed to give the best possible answers, but that can give good answers quickly\nUsed on problems when the exact methods are too slow\n\nTimetabling and scheduling and many design problems\n\n Greedy algorithms\nA common “heuristic” is to be “greedy”\n\nTake the decision that looks best in the short term – without looking ahead\n\nSometimes greedy algorithms can still give optimal answers\n\nE.g. Prim’s algorithm for constructing a Minimal Spanning Tree is a greedy algorithm\n\nUsually greedy algorithms cannot guarantee to give optimal answers\n\n\nbut often still give (nearly) optimal answers in practice\n\n\nExample: “Change-giving”:\n\nProblem: given a collection of coins (a multi-set, that allows repeated elements), and a desired target for the change. Supply the change in as few coins as possible\nPick the largest coin which is still available and does not cause to exceed the target\n\n\n\n Dynamic Programming (DP)\nDP is a general method that can be suitable when the optimal solutions satisfy a “decomposition property”\nThe general idea is roughly: solve small sub-problems first, then build up towards the full solution.\n Subset-Sum\nGiven (multi-)set S of positive integers x[i] and a target K. Is there a subset of S that sums to exactly K?\n\n\nBoolean Array, Y, for [0,…,K]\n\nY[m] = true iff some subset has been found that sums to m\n\n\n\nMain idea: if some subset summed to m, then with the inclusion x[i], we can also find a subset that sums to m+x[i]\n\n\n Pseudocode:\n123456789101112Input: x[0],…,x[n-1] and KInitialise all Y[m] = false for m=1,…, KY[0] = true; // As can always provide no changefor (int i=0 ; i&lt;n ; i++) &#123; // consider effect of x[i]    for (int m=K-x[i] ; m&gt;=0 ; m--) &#123; // “scan down”        if (Y[m]==true) &#123;                     // m was achievable with x[0]… x[i-1]            if (m+x[i] == K ) return success; // hence now also m+x[i] is achievable            if (m+x[i] &lt; K ) Y[ m+x[i] ] = true;        &#125;    &#125;&#125;\n Complexity\n\nOuter loop has to consider all the coins, hence O(n)O(n)O(n)\nInner loop scans the entire array Y, hence O(K)O(K)O(K)\nOverall is O(nK)O( n K )O(nK)\n\nHowever, “K” has the “hidden exponential” if it is represented in binary:\nThe relevant input size is the number of bits BBB that are needed to represent, B=O(log(K))B=O(log(K))B=O(log(K))\n\n\nThe complexity in terms of the size of the binary input is O(n 2B)O(n\\,2^B )O(n2B), which is called “pseudo-polynomial”\n\n Min-Coins version\nPrevious just asked if it is possible to do the change. But want to minimise the coins.\n\n\nAlgorithm: Inspect the coins one at a time keeping track of the best answers obtained so far\n\n\nMain data structure:\n\nInteger Array, Y, for [0,…,K]\n\nY[m] = -1 if have not found any sum for m as yet\nY[m] = c &gt;= 0 means that have found that can achieve the sum m with c coins.\n\n\n\n\n\nAim: when the algorithm finishes then Y[K] will be the minimum number of coins\n\n“Side-effect”: All the values of Y[m] m &lt; K, will also be the minimum number for a value of m.\n\n\n\nMain idea: if some set summed to m, then with the inclusion x[i] we can also find a subset that sums to m+x[i]\n\n\n1234567891011121314151617Input: x[0],…,x[n-1] and KInitialise: Y[0] = 0,and Y[m] = -1 for m &gt; 0 // 0 coins can give a change of 0for (i=0 ; i&lt;n ; i++) &#123; // consider effect of x[i]    for (m=K-x[i] ; m&gt;=0 ; m--) &#123; // scan array        if (Y[m] &gt;= 0 ) &#123;            // value m was achievable with x[0]…x[i-1] using Y[m] coins,            // so, m+x[i] is now achievable with Y[m]+1 coins            // but might already have found a better answer            // stored as Y[m + x[i] ] so then take the best            if (Y[m + x[i] ] == -1 )            \tY[ m + x[i] ] = Y[m] + 1;            else                Y[ m + x[i] ] = min( Y[m + x[i] ] , Y[m] + 1 );        &#125;    &#125;&#125;\n Worked example\nInput: x[] = {5,2,2,2,1} and K=6\n\nk=0, Y[] = [0,-1,-1,-1,-1,-1,-1], Y[0]=0 for change {}\nk=1, Y[] = [0,-1,-1,-1,-1,1,-1], Y[5]=1 for change {5}\nk=2, Y[] = [0,-1,1,-1,-1,1,-1], Y[2]=1 for change {2}\nk=3, Y[] = [0,-1,1,-1,2,1,-1], Y[4]=2 for change {2,2}\nk=4, Y[] = [0,-1,1,-1,2,1,3], Y[6]=3 for change {2,2,2}\nk=5, Y[] = [0,-1,1,-1,2,1,2], Y[6]=min(3, 1+1)=2 for change {5,1}\nFinished: so optimal answer is 2 coins.\n\n Minimum Spanning Trees\n Spanning Tree\n\nInput: connected, undirected graph\nOutput: a tree which connects all vertices in the graph using only the edges present in the graph\n\n Minimum Spanning Tree\n\n\nInput: connected, undirected, weighted graph\n\n\nOutput: a spanning tree\n\n(connects all vertices in the graph using only the edges present in the graph)\nand is minimum in the sense that the sum of weights of the edges is the smallest possible for any spanning tree\n\n\n\nUsages: Gas and water pipelines, optic fibers networks…\n\n\n Why MST is a tree\n\nWe really want a minimum spanning sub-graph\n\na subset of the edges that is connected and that contains every node\n\n\nIf a graph is connected and acyclic then it is a tree\n\n Prim’s algorithm\nTo construct an MST:\n\n\nStart by picking any vertex M\n\n\nChoose the shortest edge from M to any other vertex N\n\n\nAdd edge (M, N) to the MST\n\n\nLoop:\n\n\nContinue to add at every step a shortest edge from a vertex in MST to a vertex outside,\nuntil all vertices are in MST\n\n\n(If there are multiple shortest edges, then can take any arbitrary one)\n\n\n\n\n Why is this optimal!?\nArgument by contradiction\n\n\nlet V1 and V2 be a partition of the vertices of G into two disjoint non-empty sets\n\n\nSuppose that some minimum spanning tree T that containing e is better than all other trees\n\n\nThen can add edge e to T and remove some other edge between V1 and V2 and obtain a better MST\n\n\nThe algorithm adds a minimum weight edge between V1 and V2,\nand so this edge must be part of some MST\n\n\nHence, the construction cannot make a “fatal mistake” - at no point can it add an edge not part of an MST\n\n\n Binary Search Trees: Balance and Rotations\n      \n\n\nBoth trees are valid BSTs with the same content\n\n\nBoth have in-order traversal: 1,2,3,4,5. But …\n\nThe left has height 4\nThe right has height 2\n\n\n\nThis matters because the vital BST algorithms are O(height)O( height )O(height)\n\n\nA tree is said to be “balanced” if the heights of left and right subtrees of any node are (close to) equal, and so the height is O(log n)O(log \\,n)O(logn)\n\n\n The BST Imbalance problem\n\nAIM: Do “small local rebuilds” during insert/delete operations, to maintain the balance, and so retain the O(log n)O( log \\,n )O(logn) cost\n\n Example of a “rotation”\n\n\n\nBoth trees have in-order traversal: In(T1), a, In(T2), b, In(T3)\n\n\nDepths:\n\n‘a’ and T1 sink down,\n‘b’ and T3 rise up\n\n\n\na single rotation only has O(1)O(1)O(1) in cost\n\n\n Simple Example\n\n\n“Raising b” was the good choice because it is the median value and so should be the root\n\n Specific Example: Double works\n\n\n\nRotation on the edge b-c\n\n\nThen can rotate on edge a-b\n\n\nThe median node is ‘b’, needs to drop from 2 to 0\n\nEach rotation can only move ‘b’ up by 1, hence need two rotations\n\n\n\n Summary\nThe goal is then to control the usage of rotations to reduce the overall height of the tree\nAdvanced:\n\nAVL trees\n2-4 trees &amp; red-black trees\n\nThey guarantee O(log n)O( log \\,n )O(logn) – unlike Hash maps\n Shortest Paths: Floyd-Warshall (FW)\n Floyd-Warshall: All-Pairs Shortest Paths\nSuppose that wanted to find the shortest path between all pairs of start and end nodes\n Basic method\n\nBuild the optimal answers using a subset of the nodes.\nThen add the effects of other nodes one at a time\n\n Data structure\n\n\nd(i,j,k)d(i,j,k)d(i,j,k) = shortest distance between nodes iii and jjj,\nbut using only the nodes 1,…,k{1,…,k}1,…,k as potential allowed intermediary points\n\n\nd(2,5,3)d(2,5,3)d(2,5,3) = shortest distance from n2n_2n2​ to n5n_5n5​ using only {n1,n2,n3}\\{n_1,n_2,n_3\\}{n1​,n2​,n3​} as potential intermediate points\n\n\nwe will assume that d(i,i,k)=0d( i, i, k ) = 0d(i,i,k)=0 for all iii\n\n\n Initialisation\n\n\nd(i,j,0)d(i,j,0)d(i,j,0) = best distance between nodes iii and jjj, but not using any intermediate nodes,\n\n\nd(i,j,0)=w(i,j)   if there is an edge i to jd(i,j,0) = w(i,j) \\;\\text{ if there is an edge i to j}d(i,j,0)=w(i,j) if there is an edge i to j\n=∞               otherwise= \\infin \\;\\;\\;\\;\\;\\;\\;\\text{ otherwise}=∞ otherwise\n\n\n FW equations\n\n\nNow suppose that we add the node ‘n1n_1n1​’ to the set of nodes that can be intermediates,\ni.e. consider k = 1\n\n\nBest path is now the best of “either direct, or via n1n_1n1​.”\n\n\nd(i,j,1)=min( d(i,j,0),  d(i,n1,0)+d(n1,j,0))d(i,j,1) = min (\\, d(i,j,0), \\;d(i,n_1,0) + d(n_1,j,0) )d(i,j,1)=min(d(i,j,0),d(i,n1​,0)+d(n1​,j,0))\n\n\n\n\n\nNow suppose that we add the new node “(k+1)” to the set of “via nodes” that can be intermediates, but have already considered k of them\n\n\nBest path is now either direct using only the k ‘via nodes’ already accounted for,\nor else also via node ‘k+1’ (and using the previous k via’s)\n\n\nd(i,j,k+1)=min( d(i,j,k),  d(i,k+1,k)+d(k+1,j,k))d(i,j,k+1) = min ( \\,d(i,j,k), \\;d(i,k+1,k) + d(k+1,j,k) )d(i,j,k+1)=min(d(i,j,k),d(i,k+1,k)+d(k+1,j,k))\n\n\n\n\n FW code &amp; complexity\nThe main loop after initialisation is:\n1234foreach k = 1,…,|V| // size of V\tforeach i ∈ V\t\tforeach j ∈ V\t\t\td(i,j,k+1) = min( d(i,j,k), d(i,k+1,k) + d(k+1,j,k) );\n\n\nHave 3 nested loops, of ranges |V|. Hence is O(∣V∣3)O( |V|^3 )O(∣V∣3)\n\n\nIf the graph is sparse, then ∣E∣&lt;&lt;∣V∣2|E| &lt;&lt; |V|^2∣E∣&lt;&lt;∣V∣2, so then “all-starts Dijkstra” may be better.\n\n“&lt;&lt;” means “much less than”\n\n\n\n FW on digraphs\nFW also works the same on directed graphs\n\nThe initial matrix d(i,j,0)d(i,j,0)d(i,j,0) need not be symmetric, but then the remaining calculations use exactly the same formulas\n\n FW with negative edges\nFW even works if some (directed) edge weights are negative\n\nBUT it is essential that there are no cycles of total negative weight\nOtherwise simply repeatedly following around the negative cycle may reduce lengths to be as negative as desired, so there is no shortest path\n\n Key Idea\nUses that shortest path does satisfy a nice decomposition of\n\n\n\n\n\n\n\n\n\nIf P(A,B)P(A,B)P(A,B) is a shortest path, and goes via MMM, then P(A,M)P(A,M)P(A,M) is optimal for AAA to MMM and P(M,B)P(M, B)P(M,B) is optimal for MMM to BBB\n\nHence uses a version of “dynamic programming”\n\n","slug":"ade","date":"2024-06-09T15:34:56.000Z","categories_index":"Notes","tags_index":"Algorithms,Data structure,Efficiency","author_index":"Huaji1hao"},{"id":"c69b9ac1223f91f4483d13b90656a509","title":"IELTS Writing Task 2","content":" Writing Task 2\n Understanding and Analysis\n The 5 types of questions:\n\nOpinion (Agree or Disagree)\nDiscussion (Discuss both view)\nAdvantages and Disadvantages\nProblem/Causes and Solution\nDouble Question\n\n Question Analysis\nWhen analyzing a question we have to think about 3 things:\n\nTopic\nKeywords\nInstruction words\n\n\n\n\n\n\n\n\n\n\nSome experts believe that it is better for children to begin a foreign language at primary school  rather than secondary school.\nDo the advantages of this outweigh the disadvantages?\n Think like an examiner!\n Task Response Dos\n\n\nAnswer the specific question being asked, Not the general topic\n\n\nMake sure your ideas are relevant\n\n\nFully address each part of the question\n\n\nState your opinion in introduction and use the supporting paragraphs to support this opinion\n\n\nReiterate your opinion in the conclusion\n\n\n Task Response Don’ts\n\nSpend lots of time on just one part of the question\nGive very general examples\nLeave opinion until the last sentence\nRepeat the same points over and over\nWrite under 250 words\n\n Coherence and Cohesion Dos\n\nUse four paragraph structure\nOutline your main ideas and opinion in the introduction\nHave clear topic sentences in your supporting paragraphs\nSkip a line between paragraphs\nUse cohesive devices appropriately and accurately\n\n Coherence and Cohesion Don’ts\n\nInclude background statement in the introduction\nHave lots of ideas in one paragraph\nUse cohesive devices at the start of every sentence\nUse cohesive devices inaccurately\n\n Vocabulary Dos\n\nBe careful with spelling ad grammar\nBe aware of collocations (match the words)\nUse ‘less-common’ words (specific description)\nUse topic specific words\nFollow the 100% rule\n\n Vocabulary Don’ts\n\nRepeat the same words again and again\nForce complex words into your essay without knowing them 100%\nUse synonyms that are wrong\nLearn lists of ‘academic’ words out of context\n\n Grammar Dos\n\nTry to write as many error-free sentences as possible\nUse ‘complex’ sentences (two classes rather than single)\nUse a variety of structures\nCheck work when writing and at the end\nFollow the 100% rule\n\n Grammar Don’ts\n\nTry to use as many different structures as possible\nTry to impress the examiner with complex grammar\nWrite sentences that stop meaning being conveyed\n\nCLEARITY IS KING\n Planning\n Generate ideas\n 6 Questions\n\nWho? What? Why? Where? How? When?\n\n Bonus Idea Generation\n\nIf you asked 100 people, what would be the common answer?\nIf you were trying to win an argument, what idea would you use?\n\n Structure Planning\n Introduction\n\n\nParaphrase Question\n\n\nOutline\n\n\n Supporting Paragraph 1\n\nMain point\nExplanation\nExample\n\n Supporting Paragraph 2\n\nMain point\nExplanation\nExample\n\n Conclusion\n\nSummarize main points\n\n Pre-think Vocabulary (Synonyms)\n\nStudents = graduates = undergraduates\nUniversity = College = third-level education = tertiary education\nConsume = use\nPeople = human\nTeenagers = adolescents\nGovernments = States\nAdvantages = benefits\n\n Timing\n\n40 minutes to complete task2\n10 minutes planning\nMost essays are around 12 sentences long\n2 minutes per sentence = 24 minutes\n6 minutes reviewing and checking our work\n\n Introduction\n Common Mistake\n\nWriting long general background statements or hooks\nNo opinion or outline of main ideas\n\n Combining opinion and outline\nThe continued rise in the world’s population is the greatest threat faced by humanity at the present time.\nDo you agree?\n\nrun out of resources\ndamage to the planet\n\n\n\n\n\n\n\n\n\n\nIncreasing overpopulation is the biggest thread human beings face today. This essay totally agrees with this statement because this will lead to a serious depletion of resources and pollution\n Examples\n Opinion\nIn some countries an increasing number of people are suffering from health problems as a result of\neating too much fast food. It is therefore necessary for governments to impose a higher tax on this kind of food.\nDo you agree?\n\n\n\n\n\n\n\n\n\nIt is argued that states should charge fast food companies more tax because of the growing amount of men and women with health conditions associated with this type of food. This essay totally agrees with that statement because these illnesses cost the health service too much money and increasing the price of junk food would reduce the demand for it.\n Discussion\nSome people work for the same organisation all their working life.Others think that it is better to work for different organisations.\nDiscuss both views and give your own opinion.\n\n\n\n\n\n\n\n\n\nSome say that it is more beneficial to be employed with the same company all their lives, while others would argue that it is better to work for a variety of companies. This essay will argue that although working for just one employer gives you more financial and other benefits,working at lots of different places provides an employee with more experience.\n Advantages and Disadvantages\nOne of the consequences of improved medical care is that people are living longer and life expectancy is increasing.\nDo you think the advantages of this development outweigh the disadvantages?\n\n\n\n\n\n\n\n\n\nOne of the results of modern medicine is that men and women are able to live longer. This essay will argue that despite the strain this might cause on the pension system,the alleviation of suffering means that the advantages far outweigh the drawbacks.\n Problem and Solution\nSmartphones are becoming a common sight in the primary school classroom.\nWhat problem does this cause and what is a viable solution?\n\n\n\n\n\n\n\n\n\niPhones and other devices are being used more often by primary school children. This essay will suggest that they are very distracting and that the best solution is to ban them completely in class.\n Double Questions\nCar ownership has increased so rapidly over the past thirty years that many cities in the world are now ‘one big traffic jam’.\nHow true do you think this statement is?\nWhat can governments do to discourage people from using their cars?\n\n\n\n\n\n\n\n\n\nLots of city centers are highly congested due to an increase in the ownership of cars in the last three decades. This essay will argue that this is true only during peak times and that improving public transport will reduce this.\n Body paragraphs\n Common Mistake\n\nToo many ideas\nUndeveloped ideas\nNo/Poor explanations or examples\nFirstly, secondly, thirdly, finally\nPoor grammar and vocabulary\n\n Checklist\n\nRelevant ideas\nFully address all parts of the task\nClear position throughout\nFully extended and well supported ideas\n\n Structure - 3 Key Elements\n\nTopic Sentence\nExplanation Sentences\nExample\n\n\n\n\n\n\n\n\n\n\nIt is argued that states should charge fast food companies more tax because of the growing amount of men and women with health conditions associated with this type of food. This essay totally agrees with that statement because these illnesses cost the health service too much money and increasing the price of junk food would reduce the demand for it.\nObesity related illnesses cost the taxpayer billions of dollars every year. People who eat too much Food are more likely to suffer from costly diseases associated with being overweight. As a result, they have to go to hospital more regularly for treatment and this puts a strain on the health service. For example,a large proportion of the United Kingdom’s National Health Service budget is spent on preventable, obesity-related diseases, such as heart disease, hypertension and diabetes.\n Topic Sentence\n\nShort clear statement about what the paragraph is about\nDirectly answers the question\nNot much detail.\nMakes your main ideas clear\nCan’t write one unless you have clear ideas already\n\n\n\n\n\n\n\n\n\n\nIntroduction:\nincreasing the price of junk food would reduce the demand for it.\n↓\nTopic Sentence:\nAnother reason why fast food restaurants should pay extra tax is to raise the cost in order to decrease demand.\n Explanation\n\n\nPretend that you are writing to someone with on knowledge of the subject\n\n\nClear explain\n\n\nwhat you topic sentence means\n\n\nhow it answers the question\n\n\nwhat the result is\n\n\n\n\nShould be around 2-4 sentences\n\n\nUseful language\n\nThat is to say… = In other words…\nThis is because… = The reason is…\nAs a result… = Therefore…\n\n\n\n\n\n\n\n\n\n\n\n\nAnother reason why fast food restaurants should pay extra tax is to raise the cost in order to decrease demand. Any extra tax would be added to the normal price of the food, and as prices go up fewer people will be able to afford to buy fatty food. Therefore, this would reduce the amount of junk food people eat and the result would be a healthier nation.\n Examples\n Better Examples\n\nReal examples\nConverting personal examples\n\n How to think of examples\n\n\nTake examples from your own life experience\n\n\nConvert them into more general examples\n\n\nThink about\n\nWhere are you form?\nWhere do you live\nWhat is  your job?\nWhat about your family and friends?\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal:\nFor example, I used to smoke 20 cigarettes everyday, but the government kept increasing the tax and now I can’t afford to smoke.\n↓\nGeneral:\nFor example, when the duty on cigarettes is raised each year in the UK, more people quit smoking and this has a knock on effect on the number of people dying from lung cancer and other smoking related diseases.\n\n\n\n\n\n\n\n\n\nYour own personal experience:\nMy sister’s bookshop closed down because it couldn’t compete with Amazon.\n↓\nGeneral:\nFor instance, many bookshops in the UK closed down because of competition from online.\n\n\n\n\n\n\n\n\n\nYour own personal experience:\nI can’t afford to buy a laptop for my child.\n↓\nGeneral:\nFor example, in Ireland laptops are very expensive which would prevent most parents from being able to purchase them for their children.\n\n\n\n\n\n\n\n\n\nState something and add:\nas reported by many media outlets.\n Conclusion\n Big Mistakes\n\n\nNew ideas\n\n\nTrying to be entertaining\n\n\nBeing too vague\n\n\nRepeating exactly the same thing as in the rest of your essay\n\n\nUsing the wrong cohesive devices\n\n\n Why are conclusions important?\n\nIt is the last thing the examiner reads\nIt shows the examiner that you can summarise\nOpinion is clear the whole way through\nMakes essay cohesive and coherent\n\n Appropriate Cohesive Devices\n\nIn conclusion,\nTo conclude,\n\n Structure\n\n\nTwo essential things are\n\nSummary of main points\nOpinion\n\n\n\nOne sentences\n\n\n Example\n\n\n\n\n\n\n\n\n\nQuestion:\nThe continued rise in the world’s population is the greatest threat faced by humanity at the present time. Do you agree?\nIntroduction:\nIncreasing overpopulation is the biggest threat human beings face today. This essay totally agrees with this statement because this will lead to a serious depletion of resources and pollution.\nConclusion:\nIn conclusion, there are too many people in the world and this is a huge threat to everyone because\nessential resources are running out and we are also polluting our planet.\n\n\n\n\n\n\n\n\n\nIntroduction:\niPhones and other devices are being used more often by primary school children. This essay will\nsuggest that they are very distracting and that the best solution is to ban them completely in class.\nConclusion:\nIn conclusion, mobiles are not a good idea for young children because they interfere with the learning process and this should be stopped by telling parents that they are not allowed in schools.\n Review\n\n\nGrammar\n\nPrepositions\nArticles\nVerb-subject agreement\nCountable/Uncountable nouns\nTenses\nCapital letters\n\n\n\nVocabulary\nFirst you should scan for any words repeated that you can easily change using synonyms.\nFocus on\n\nCollocations\nMeaning\nSpelling\nWord form\n\n\n\nChecklist\n\nDoes the essay answer all parts of the question?\nIs the opinion clear in the introduction?\nDoes each supporting paragraph have a clear topic sentence?\nAre ideas fully developed with explanations and examples?\nIs your opinion clear throughout the whole essay?\nIs there a suitable conclusion?\nOver 250 words?\nRange of complex and simple sentences.\nAnything confusing or unclear?\n\n\n\n 1. Opinion\n Question\n\nDo you agree or disagree?\nTo what extent do you agree or disagree\n\nalways state ‘strongly agree’ or ‘strongly disagree’\n\n\n\n Alternative wording of opinion questions\n\nIs this a positive or negative development?\nTo what extent is this a positive or negative development\n\n Common Mistakes\n\nNot giving your opinion\nLeaving opinion until the conclusion\nDiscussing someone else’s opinion\nDiscussing both sides of the argument\n\n Band 9 Checklist\n\nGive a clear opinion in the introduction\nThink of two main points supporting your opinion\nDevelop these main points with explanations and examples\nShort conclusion summarizing your main points and reiterating your opinion\n\n Deciding Opinion\n\nWastes time\nPersonal opinion does not matter\nPick one side only\nPick the side  you can easily write about\n\n Language\nGood in introduction:\n\nThis essay completely agrees that…\nThis essay totally disagrees that…\nIn my opinion,\nI totally agree/disagree\n\n Structure\nParagraph Introduction\n\nParaphrase Question\nGive opinion and outline main points\n\nSupporting Paragraph I\n\nTopic Sentence\nExplanations\nExample\n\nSupporting Paragraph II\n\nTopic Sentence\nExplanations\nExample\n\nConclusion\n\nSummary\n\n Planning- Idea Generation\n\n\n\n\n\n\n\n\n\nIn many countries it is now illegal to advertise alcohol. Do you agree or disagree?\nThoughts:\n\n\nWhy do I agree? (Coffee shop method)\n\n\nAlcohol is a problem both socially and for our health\n\n\nTaking advantage of alcoholics\n\n\nAdvertisers can influence people to drink alcohol\n\n\nYoung people are exposed to persuasive ads\n\n\nIntroduction\n\nParaphrase Question\nAgree and outline main points\n\nSupporting Paragraph 1\n\nTopic Sentence - health and social problems\nExplanations - addiction and bad behaviour\nExample - Russia\n\nSupporting Paragraph 2\n\nTopic Sentence - influence\nExplanations - glamorise and young people\nExample - Specific advert\n\nConclusion\n\nSummary\n\nVocabulary:\n\nIllegal-banned, ban, prohibited, not allowed.\nAdvertise-TV, radio, print, ads, commercials, promoting.\nAlcohol-drinking, alcoholic drinks/beverages,beer, wine and spirits.\nAddiction-alcoholic, addicted to alcohol, alcohol dependence, dependent on alcohol.\nBad behavior -anti-social behavior, violence,vandalism.\nInfluence-sway,persuade,convince.\n\n\n\n\n\n\n\n\n\n\nMore and more people are now working remotely from home. Is this a positive or negative development?\nIntroduction\n\nParaphrase Question\nPositive and outline main points\n\nSupporting Paragraph 1\n\nTopic Sentence - less stress\nExplanations - no commute in heavy traffic / timeable adaptable to personal needs\nExample - workers can make timetable to incorporate regular breaks in order to relax\n\nSupporting Paragraph 2\n\nTopic Sentence - less environmental impact\nExplanations - lower carbon footprint due to less travel and energy to power office buildings\nExample - Pandemic meant working from home, which meant lower levels of pollution\n\nConclusion\n\nSummary\n\n Review\n\nNow don’t forget to review your essay\nMake sure that:\n\nGive a clear opinion in the introduction\nThink of two main points supporting your opinion\nDevelop these main points with explanations and examples\nShort conclusion summarizing your main points and reiterating your opinion\n\n\n\n Template\n\nsentences\n\nIn the contemporary era, discussions regarding xxx have consistently garnered public interest. Some advocate for\nthe idea that xxx, a position I find myself inclined to support.\n\n\n\n\nAgree or disagree\nIt is important for everyone, including young people, to save money for their future. To what extent do you agree or disagree with this statement?\nIt is often said that saving money for the future is important for everyone, including young people. This essay fully agrees with this view because reducing unnecessary expenses enables people to purchase higher-quality items and helps them improve their quality of life after retirement.\nBy spending less money on unnecessary items, people can save more and afford higher-quality products. Young people, in particular, may often be tempted by attractive items in stores, such as decorative crafts or small models, which serve little purpose beyond taking up space. Instead of falling into these consumption traps, it is wiser to invest in durable, high-quality products that can have a positive impact on daily life. Many experts suggest that choosing quality over quantity can lead to long-term savings and greater satisfaction.\nAfter retirement, those who have saved enough money are more likely to enjoy a comfortable life. Although many elderly people receive government pensions, these payments are often insufficient to maintain their previous lifestyle. With extra savings, individuals can earn interest by keeping their money in the bank, which reduces financial pressure. This allows them to spend more on essential areas like healthcare and nutrition, helping them live longer and healthier lives. Additionally, they may also feel more confident about supporting their children financially.\nIn conclusion, saving money is essential for everyone, as it allows them to make wiser purchases by investing in higher-quality items. Moreover, it ensures a more comfortable life in retirement, where individuals can spend more on healthcare and other important aspects, reducing the financial burden as they age.\n\nThe working week should be shorter and workers should have a longer weekend. Do you agree or disagree?\nIt is often argued that workers should have shorter working weeks and longer weekends. This essay fully agrees with this view because it allows employees to better balance their work and personal lives, increases their efficiency, and helps companies save money while becoming more attractive to potential employees.\nHaving more time on weekends can help workers achieve a better work-life balance. With additional free time, they can take care of their children, spend quality time with family, and pursue hobbies such as sports or music. This not only benefits individuals but also makes companies more attractive to talented employees. For example, Google promotes a strong work-life balance, and as a result, many young professionals prefer working there over companies that offer higher salaries but demand more working hours.\nWith fewer working hours, employees are likely to work more efficiently and smartly. After having more rest, their brains are refreshed, allowing them to focus better and complete tasks in less time. Consequently, the company’s overall efficiency may improve by providing employees with more relaxation. Furthermore, companies can reduce operational costs, such as electricity and air conditioning, by operating fewer hours each week.\nIn conclusion, reducing the working week is beneficial not only for employees’ personal lives but also for companies. It can increase efficiency, attract talented workers, and reduce operational costs, making it a positive change for both individuals and businesses.\n\nThe most important aim of science should be to improve people’s lives.\nTo what extent do you agree or disagree with this statement? Give reasons for your answer and include any relevant examples from your own knowledge or experience.\nIt is often argued that the primary role of science is to enhance human lives. This essay fully agrees with this view, as scientific advancements have significantly improved healthcare and greatly enhanced the quality of life.\nThere is no doubt that medical and healthcare systems have advanced significantly over time, benefiting nearly everyone. In the past, many deadly diseases, spread by animals like mice and insects, led to the deaths of thousands. However, thanks to the diligent efforts of scientists, many of these bacterial and viral illnesses have been eradicated through vaccines developed from organic materials. These scientific breakthroughs have saved countless lives.\nScientific advancements have also played a vital role in improving the quality of life. The first and second industrial revolutions introduced humans to the power of fire and electricity, now essential domestic resources that have made life more convenient. For instance, while people once had to burn wood for heat, today we can simply turn a knob to enjoy hot water. Additionally, in the past, ice was a luxury only the wealthy could afford during summer, but now nearly every household owns a refrigerator, making ice readily available. Without scientific development, modern life would not be as comfortable as it is today.\nIn conclusion, the ultimate goal of science is to improve people’s lives, whether through advancements in healthcare or living conditions. This is a meaningful endeavor that benefits humanity as a whole.\n\nSome people believe that zoos where animals are kept in a man-made environment should no longer exist in the 21st century. To what extent do you agree or disagree?\nMany people argue that zoos where creatures are contained in artificial enclosures should be banned in this century. This essay totally agrees with this statement because it is cruel to keep an animal in a confined space, and nature programs allow us to view animals without the need of a zoo.\nThe first reason that animals should not be kept in artificial enclosures is that it is inherently cruel. That is to say that as human beings, we are intelligent enough to know that the animal is suffering and if we decide to do it anyway, we are causing unneeded pain and discomfort. For example, Sea World in California recently admitted that their whales were depressed and not in optimal health because they cannot swim in the ocean, but decided to keep them in the park because they were a big attraction.\nThe second main reason is that high quality documentaries remove our need to see animals in captivity. This is because TVs have such high quality screens and programmers are now able to offer us a very intimate glimpse of animals in their natural habitat. Thus, we can get a better insight into the lives of wild animals by staying at home. For instance, the BBC recently produced a series called Life in the Undergrowth which allowed us to understand the lives of insects in stunning high definition.\nIn conclusion, wild animals should all be allowed to live freely because it is evil to trap and force them to live in an animal park, and television shows about animals allow us to learn about them without going to see them in person.\n\nIn many countries it is now illegal to advertise alcohol. Do you agree or disagree?\nPlan:\nMain idea-drinking can lead to serious health problems\nWhy/How-drinking too much can cause liver damage. People often get violent and this can lead to assaults and vandalism.\nMain idea-influences people to drink more.\nWhy/How-advertisers main goal is to sell. Adverts glamorise drinking.\nExample-Hennessy adverts.\nOpinion-Agree that it should be made illegal.\nMain points-harmful to health and influences people unfairly.\nEssay:\nAdverts for beer, wine and spirits are now banned in many nations. This essay agrees with this idea because drinking can lead to serious health problems and advertisers can often influence people to drink more than they should.\nAdvertising alcoholic drinks should be banned because alcohol is harmful to a person’s health. Drinking too much causes several conditions including liver cancer,diabetes and brain damage. This puts a huge strain on the health service and not only causes suffering to the patients but also to their families. For example, Russia had a huge problem with alcoholism and many men were dying from alcohol-related diseases, but after they decided to stop companies promoting alcoholic beverages on TV and in print ads, it resulted in a huge reduction of such cases.\nAnother reason why the promotion of alcohol should be prohibited is the fact that it influences people unfairly to buy that product. The main goal of any advertising campaign is to sell more and this could be seen as irresponsible considering the harm drinking can do. Commercials often make certain drink brands look cool or sexy, in order to sway potential customers,and don’t actually talk about the dangers. For example, Hennessy adverts often portray attractive men and women in a luxurious setting, drinking cognac with a beautiful woman.\nIn conclusion, alcohol is a hazardous product and advertisers should not be allowed to try and convince people to drink it and I, therefore, believe that more countries should ban the advertising of liquor.\n\n\n\nAdvantages and disadvantages\nMany teenagers have their own smartphones.\nDoes this situation have more advantages or more disadvantages?\nMany adolescents own a smart device. This essay argues that despite the main advantage of this being that they have the ability to stay in contact with their parents, I believe that there are more disadvantages as this creates a lack of face-to-face communication, which has many detrimental effects.\nThe biggest benefit of teenagers having mobile phones is the fact that their parents can call them and even check on their location. This is because smartphones have GPS, and many apps allow parents to accurately determine where their child is or simply message them to make sure they are safe. For example, messaging apps like Viber have a location setting so that parents can ask where their children are in a message and then check on Google Maps when their loved ones respond. However, I think that the lack of physical contact caused by mobile phone usage\nis a more serious concern.\nSome believe that the drawback to teens having smartphones is that they reduce the amount of time they spend talking face to face. Most adolescents spend too much time on their phones. Face-to-face communication allows children to develop social skills and grow as human beings, and they may become anti-social adults in the future if they stare at a screen all day. For instance, it is very common to go into any coffee shop and see large groups of young people sitting around and not talking for several minutes because they are so engrossed in their smartphones. Therefore, I believe that smartphones, overall, have a detrimental effect on adolescents.\nIn conclusion, the primary value of teenagers having smartphones is the fact that their parents can easily stay in touch with them; however, I feel that the lack of in-person interaction is a more significant downside.\n\nIt is important for people to take risks, both in their professional lives and their personal lives.\nDo you think the advantages of taking risks outweigh the disadvantages?\nGive reasons for your answer and include any relevant examples from your own knowledge or experience.\n(这篇内容写错了，应该两边都要讨论，然后说哪个好)\nMany people are willing to take risks not only in their personal lives but also in their professional careers. This essay completely agrees with this view, as without risks, our lives would be relatively monotonous, and taking risks can lead to more business or career opportunities.\nPeople generally find a life without risks dull and unfulfilling. If nothing changes and we simply go through daily routines—such as going to school or work in the morning and returning home in the evening—there would be little room for growth or progress. However, by stepping out of our comfort zones and trying new experiences, like traveling or climbing a mountain to appreciate the view, life becomes more exciting and meaningful. For instance, the Japanese culture of lifelong employment in one company discourages people from changing their lifestyles, and this rigidity has contributed to a higher suicide rate compared to other countries.\nTaking risks in the business world often leads to more opportunities and greater rewards. In the highly competitive market, companies that refuse to innovate risk being left behind by their competitors. However, taking calculated risks, such as developing new products or changing marketing strategies, may require effort but can result in significant success. For instance, Elon Musk, a well-known entrepreneur, consistently takes risks by investing in seemingly unprofitable technology ventures, many of which have since become highly successful.\nIn conclusion, embracing risks is important because it brings excitement and growth to our personal lives, and in business, it often leads to more opportunities for success. For these reasons, the advantages of taking risks outweigh the disadvantages.\n\n\n\nOpinion\nSome university students want to learn about other subjects in addition to their main subjects. Others believe it is more important to give all their time and attention to studying for a qualification.\nDiscuss both these views and give your own opinion.\nIt is argued that university students are interested in learning subjects outside their main courses, while others believe that focusing entirely on obtaining a qualification is more important. This essay will argue that although learning additional subjects may be interesting, obtaining a qualification is ultimately more beneficial for students’ future careers.\nSome university students choose to learn additional subjects because they find them fun, interesting, and helpful for socializing. For example, universities often offer extracurricular activities such as basketball, music, or climbing, where students can bond with peers and enjoy their time. However, some students may devote too much time to these activities, which could negatively impact their academic performance, as reported by various media outlets.\nOn the other hand, some students focus on obtaining professional qualifications during their university years, which can make their career path smoother. They spend time studying for qualifications in fields such as law, medicine, or sports. After graduation, those who possess the necessary qualifications will have a significant advantage over those who do not. This is why many Asian families encourage their children to prioritize studying for professional exams over participating in social activities. Although this may put pressure on students and affect their mental health, it ultimately helps them integrate more easily into the workforce.\nIn conclusion, while participating in extracurricular activities may be more appealing to students, obtaining qualifications is crucial as it provides them with better job opportunities and a smoother transition into the workforce.\n\nSome people think that a sense of competition in children should be encouraged. Others believe that children who are taught to cooperate rather than compete become more useful adults.\nDiscuss both views and give your own opinion.\nSome people believe that competition in our careers, school, and daily life is beneficial, while others argue that cooperation is more important. This essay will argue that although competition can make individuals stronger in skills, teamwork creates greater value and can achieve more than what individuals can accomplish on their own.\nCompetition between individuals can undoubtedly make them stronger. When competition is encouraged, people are more motivated to succeed. Winning releases chemicals in the brain, providing a sense of reward and accomplishment. At the same time, those who lose are often driven to improve their skills to succeed in future competitions. For example, many high school students enjoy competing in one-on-one basketball games. Although some may lose initially, they often improve their basketball skills in subsequent games.\nHowever, cooperation has been a key theme throughout human history. A team is always stronger than an individual because each member contributes their unique skills, making the project more efficient and manageable. In a well-organized team, individuals focus on their specific tasks, allowing the workflow to move smoothly rather than placing all the responsibility on one person. For example, most software companies have clearly structured employee levels and delegate tasks to the most qualified individuals, which has led to the development of successful software like Google Search.\nIn conclusion, while competition can enhance individual skills, cooperation is more valuable as it allows people to work together efficiently and achieve greater outcomes.\nIt is argued that children should be motivated to be competitive, while others feel that teaching them to be cooperative will be of more value as they enter adulthood. I believe that while competition can help children be successful, cooperation is more important because it teaches them to work within a team, a crucial adult skill.\nSome argue that instilling a sense of competition in children helps them to achieve success in whatever they do. This is because being competitive creates a drive to win, which teaches them that hard work and discipline are the keys to success. For example, it is often the case that children who participate in competitive sports are less likely to quit when things are difficult and are, therefore, more likely to overcome obstacles in their jobs as adults. Despite this, I would argue that children require lessons on teamwork more than the will to win.\nIf children are taught to be cooperative, they learn the importance of working in a team, which is something adults are expected to do. Through working with others, children learn not only how to respect different opinions but also how to pool their strengths. For instance, preschools include cooperation as one of the first skills in their curriculum as they recognise that it is a vital social skill in all spheres of life. I therefore believe that teaching children to work with others is more important than giving them a sense of competition.\nIn conclusion, while instilling a sense of competition in children can help them succeed, I think that teaching children to be cooperative gives them the ability to work as part of a team, which is far more valuable later in life.\n\n\n\n","slug":"Writing_Task_2","date":"2024-06-09T13:53:00.000Z","categories_index":"Notes","tags_index":"English,IELTS","author_index":"Huaji1hao"},{"id":"79276e65a56422ae2094bb3bd778a530","title":"Configure Namesilo domain for Github page","content":" use Namesilo domain for Github page / 使用 Namesilo 域名配置 Github Pages\n 1. buy a domain in namesilo\n 2. manage the domain DNS\n1234567891011121314151617TYPE A    HOSTNAME: leave blank here    Address(add four times):     185.199.108.153    185.199.109.153    185.199.110.153    185.199.111.153TYPE CNAME    HOSTNAME: www    Address: xxx.github.ioTYPE AAAA(optional, for ipv6)    HOSTNAME: retain empty    Address:     2606:50c0:8000::153    2606:50c0:8001::153    2606:50c0:8002::153    2606:50c0:8003::153\n 3. verify domain in user site\n\n\nprofile-&gt;settings-&gt;Code, planning, and automation-&gt;Pages-&gt;Add a domain\n\n\nenter the domain name\n\n\nopen Namesilo DNS management and add\n 123TYPE TXT\tHOSTNAME: _github-pages-challenge-....(your TXT record)\tAddress: asdd324fsdf(your code)\n\n\nVerify until success\n\n\n 4. add custom domain in repository\n\nxxx.github.io-&gt;settings-&gt;Pages-&gt;Custom domain\nadd your domain and save\nclick enforce HTTPS\n\n 使用 Namesilo 域名配置 Github Pages\n 1. 在 Namesilo 购买域名\n 2. 管理域名的 DNS\n在 Namesilo 的域名管理界面，添加以下 DNS 记录：\n1234567891011121314151617TYPE A    HOSTNAME: 这里留空    Address（添加四次不同的地址）:     185.199.108.153    185.199.109.153    185.199.110.153    185.199.111.153TYPE CNAME    HOSTNAME: www    Address: xxx.github.io（将 xxx 替换为你的 GitHub 用户名）TYPE AAAA（可选，用于 IPv6）    HOSTNAME: 保持空白    Address:     2606:50c0:8000::153    2606:50c0:8001::153    2606:50c0:8002::153    2606:50c0:8003::153\n 3. 在 GitHub 用户站点验证域名\n\n\n进入 GitHub 个人资料页，依次点击：Settings（设置） -&gt; Code, planning, and automation（代码、计划和自动化） -&gt; Pages（页面） -&gt; Add a domain（添加域名）\n\n\n输入你的域名\n\n\n打开 Namesilo 的 DNS 管理界面，添加以下记录：\n123TYPE TXT\tHOSTNAME: _github-pages-challenge-....(你的 TXT 记录)\tAddress: asdd324fsdf(你的代码)\n\n\n等待并验证，直到成功\n\n\n 4. 在 GitHub 仓库中添加自定义域名\n\n进入你的 GitHub Pages 仓库（例如 xxx.github.io），依次点击：Settings（设置） -&gt; Pages（页面） -&gt; Custom domain（自定义域名）\n添加你的域名并保存\n勾选 Enforce HTTPS（强制 HTTPS）\n\n这样，你就成功地将 Namesilo 域名配置到你的 GitHub Pages 上了。\n","slug":"Namesilo_GithubPage","date":"2024-06-04T10:49:36.000Z","categories_index":"Article","tags_index":"Domain,Github","author_index":"Huaji1hao"},{"id":"156564d90d95b8548776617619625b2a","title":"Something Interesting","content":" QAQ\n\n你真的会百度吗？\n你谷模板\n剪贴板迷宫\n2048\n2048朝代版\n给神犇推荐一个构图网站\n 万能头文件（手动滑稽）：\n1include &lt;bugs/stdc--.h&gt;\n\n 快读\n123456inline int read()&#123;    register int x=0,f=1;char c=getchar();    while(c&lt;&#x27;0&#x27;||c&gt;&#x27;9&#x27;)&#123;if(c==&#x27;-&#x27;)f=-1;c=getchar();&#125;    while(c&gt;=&#x27;0&#x27;&amp;&amp;c&lt;=&#x27;9&#x27;) x=(x&lt;&lt;3)+(x&lt;&lt;1)+(c^48),c=getchar();    return x*f;&#125;\n\n 快排\n12345678910111213141516void qsort(int a[], int l, int r)&#123;    // sort a[] in the range of [l, r].\tif(l == r) return;\tint i = l, j = r;\tint pivot = a[l + (r - l) / 2];\tdo&#123;\t\twhile(a[i] &lt; pivot) i++;\t    while(a[j] &gt; pivot) j--;\t    if(i &lt;= j)&#123;\t    \tint tmp = a[i];a[i] = a[j];a[j] = tmp;\t        i++;j--;\t\t&#125;\t&#125;while(i &lt;= j);\tif(l &lt; j) qsort(a, l, j);\tif(i &lt; r) qsort(a, i, r);&#125;\n\n 快速幂\n12345678910int quickPower(int a,int b)&#123;    // calculate a ^ b.\tint base=1;\twhile(b)&#123;\t\tif(b&amp;1)base*=a;//base%=mod;\t\ta*=a;//a%=mod;\t\tb&gt;&gt;=1;\t&#125;\treturn base;&#125;\n\n 秦九韶\n123456789double QinJiushao(double x)&#123;    // f(x) = a_1 * x^n + ... + a_n-1 * x^2 + a_n * x + a_n+1    double sum = indexNum[1];    for(int i = 1; i &lt;= n; ++i)&#123;        sum *= x;        sum += indexNum[i + 1];    &#125;    return sum;&#125;\n\n本人精通CSS、JavaScript、PHP、ASP、C、C＋＋、C#、Java、Ruby、Perl、Lisp、python、Objective-C、ActionScript、Pascal等单词的拼写\n熟悉Windows、Linux、Mac、Android、IOS、WP8等系统的开关机\n\n\n","slug":"Luogu_profile","date":"2024-06-01T10:49:36.000Z","categories_index":"Memes","tags_index":"Fun","author_index":"Huaji1hao"}]